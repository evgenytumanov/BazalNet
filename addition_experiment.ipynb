{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition experiment\n",
    "\n",
    "An implementation of sequence to sequence learning for performing addition\n",
    "\n",
    "* Input: \"535+61\"\n",
    "\n",
    "* Output: \"596\"\n",
    "\n",
    "Padding is handled by using a repeated sentinel character (space)\n",
    "Input may optionally be inverted, shown to increase performance in many tasks in:\n",
    "\"Learning to Execute\"\n",
    "http://arxiv.org/abs/1410.4615\n",
    "and\n",
    "\"Sequence to Sequence Learning with Neural Networks\"\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "Theoretically it introduces shorter term dependencies between source and target.\n",
    "Two digits inverted:\n",
    "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
    "Three digits inverted:\n",
    "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
    "Four digits inverted:\n",
    "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
    "Five digits inverted:\n",
    "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:\n",
      "b'C:\\\\Users\\\\tumanov\\\\AppData\\\\Local\\\\Temp\\\\try_flags__ck3yvy_.c:4:19: fatal error: cudnn.h: No such file or directory\\r\\ncompilation terminated.\\r\\n'\n",
      "Mapped name None to device cuda0: GeForce GTX 1070 (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_trainable(net):\n",
    "    for tags in net.params.values():\n",
    "        tags -= {'trainable', 'regularizable'}\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "INVERT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try replacing GRU, or SimpleRNN.\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(size, digits, problem = lambda a, b: a+b, problem_operator='{}+{}'):\n",
    "    source_seqs = []\n",
    "    target_seqs = []\n",
    "    seen = set()\n",
    "    print('Generating data...')\n",
    "    while len(source_seqs) < TRAINING_SIZE:\n",
    "        f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                        for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "        a, b = f(), f()\n",
    "        # Skip any addition questions we've already seen\n",
    "        # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "        key = tuple(sorted((a, b)))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        # Pad the data with spaces such that it is always MAXLEN.\n",
    "        q = problem_operator.format(a, b)\n",
    "        ans = str(problem(a, b))\n",
    "\n",
    "        source_seqs.append(q)\n",
    "        target_seqs.append([\"START\"] + list(ans) + [\"END\"])\n",
    "\n",
    "    print('Total addition questions:', len(source_seqs))\n",
    "    \n",
    "    target_letters = list(set([token for ts in target_seqs for token in ts]))\n",
    "    target_letter_to_ix = {ph:i for i,ph in enumerate(target_letters)}\n",
    "    \n",
    "    source_letters = list(set([token for word in source_seqs for token in word]))\n",
    "    source_letter_to_ix = {l:i for i,l in enumerate(source_letters)}\n",
    "    \n",
    "    return np.array(source_seqs), source_letters, source_letter_to_ix, \\\n",
    "           np.array(target_seqs), target_letters, target_letter_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n"
     ]
    }
   ],
   "source": [
    "source_seqs, source_letters, source_letter_to_ix, target_seqs, target_letters, target_letter_to_ix =\\\n",
    "                    generate_data(TRAINING_SIZE, DIGITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929+19 : 948\n",
      "682+80 : 762\n",
      "673+282 : 955\n",
      "13+3 : 16\n",
      "50+23 : 73\n"
     ]
    }
   ],
   "source": [
    "for source, target in zip(source_seqs[:5],target_seqs[:5]):\n",
    "    print( source,':',\"\".join(target[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFqNJREFUeJzt3X+snuV93/H3JzYjXloTAmeeZ5OZCG+SQYtTLM9bqioN\nSnFIVBMJMkdqsCoLZ8OLUq1SC/1jTf6wBH+kdGyDiZQMQ9OCRZthJZCJQKYu0jA9pARjE5SjAMJH\nBrtAoNkGlZ3v/niu0z1+7mOf5/ywzzn2+yXdOtfzva/rea5LN+Lj+8d5TqoKSZL6vWe+JyBJWngM\nB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1DF0OCRZkuSvknyrvf5AkseS/Lj9vLCv7y1J\nxpK8kOTqvvqVSfa3fXckSaufn+TBVt+XZM3cLVGSNF1Lp9H3S8DzwPL2+mbg8aq6NcnN7fXvJlkH\nbAUuB/4R8N0k/6SqjgN3ATcC+4BHgM3Ao8B24M2quizJVuA24F+dajIXX3xxrVmzZhrTlyQ9/fTT\nf11VI1P1GyockqwGPgXsAv5dK28BPtbau4H/Afxuqz9QVe8CLyYZAzYmeQlYXlVPtve8D7iWXjhs\nAb7c3ush4D8lSZ3iuz3WrFnD6OjoMNOXJDVJXh6m37CXlf4Q+B3g5321FVV1uLVfBVa09irglb5+\nh1ptVWsP1k8YU1XHgLeAiwYnkWRHktEko0ePHh1y6pKk6ZoyHJJ8GjhSVU+frE/7F/5p/wa/qrq7\nqjZU1YaRkSnPiiRJMzTMZaWPAr+e5BrgvcDyJH8MvJZkZVUdTrISONL6jwOX9I1f3WrjrT1Y7x9z\nKMlS4ALg9RmuSZI0S1OeOVTVLVW1uqrW0LvR/ERV/QawF9jWum0DHm7tvcDW9gTSpcBa4Kl2Cert\nJJvaU0o3DIyZeK/r2mf4XeKSNE+m87TSoFuBPUm2Ay8DnwWoqgNJ9gAHgWPAzvakEsBNwL3AMno3\noh9t9XuA+9vN6zfohZAkaZ5ksf4DfcOGDeXTSpI0PUmerqoNU/XzN6QlSR2GgySpw3CQJHXM5oa0\npEVuzc3fnlb/l2791GmaiRYazxwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAc\nJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjqmDIck703yVJIfJjmQ5Cut/uUk40meads1fWNuSTKW\n5IUkV/fVr0yyv+27I0la/fwkD7b6viRr5n6pkqRhDXPm8C7w8ar6MLAe2JxkU9t3e1Wtb9sjAEnW\nAVuBy4HNwJ1JlrT+dwE3AmvbtrnVtwNvVtVlwO3AbbNfmiRppqYMh+r5WXt5XtvqFEO2AA9U1btV\n9SIwBmxMshJYXlVPVlUB9wHX9o3Z3doPAVdNnFVIks68oe45JFmS5BngCPBYVe1ru76Y5NkkX09y\nYautAl7pG36o1Va19mD9hDFVdQx4C7hoknnsSDKaZPTo0aNDLVCSNH1DhUNVHa+q9cBqemcBV9C7\nRPQhepeaDgNfPW2z/P/zuLuqNlTVhpGRkdP9cZJ0zprW00pV9VPge8DmqnqthcbPga8BG1u3ceCS\nvmGrW228tQfrJ4xJshS4AHh9ekuRJM2VYZ5WGkny/tZeBnwC+FG7hzDhM8Bzrb0X2NqeQLqU3o3n\np6rqMPB2kk3tfsINwMN9Y7a19nXAE+2+hCRpHiwdos9KYHd74ug9wJ6q+laS+5Osp3dz+iXgCwBV\ndSDJHuAgcAzYWVXH23vdBNwLLAMebRvAPcD9ScaAN+g97SRJmidThkNVPQt8ZJL6508xZhewa5L6\nKHDFJPV3gOunmosk6czwN6QlSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkd\nhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHcP8Den3JnkqyQ+THEjylVb/QJLH\nkvy4/bywb8wtScaSvJDk6r76lUn2t313tL8lTft70w+2+r4ka+Z+qZKkYQ1z5vAu8PGq+jCwHtic\nZBNwM/B4Va0FHm+vSbKO3t+AvhzYDNzZ/v40wF3AjcDatm1u9e3Am1V1GXA7cNscrE2SNENThkP1\n/Ky9PK9tBWwBdrf6buDa1t4CPFBV71bVi8AYsDHJSmB5VT1ZVQXcNzBm4r0eAq6aOKuQJJ15Q91z\nSLIkyTPAEeCxqtoHrKiqw63Lq8CK1l4FvNI3/FCrrWrtwfoJY6rqGPAWcNG0VyNJmhNDhUNVHa+q\n9cBqemcBVwzsL3pnE6dVkh1JRpOMHj169HR/nCSds6b1tFJV/RT4Hr17Ba+1S0W0n0dat3Hgkr5h\nq1ttvLUH6yeMSbIUuAB4fZLPv7uqNlTVhpGRkelMXZI0DcM8rTSS5P2tvQz4BPAjYC+wrXXbBjzc\n2nuBre0JpEvp3Xh+ql2CejvJpnY/4YaBMRPvdR3wRDsbkSTNg6VD9FkJ7G5PHL0H2FNV30ryv4A9\nSbYDLwOfBaiqA0n2AAeBY8DOqjre3usm4F5gGfBo2wDuAe5PMga8Qe9pJ0nSPJkyHKrqWeAjk9Rf\nB646yZhdwK5J6qPAFZPU3wGuH2K+kqQzwN+QliR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJ\nHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkjinDIckl\nSb6X5GCSA0m+1OpfTjKe5Jm2XdM35pYkY0leSHJ1X/3KJPvbvjuSpNXPT/Jgq+9LsmbulypJGtYw\nZw7HgN+uqnXAJmBnknVt3+1Vtb5tjwC0fVuBy4HNwJ1JlrT+dwE3AmvbtrnVtwNvVtVlwO3AbbNf\nmiRppqYMh6o6XFU/aO2/AZ4HVp1iyBbggap6t6peBMaAjUlWAsur6smqKuA+4Nq+Mbtb+yHgqomz\nCknSmTetew7tcs9HgH2t9MUkzyb5epILW20V8ErfsEOttqq1B+snjKmqY8BbwEXTmZskae4MHQ5J\nfgH4M+C3quptepeIPgSsBw4DXz0tMzxxDjuSjCYZPXr06On+OEk6Zw0VDknOoxcM36iqPweoqteq\n6nhV/Rz4GrCxdR8HLukbvrrVxlt7sH7CmCRLgQuA1wfnUVV3V9WGqtowMjIy3AolSdM2zNNKAe4B\nnq+qP+irr+zr9hngudbeC2xtTyBdSu/G81NVdRh4O8mm9p43AA/3jdnW2tcBT7T7EpKkebB0iD4f\nBT4P7E/yTKv9HvC5JOuBAl4CvgBQVQeS7AEO0nvSaWdVHW/jbgLuBZYBj7YNeuFzf5Ix4A16TztJ\nkubJlOFQVd8HJnty6JFTjNkF7JqkPgpcMUn9HeD6qeYiSToz/A1pSVKH4SBJ6jAcJEkdhoMkqcNw\nkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJ\nUofhIEnqmDIcklyS5HtJDiY5kORLrf6BJI8l+XH7eWHfmFuSjCV5IcnVffUrk+xv++5IklY/P8mD\nrb4vyZq5X6okaVjDnDkcA367qtYBm4CdSdYBNwOPV9Va4PH2mrZvK3A5sBm4M8mS9l53ATcCa9u2\nudW3A29W1WXA7cBtc7A2SdIMTRkOVXW4qn7Q2n8DPA+sArYAu1u33cC1rb0FeKCq3q2qF4ExYGOS\nlcDyqnqyqgq4b2DMxHs9BFw1cVYhSTrzpnXPoV3u+QiwD1hRVYfbrleBFa29Cnilb9ihVlvV2oP1\nE8ZU1THgLeCi6cxNkjR3hg6HJL8A/BnwW1X1dv++diZQczy3yeawI8loktGjR4+e7o+TpHPWUOGQ\n5Dx6wfCNqvrzVn6tXSqi/TzS6uPAJX3DV7faeGsP1k8Yk2QpcAHw+uA8quruqtpQVRtGRkaGmbok\naQaGeVopwD3A81X1B3279gLbWnsb8HBffWt7AulSejeen2qXoN5Osqm95w0DYybe6zrgiXY2Ikma\nB0uH6PNR4PPA/iTPtNrvAbcCe5JsB14GPgtQVQeS7AEO0nvSaWdVHW/jbgLuBZYBj7YNeuFzf5Ix\n4A16TztJkubJlOFQVd8HTvbk0FUnGbML2DVJfRS4YpL6O8D1U81FknRm+BvSkqQOw0GS1GE4SJI6\nDAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNw\nkCR1GA6SpA7DQZLUMWU4JPl6kiNJnuurfTnJeJJn2nZN375bkowleSHJ1X31K5Psb/vuSJJWPz/J\ng62+L8mauV2iJGm6hjlzuBfYPEn99qpa37ZHAJKsA7YCl7cxdyZZ0vrfBdwIrG3bxHtuB96sqsuA\n24HbZrgWSdIcmTIcquovgDeGfL8twANV9W5VvQiMARuTrASWV9WTVVXAfcC1fWN2t/ZDwFUTZxWS\npPkxm3sOX0zybLvsdGGrrQJe6etzqNVWtfZg/YQxVXUMeAu4aBbzkiTN0kzD4S7gQ8B64DDw1Tmb\n0Skk2ZFkNMno0aNHz8RHStI5aUbhUFWvVdXxqvo58DVgY9s1DlzS13V1q4239mD9hDFJlgIXAK+f\n5HPvrqoNVbVhZGRkJlOXJA1hRuHQ7iFM+Aww8STTXmBrewLpUno3np+qqsPA20k2tfsJNwAP943Z\n1trXAU+0+xKSpHmydKoOSf4U+BhwcZJDwO8DH0uyHijgJeALAFV1IMke4CBwDNhZVcfbW91E78mn\nZcCjbQO4B7g/yRi9G99b52JhkqSZmzIcqupzk5TvOUX/XcCuSeqjwBWT1N8Brp9qHpKkM8ffkJYk\ndRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH\n4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpI4pwyHJ15McSfJcX+0DSR5L8uP288K+fbckGUvyQpKr++pX\nJtnf9t2RJK1+fpIHW31fkjVzu0RJ0nQNc+ZwL7B5oHYz8HhVrQUeb69Jsg7YClzextyZZEkbcxdw\nI7C2bRPvuR14s6ouA24HbpvpYiRJc2PKcKiqvwDeGChvAXa39m7g2r76A1X1blW9CIwBG5OsBJZX\n1ZNVVcB9A2Mm3ush4KqJswpJ0vyY6T2HFVV1uLVfBVa09irglb5+h1ptVWsP1k8YU1XHgLeAi2Y4\nL0nSHJj1Del2JlBzMJcpJdmRZDTJ6NGjR8/ER0rSOWmm4fBau1RE+3mk1ceBS/r6rW618dYerJ8w\nJslS4ALg9ck+tKrurqoNVbVhZGRkhlOXJE1lpuGwF9jW2tuAh/vqW9sTSJfSu/H8VLsE9XaSTe1+\nwg0DYybe6zrgiXY2IkmaJ0un6pDkT4GPARcnOQT8PnArsCfJduBl4LMAVXUgyR7gIHAM2FlVx9tb\n3UTvyadlwKNtA7gHuD/JGL0b31vnZGWSpBmbMhyq6nMn2XXVSfrvAnZNUh8Frpik/g5w/VTzkCSd\nOVOGgyTp9Flz87enPealWz91GmZyIr8+Q5LUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgO\nkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktQxq3BI8lKS/UmeSTLa\nah9I8liSH7efF/b1vyXJWJIXklzdV7+yvc9YkjuSZDbzkiTNzlycOfxqVa2vqg3t9c3A41W1Fni8\nvSbJOmArcDmwGbgzyZI25i7gRmBt2zbPwbwkSTN0Oi4rbQF2t/Zu4Nq++gNV9W5VvQiMARuTrASW\nV9WTVVXAfX1jJEnzYLbhUMB3kzydZEerraiqw639KrCitVcBr/SNPdRqq1p7sC5JmidLZzn+l6tq\nPMk/AB5L8qP+nVVVSWqWn/F3WgDtAPjgBz84V28rSRowqzOHqhpvP48A3wQ2Aq+1S0W0n0da93Hg\nkr7hq1ttvLUH65N93t1VtaGqNoyMjMxm6pKkU5hxOCR5X5JfnGgDvwY8B+wFtrVu24CHW3svsDXJ\n+UkupXfj+al2CertJJvaU0o39I2RJM2D2VxWWgF8sz11uhT4k6r6TpK/BPYk2Q68DHwWoKoOJNkD\nHASOATur6nh7r5uAe4FlwKNtkyTNkxmHQ1X9BPjwJPXXgatOMmYXsGuS+ihwxUznIkmaW/6GtCSp\nw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkds/3iPWnG1tz87Wn1f+nWT52mmUga5JmD\nJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpY8GEQ5LNSV5IMpbk5vmejySd\nyxZEOCRZAvxn4JPAOuBzSdbN76wk6dy1IMIB2AiMVdVPqupvgQeALfM8J0k6Zy2UcFgFvNL3+lCr\nSZLmwaL6VtYkO4Ad7eXPkrwww7e6GPjruZnVvDtn1pLbzuBMZuesPSaL6BhM5qw5LrltVmv5x8N0\nWijhMA5c0vd6daudoKruBu6e7YclGa2qDbN9n4XAtSw8Z8s6wLUsVGdiLQvlstJfAmuTXJrk7wFb\ngb3zPCdJOmctiDOHqjqW5N8C/x1YAny9qg7M87Qk6Zy1IMIBoKoeAR45Qx8360tTC4hrWXjOlnWA\na1moTvtaUlWn+zMkSYvMQrnnIElaQM7acEjy3iRPJflhkgNJvjJJnyS5o31lx7NJfmk+5jqVIdfy\nsSRvJXmmbf9+PuY6jCRLkvxVkm9Nsm9RHJMJU6xlMR2Tl5Lsb/McnWT/ojkuQ6xlURyXJO9P8lCS\nHyV5Psm/GNh/Wo/JgrnncBq8C3y8qn6W5Dzg+0keraon+/p8Eljbtn8O3NV+LjTDrAXgf1bVp+dh\nftP1JeB5YPkk+xbLMZlwqrXA4jkmAL9aVSd7dn6xHZdTrQUWx3H5D8B3quq69hTn3x/Yf1qPyVl7\n5lA9P2svz2vb4A2WLcB9re+TwPuTrDyT8xzGkGtZFJKsBj4F/NFJuiyKYwJDreVssmiOy9kgyQXA\nrwD3AFTV31bVTwe6ndZjctaGA/zdKf8zwBHgsaraN9Bl0XxtxxBrAfiX7fTy0SSXn+EpDusPgd8B\nfn6S/YvmmDD1WmBxHBPo/WPju0mebt9EMGgxHZep1gIL/7hcChwF/mu7bPlHSd430Oe0HpOzOhyq\n6nhVraf3G9cbk1wx33OaqSHW8gPgg1X1z4D/CPy3Mz3HqST5NHCkqp6e77nM1pBrWfDHpM8vt/++\nPgnsTPIr8z2hWZhqLYvhuCwFfgm4q6o+Avxv4Iz+KYOzOhwmtNOx7wGbB3YN9bUdC8nJ1lJVb09c\nemq/M3JekovnYYqn8lHg15O8RO+bdz+e5I8H+iyWYzLlWhbJMQGgqsbbzyPAN+l9U3K/xXJcplzL\nIjkuh4BDfVcIHqIXFv1O6zE5a8MhyUiS97f2MuATwI8Guu0Fbmh3/TcBb1XV4TM81SkNs5Yk/zBJ\nWnsjvWP7+pme66lU1S1Vtbqq1tD7ipQnquo3BrotimMyzFoWwzEBSPK+JL840QZ+DXhuoNuiOC7D\nrGUxHJeqehV4Jck/baWrgIMD3U7rMTmbn1ZaCexO7w8JvQfYU1XfSvKvAarqv9D7jexrgDHg/wC/\nOV+TncIwa7kO+DdJjgH/F9hai+Q3HBfpMZnUIj0mK4Bvtv9fLgX+pKq+s0iPyzBrWSzH5YvAN9qT\nSj8BfvNMHhN/Q1qS1HHWXlaSJM2c4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjr+H7vZ\nx9qUO1XrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x155cfe73208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(list(map(len,target_seqs)),bins=25);\n",
    "\n",
    "# Truncate names longer than MAX_LEN characters. This can be changed\n",
    "MAX_LEN = min([150,max(list(map(len, target_seqs)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast everything from symbols into matrix of int32. Pad with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_matrix(sequences, token_to_i, max_len=None, PAX_ix=-1):\n",
    "    \"\"\"\n",
    "    Converts several sequences of tokens to a matrix, edible a neural network.\n",
    "    Crops at max_len(if given), pads shorter sequences with -1 or PAD_ix.\n",
    "    \"\"\"\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences),max_len),dtype='int8') -1\n",
    "    for i,seq in enumerate(sequences):\n",
    "        \n",
    "        row_ix = [token_to_i.get(_, 0) for _ in seq[:max_len]]\n",
    "        matrix[i,:len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = T.matrix('token sequence', 'int32')\n",
    "output_sequence = T.matrix('target target_letters', 'int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##ENCODER\n",
    "l_in = InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "l_mask = InputLayer(shape=(None, None),input_var=T.neq(input_sequence,-1)) \n",
    "\n",
    "\n",
    "l_emb = non_trainable(EmbeddingLayer(l_in, len(source_letters),  len(source_letters), W=np.diag(np.ones(len(source_letters)))))\n",
    "l_rnn = LSTMLayer(l_emb, HIDDEN_SIZE, only_return_final=True, mask_input=l_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##DECODER\n",
    "dec_in = InputLayer(shape=(None, None),input_var=output_sequence)\n",
    "dec_mask = InputLayer(shape=(None, None),input_var=T.neq(output_sequence,-1))\n",
    "\n",
    "dec_emb = non_trainable(EmbeddingLayer(dec_in, len(target_letters), len(target_letters), W=np.diag(np.ones(len(target_letters)))))\n",
    "dec_rnn = LSTMLayer(dec_emb, num_units=HIDDEN_SIZE, cell_init=l_rnn, mask_input=dec_mask)\n",
    "# WARNING! if it's lstm use cell_init, not hid_init\n",
    "\n",
    "\n",
    "#flatten batch and time to be compatible with feedforward layers (will un-flatten later)\n",
    "dec_rnn_flat = reshape(dec_rnn, (-1,dec_rnn.output_shape[-1]))\n",
    "\n",
    "l_out = DenseLayer(dec_rnn_flat, len(target_letters), nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_model(nn, learning_rate=0.001):\n",
    "    # Model weights\n",
    "    weights = get_all_params(nn)\n",
    "    network_output = get_output(nn)\n",
    "    network_output = network_output.reshape([output_sequence.shape[0],\\\n",
    "                                         output_sequence.shape[1], -1])\n",
    "    predictions_flat = network_output[:,:-1,:].reshape([-1,len(target_letters)])\n",
    "    targets = output_sequence[:,1:].ravel()\n",
    "\n",
    "    #do not count loss for '-1' tokens\n",
    "    mask = T.nonzero(T.neq(targets,-1))\n",
    "\n",
    "    loss = lasagne.objectives.categorical_crossentropy(predictions_flat[mask], targets[mask]).mean()\n",
    "    updates = lasagne.updates.adam(loss, weights, learning_rate=learning_rate)\n",
    "    #training\n",
    "    train = theano.function([input_sequence, output_sequence], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    #computing loss without training\n",
    "    compute_cost = theano.function([input_sequence, output_sequence], loss, allow_input_downcast=True)\n",
    "    #compile the function that computes probabilities for next token given previous text.\n",
    "\n",
    "    last_probas =network_output[:, -1]\n",
    "\n",
    "    probs = theano.function([input_sequence, output_sequence], last_probas)\n",
    "    return train, compute_cost, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "We now need to implement a function that generates output sequence given input.\n",
    "\n",
    "Such function must work thusly:\n",
    "```\n",
    "Init:\n",
    "x = input\n",
    "y = [\"START\"]\n",
    "\n",
    "While not_too_long:\n",
    "  p(y_next|x,y) = probabilities of next letter for y\n",
    "  \n",
    "  y_next ~ p(y_next|x,y)\n",
    "  \n",
    "  y.append(y_next)\n",
    "  \n",
    "  if y_next == \"END\":\n",
    "      break\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_output(input, probs, target_letters, target_letter_to_ix, source_letter_to_ix,\n",
    "                    output_prefix = (\"START\",),\n",
    "                    END_token=\"END\",\n",
    "                    temperature=1,\n",
    "                    sample=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement a function that generates output sequence given input.\n",
    "    \n",
    "    We recommend (but not require) you to use the pseudo-code above and inline instructions.\n",
    "    \"\"\"\n",
    "    x = as_matrix([input], source_letter_to_ix) \n",
    "    output = list(output_prefix)\n",
    "    while True:\n",
    "        y = as_matrix([output], target_letter_to_ix)\n",
    "        next_y_probs = probs(x, y)\n",
    "        next_y_probs = (next_y_probs ** temperature) / (next_y_probs ** temperature).sum()\n",
    "        if sample:\n",
    "            next_y = np.random.choice(target_letters, p=next_y_probs[0])\n",
    "        else:\n",
    "            next_y = target_letters[next_y_probs[0].argmax()]\n",
    "        next_y = str(next_y)             \n",
    "        assert type(next_y) is str, \"please return token(string/character), not it's index\"\n",
    "        \n",
    "        output.append(next_y)\n",
    "\n",
    "        if next_y==END_token:\n",
    "            break\n",
    "            \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#source_seqs = np.array(source_seqs)\n",
    "#target_seqs = np.array(target_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(source_seqs, source_letter_to_ix, target_seqs, target_letter_to_ix, batch_size):\n",
    "    \"\"\"samples a random batch of source and target sequences, batch_size elements\"\"\"\n",
    "    batch_ix = np.random.randint(0,len(source_seqs),size=batch_size)\n",
    "    source_seqs_batch=as_matrix(source_seqs[batch_ix], source_letter_to_ix) \n",
    "    target_seqs_batch=as_matrix(target_seqs[batch_ix], target_letter_to_ix)\n",
    "    \n",
    "    return source_seqs_batch,target_seqs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train average loss = 0.08197419669138639\n",
      "Epoch 0 val average loss = 0.11527614729696303\n",
      "59+996 : 1055  |  1055\n",
      "534+873 : 1397  |  1407\n",
      "267+31 : 308  |  298\n",
      "5+515 : 520  |  520\n",
      "388+9 : 407  |  397\n",
      "Epoch 1 train average loss = 0.07638746983330547\n",
      "Epoch 1 val average loss = 0.11739519589507472\n",
      "272+5 : 277  |  277\n",
      "716+37 : 753  |  753\n",
      "681+81 : 762  |  762\n",
      "547+75 : 622  |  622\n",
      "29+857 : 886  |  886\n",
      "Epoch 2 train average loss = 0.07240082069820562\n",
      "Epoch 2 val average loss = 0.12240237601262764\n",
      "416+0 : 416  |  416\n",
      "803+670 : 1483  |  1473\n",
      "485+0 : 485  |  485\n",
      "97+406 : 503  |  503\n",
      "366+991 : 1357  |  1357\n",
      "Epoch 3 train average loss = 0.06832969206544318\n",
      "Epoch 3 val average loss = 0.09738800715904881\n",
      "924+232 : 1046  |  1156\n",
      "336+578 : 914  |  914\n",
      "798+963 : 1761  |  1761\n",
      "37+964 : 1001  |  1001\n",
      "549+66 : 615  |  615\n",
      "Epoch 4 train average loss = 0.06131126798776717\n",
      "Epoch 4 val average loss = 0.09616244593585298\n",
      "167+24 : 191  |  191\n",
      "62+89 : 151  |  151\n",
      "646+270 : 906  |  916\n",
      "1+952 : 953  |  953\n",
      "488+495 : 883  |  983\n",
      "Epoch 5 train average loss = 0.061328818428661364\n",
      "Epoch 5 val average loss = 0.0891181521692748\n",
      "639+3 : 642  |  642\n",
      "960+768 : 1728  |  1728\n",
      "454+301 : 755  |  755\n",
      "156+321 : 477  |  477\n",
      "26+434 : 460  |  460\n",
      "Epoch 6 train average loss = 0.056341027952983576\n",
      "Epoch 6 val average loss = 0.09772724745382756\n",
      "505+164 : 669  |  669\n",
      "16+231 : 247  |  247\n",
      "26+815 : 841  |  841\n",
      "777+158 : 935  |  935\n",
      "8+322 : 330  |  330\n",
      "Epoch 7 train average loss = 0.05554267240868105\n",
      "Epoch 7 val average loss = 0.09500756026966033\n",
      "552+65 : 617  |  617\n",
      "93+671 : 764  |  764\n",
      "198+35 : 233  |  233\n",
      "14+301 : 315  |  315\n",
      "630+74 : 694  |  704\n",
      "Epoch 8 train average loss = 0.049498748861292706\n",
      "Epoch 8 val average loss = 0.09078536681707769\n",
      "582+58 : 640  |  640\n",
      "947+162 : 1119  |  1109\n",
      "772+760 : 1532  |  1532\n",
      "332+644 : 976  |  976\n",
      "804+363 : 1167  |  1167\n",
      "Epoch 9 train average loss = 0.0466625551476665\n",
      "Epoch 9 val average loss = 0.07915094139023565\n",
      "86+446 : 532  |  532\n",
      "253+3 : 256  |  256\n",
      "417+94 : 511  |  511\n",
      "247+757 : 994  |  1004\n",
      "69+267 : 326  |  336\n",
      "Epoch 10 train average loss = 0.05303136426517701\n",
      "Epoch 10 val average loss = 0.09109121912388841\n",
      "165+698 : 863  |  863\n",
      "449+376 : 825  |  825\n",
      "959+19 : 988  |  978\n",
      "326+695 : 1021  |  1021\n",
      "16+169 : 185  |  185\n",
      "Epoch 11 train average loss = 0.04537219673986813\n",
      "Epoch 11 val average loss = 0.08914087321550763\n",
      "930+91 : 1022  |  1021\n",
      "15+71 : 86  |  86\n",
      "33+384 : 417  |  417\n",
      "942+227 : 1169  |  1169\n",
      "225+53 : 278  |  278\n",
      "Epoch 12 train average loss = 0.040925229031850224\n",
      "Epoch 12 val average loss = 0.06782731788134382\n",
      "76+821 : 907  |  897\n",
      "34+386 : 420  |  420\n",
      "425+21 : 446  |  446\n",
      "847+48 : 895  |  895\n",
      "93+68 : 161  |  161\n",
      "Epoch 13 train average loss = 0.044844899372401936\n",
      "Epoch 13 val average loss = 0.0715847643294914\n",
      "57+897 : 954  |  954\n",
      "668+30 : 698  |  698\n",
      "32+556 : 588  |  588\n",
      "48+389 : 437  |  437\n",
      "860+455 : 1315  |  1315\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=30\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "train_batches_per_epoch = 500\n",
    "val_batches_per_epoch = 50\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "train_source_seqs, val_source_seqs, train_target_seqs, val_target_seqs = train_test_split(source_seqs, target_seqs,\\\n",
    "                                                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        train_avg_cost = 0;\n",
    "        val_avg_cost = 0;\n",
    "\n",
    "        for _ in range(train_batches_per_epoch):\n",
    "            x,y = sample_batch(train_source_seqs, source_letter_to_ix, train_target_seqs, target_letter_to_ix, batch_size)\n",
    "            train_avg_cost += train(x, y).mean()\n",
    "        \n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            x,y = sample_batch(val_source_seqs, source_letter_to_ix, val_target_seqs, target_letter_to_ix, batch_size)\n",
    "            val_avg_cost += compute_cost(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} train average loss = {}\".format(epoch, train_avg_cost / train_batches_per_epoch))\n",
    "        print(\"Epoch {} val average loss = {}\".format(epoch, val_avg_cost / val_batches_per_epoch))\n",
    "        \n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(val_source_seqs))\n",
    "            print (val_source_seqs[ind],':', ''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(val_target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bazal module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CerMemory(lasagne.layers.Layer):\n",
    "    def __init__(self, incoming, memory_size, M=lasagne.init.Orthogonal(), **kwargs):\n",
    "        super(CerMemory, self).__init__(incoming, **kwargs)\n",
    "        self.query_shape = self.input_shape[1]\n",
    "        self.memory_size = memory_size\n",
    "        self.M = self.add_param(M, (self.query_shape, memory_size), name='M')\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        m = self.M / T.sqrt(T.sqr(self.M).sum(axis=0)).reshape(self.M.shape[1], 1)\n",
    "        weights =  T.dot(input, m)\n",
    "        return T.dot(weights, m.T)\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], self.query_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EvcNormalizer(lasagne.layers.Layer):\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return (input.T / T.sqrt(T.sqr(input).sum(axis=1)).reshape(input.shape[0], 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_SIZE = 16\n",
    "MEMORY_SIZE = 64\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bazal_model(query_size, memory_size, hidden_size, memory_benchmark=False):\n",
    "\n",
    "    ##ENCODER\n",
    "    l_in = InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "    l_mask = InputLayer(shape=(None, None),input_var=T.neq(input_sequence,-1)) \n",
    "\n",
    "\n",
    "    l_emb = non_trainable(EmbeddingLayer(l_in, len(source_letters),  len(source_letters), W=np.diag(np.ones(len(source_letters)))))\n",
    "    features = LSTMLayer(l_emb, HIDDEN_SIZE, only_return_final=True, mask_input=l_mask)\n",
    "    \n",
    "    if not memory_benchmark:\n",
    "        ## QUERY BUILDER\n",
    "        query = DenseLayer(features, QUERY_SIZE, nonlinearity=None)\n",
    "        query = EvcNormalizer(query)\n",
    "        ## Memory\n",
    "        memory = CerMemory(query, MEMORY_SIZE)\n",
    "    else:\n",
    "        memory = DenseLayer(DenseLayer(features, QUERY_SIZE), QUERY_SIZE)\n",
    "        \n",
    "    to_decode = ConcatLayer([features, memory])\n",
    "    \n",
    "    ##DECODER\n",
    "    dec_in = InputLayer(shape=(None, None),input_var=output_sequence)\n",
    "    dec_mask = InputLayer(shape=(None, None),input_var=T.neq(output_sequence,-1))\n",
    "\n",
    "    dec_emb = non_trainable(EmbeddingLayer(dec_in, len(target_letters), len(target_letters), W=np.diag(np.ones(len(target_letters)))))\n",
    "    dec_rnn = LSTMLayer(dec_emb, num_units=to_decode.output_shape[-1], cell_init=to_decode, mask_input=dec_mask)\n",
    "    # WARNING! if it's lstm use cell_init, not hid_init\n",
    "\n",
    "\n",
    "    #flatten batch and time to be compatible with feedforward layers (will un-flatten later)\n",
    "    dec_rnn_flat = reshape(dec_rnn, (-1,dec_rnn.output_shape[-1]))\n",
    "\n",
    "    l_out = DenseLayer(dec_rnn_flat, len(target_letters), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out, memory = bazal_model(QUERY_SIZE, MEMORY_SIZE, HIDDEN_SIZE, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train average loss = 1.703139966258563\n",
      "Epoch 0 val average loss = 1.5361868813827226\n",
      "16+192 : 616  |  208\n",
      "383+915 : 1500  |  1298\n",
      "552+38 : 778  |  590\n",
      "201+179 : 958  |  380\n",
      "26+244 : 380  |  270\n",
      "Epoch 1 train average loss = 1.3784032784319356\n",
      "Epoch 1 val average loss = 1.2626754988447817\n",
      "565+978 : 1508  |  1543\n",
      "439+47 : 548  |  486\n",
      "853+534 : 1850  |  1387\n",
      "2+476 : 663  |  478\n",
      "656+7 : 660  |  663\n",
      "Epoch 2 train average loss = 1.18885158952572\n",
      "Epoch 2 val average loss = 1.1272228235305186\n",
      "891+84 : 957  |  975\n",
      "3+260 : 223  |  263\n",
      "485+49 : 537  |  534\n",
      "80+667 : 752  |  747\n",
      "98+45 : 139  |  143\n",
      "Epoch 3 train average loss = 1.068614112183761\n",
      "Epoch 3 val average loss = 1.0230188550521002\n",
      "772+97 : 860  |  869\n",
      "194+162 : 338  |  356\n",
      "136+15 : 105  |  151\n",
      "917+79 : 1016  |  996\n",
      "349+344 : 798  |  693\n",
      "Epoch 4 train average loss = 0.9727485846004202\n",
      "Epoch 4 val average loss = 0.9426264083779726\n",
      "559+844 : 1427  |  1403\n",
      "825+93 : 886  |  918\n",
      "267+76 : 356  |  343\n",
      "698+9 : 794  |  707\n",
      "985+3 : 959  |  988\n",
      "Epoch 5 train average loss = 0.902198971727695\n",
      "Epoch 5 val average loss = 0.8718836626727373\n",
      "654+861 : 1483  |  1515\n",
      "497+449 : 971  |  946\n",
      "4+809 : 817  |  813\n",
      "70+535 : 597  |  605\n",
      "519+905 : 1456  |  1424\n",
      "Epoch 6 train average loss = 0.8414067559617658\n",
      "Epoch 6 val average loss = 0.8252792487124664\n",
      "73+185 : 247  |  258\n",
      "974+206 : 1159  |  1180\n",
      "493+89 : 565  |  582\n",
      "49+334 : 397  |  383\n",
      "45+813 : 854  |  858\n",
      "Epoch 7 train average loss = 0.7889526997687042\n",
      "Epoch 7 val average loss = 0.7806911125399735\n",
      "603+33 : 634  |  636\n",
      "348+16 : 361  |  364\n",
      "346+37 : 378  |  383\n",
      "165+821 : 980  |  986\n",
      "45+319 : 366  |  364\n",
      "Epoch 8 train average loss = 0.7432101365650262\n",
      "Epoch 8 val average loss = 0.7421034455583262\n",
      "20+445 : 460  |  465\n",
      "72+581 : 648  |  653\n",
      "432+83 : 512  |  515\n",
      "644+74 : 721  |  718\n",
      "31+91 : 122  |  122\n",
      "Epoch 9 train average loss = 0.7055844888884129\n",
      "Epoch 9 val average loss = 0.7349520674335903\n",
      "93+683 : 778  |  776\n",
      "162+614 : 773  |  776\n",
      "178+149 : 325  |  327\n",
      "78+43 : 123  |  121\n",
      "52+876 : 918  |  928\n",
      "Epoch 10 train average loss = 0.6662682878655414\n",
      "Epoch 10 val average loss = 0.6685931335315233\n",
      "41+59 : 101  |  100\n",
      "889+7 : 897  |  896\n",
      "522+826 : 1336  |  1348\n",
      "88+506 : 595  |  594\n",
      "193+51 : 248  |  244\n",
      "Epoch 11 train average loss = 0.6255346896533583\n",
      "Epoch 11 val average loss = 0.6417344612291825\n",
      "14+792 : 806  |  806\n",
      "5+625 : 630  |  630\n",
      "43+472 : 517  |  515\n",
      "82+795 : 873  |  877\n",
      "28+958 : 985  |  986\n",
      "Epoch 12 train average loss = 0.5858179734853382\n",
      "Epoch 12 val average loss = 0.5742294925668009\n",
      "35+687 : 721  |  722\n",
      "87+706 : 789  |  793\n",
      "822+1 : 823  |  823\n",
      "329+349 : 673  |  678\n",
      "2+980 : 982  |  982\n",
      "Epoch 13 train average loss = 0.5192592871141377\n",
      "Epoch 13 val average loss = 0.5009546193198036\n",
      "135+424 : 471  |  559\n",
      "994+5 : 994  |  999\n",
      "162+614 : 777  |  776\n",
      "1+9 : 29  |  10\n",
      "956+545 : 1514  |  1501\n",
      "Epoch 14 train average loss = 0.4246423960370041\n",
      "Epoch 14 val average loss = 0.3979274766139897\n",
      "924+574 : 1498  |  1498\n",
      "510+17 : 527  |  527\n",
      "776+442 : 1220  |  1218\n",
      "811+7 : 820  |  818\n",
      "47+524 : 571  |  571\n",
      "Epoch 15 train average loss = 0.33068487431024557\n",
      "Epoch 15 val average loss = 0.323623196425522\n",
      "333+3 : 344  |  336\n",
      "60+538 : 598  |  598\n",
      "695+521 : 1236  |  1216\n",
      "86+777 : 857  |  863\n",
      "975+996 : 1970  |  1971\n",
      "Epoch 16 train average loss = 0.2707703657913868\n",
      "Epoch 16 val average loss = 0.27673567507477875\n",
      "949+231 : 1180  |  1180\n",
      "3+93 : 98  |  96\n",
      "68+264 : 332  |  332\n",
      "734+23 : 757  |  757\n",
      "580+81 : 671  |  661\n",
      "Epoch 17 train average loss = 0.22222907370447426\n",
      "Epoch 17 val average loss = 0.22049952508067494\n",
      "773+0 : 774  |  773\n",
      "96+155 : 251  |  251\n",
      "70+56 : 126  |  126\n",
      "252+2 : 253  |  254\n",
      "94+509 : 603  |  603\n",
      "Epoch 18 train average loss = 0.18582503960142607\n",
      "Epoch 18 val average loss = 0.18668340804920022\n",
      "9+237 : 236  |  246\n",
      "978+882 : 1859  |  1860\n",
      "724+65 : 790  |  789\n",
      "85+595 : 680  |  680\n",
      "314+14 : 328  |  328\n",
      "Epoch 19 train average loss = 0.15912343437023627\n",
      "Epoch 19 val average loss = 0.17643251544187916\n",
      "40+756 : 796  |  796\n",
      "592+244 : 827  |  836\n",
      "316+74 : 390  |  390\n",
      "33+109 : 141  |  142\n",
      "350+15 : 365  |  365\n",
      "Epoch 20 train average loss = 0.1395940149691874\n",
      "Epoch 20 val average loss = 0.14374563939541107\n",
      "42+162 : 194  |  204\n",
      "951+625 : 1676  |  1576\n",
      "873+7 : 880  |  880\n",
      "83+302 : 385  |  385\n",
      "4+179 : 183  |  183\n",
      "Epoch 21 train average loss = 0.12056768929079464\n",
      "Epoch 21 val average loss = 0.12721099417294743\n",
      "811+774 : 1585  |  1585\n",
      "271+220 : 491  |  491\n",
      "165+821 : 986  |  986\n",
      "690+591 : 1272  |  1281\n",
      "114+60 : 174  |  174\n",
      "Epoch 22 train average loss = 0.10934801193185728\n",
      "Epoch 22 val average loss = 0.1302397312770682\n",
      "466+1 : 467  |  467\n",
      "3+376 : 379  |  379\n",
      "5+44 : 50  |  49\n",
      "994+58 : 1052  |  1052\n",
      "404+0 : 404  |  404\n",
      "Epoch 23 train average loss = 0.09918915293474896\n",
      "Epoch 23 val average loss = 0.11652031405787719\n",
      "56+709 : 765  |  765\n",
      "274+8 : 282  |  282\n",
      "281+554 : 835  |  835\n",
      "712+610 : 1311  |  1322\n",
      "144+299 : 443  |  443\n",
      "Epoch 24 train average loss = 0.0880005032166064\n",
      "Epoch 24 val average loss = 0.10005494827784045\n",
      "523+8 : 521  |  531\n",
      "629+45 : 674  |  674\n",
      "3+886 : 889  |  889\n",
      "395+600 : 995  |  995\n",
      "282+809 : 1091  |  1091\n",
      "Epoch 25 train average loss = 0.08500895412408237\n",
      "Epoch 25 val average loss = 0.09998148784539797\n",
      "342+402 : 744  |  744\n",
      "118+655 : 773  |  773\n",
      "58+4 : 63  |  62\n",
      "13+59 : 72  |  72\n",
      "90+45 : 135  |  135\n",
      "Epoch 26 train average loss = 0.07111023061893246\n",
      "Epoch 26 val average loss = 0.11281337281092466\n",
      "43+44 : 87  |  87\n",
      "21+936 : 957  |  957\n",
      "4+494 : 498  |  498\n",
      "79+41 : 120  |  120\n",
      "613+428 : 1041  |  1041\n",
      "Epoch 27 train average loss = 0.0701958390731548\n",
      "Epoch 27 val average loss = 0.08537755336769157\n",
      "0+162 : 162  |  162\n",
      "68+87 : 155  |  155\n",
      "657+821 : 1478  |  1478\n",
      "514+16 : 530  |  530\n",
      "788+34 : 822  |  822\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=30\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "train_batches_per_epoch = 500\n",
    "val_batches_per_epoch = 50\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "train_source_seqs, val_source_seqs, train_target_seqs, val_target_seqs = train_test_split(source_seqs, target_seqs,\\\n",
    "                                                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        train_avg_cost = 0;\n",
    "        val_avg_cost = 0;\n",
    "\n",
    "        for _ in range(train_batches_per_epoch):\n",
    "            x,y = sample_batch(train_source_seqs, source_letter_to_ix, train_target_seqs, target_letter_to_ix, batch_size)\n",
    "            train_avg_cost += train(x, y).mean()\n",
    "        \n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            x,y = sample_batch(val_source_seqs, source_letter_to_ix, val_target_seqs, target_letter_to_ix, batch_size)\n",
    "            val_avg_cost += compute_cost(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} train average loss = {}\".format(epoch, train_avg_cost / train_batches_per_epoch))\n",
    "        print(\"Epoch {} val average loss = {}\".format(epoch, val_avg_cost / val_batches_per_epoch))\n",
    "        \n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(val_source_seqs))\n",
    "            print (val_source_seqs[ind],':', ''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(val_target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M_add = np.array(memory.M.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./zoo/addprob_memory_after30epochs.npy', M_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same for prod problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 100000\n",
    "DIGITS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 100000\n"
     ]
    }
   ],
   "source": [
    "source_seqs, source_letters, source_letter_to_ix, target_seqs, target_letters, target_letter_to_ix =\\\n",
    "                    generate_data(TRAINING_SIZE, DIGITS, lambda a, b: a * b, '{}*{}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8*81 : 648\n",
      "2*90 : 180\n",
      "6*45 : 270\n",
      "6*59 : 354\n",
      "1*83 : 83\n"
     ]
    }
   ],
   "source": [
    "for source, target in zip(source_seqs[:5],target_seqs[:5]):\n",
    "    print( source,':',\"\".join(target[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_SIZE = 16\n",
    "MEMORY_SIZE = 64\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out, memory = bazal_model(QUERY_SIZE, MEMORY_SIZE, HIDDEN_SIZE,  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train average loss = 0.4745134306164804\n",
      "Epoch 0 val average loss = 0.5041249347773323\n",
      "269*629 : 170221  |  169201\n",
      "590*943 : 558570  |  556370\n",
      "714*80 : 57240  |  57120\n",
      "69*852 : 58768  |  58788\n",
      "38*897 : 34214  |  34086\n",
      "Epoch 1 train average loss = 0.47013104617667933\n",
      "Epoch 1 val average loss = 0.49574675095872833\n",
      "0*561 : 0  |  0\n",
      "24*894 : 21296  |  21456\n",
      "104*71 : 7504  |  7384\n",
      "317*780 : 243860  |  247260\n",
      "96*949 : 89904  |  91104\n",
      "Epoch 2 train average loss = 0.46767226161750364\n",
      "Epoch 2 val average loss = 0.5003663286992424\n",
      "269*37 : 9963  |  9953\n",
      "627*618 : 389326  |  387486\n",
      "46*67 : 3082  |  3082\n",
      "472*73 : 34216  |  34456\n",
      "653*4 : 2612  |  2612\n",
      "Epoch 3 train average loss = 0.4647098757192144\n",
      "Epoch 3 val average loss = 0.4996647706121669\n",
      "213*638 : 135754  |  135894\n",
      "235*681 : 158895  |  160035\n",
      "387*865 : 332905  |  334755\n",
      "87*915 : 79735  |  79605\n",
      "332*45 : 15040  |  14940\n",
      "Epoch 4 train average loss = 0.4655186190564228\n",
      "Epoch 4 val average loss = 0.49633644803236115\n",
      "169*9 : 1521  |  1521\n",
      "211*419 : 89409  |  88409\n",
      "78*130 : 10220  |  10140\n",
      "59*853 : 49887  |  50327\n",
      "104*98 : 10192  |  10192\n",
      "Epoch 5 train average loss = 0.46131435022733297\n",
      "Epoch 5 val average loss = 0.4970284882868481\n",
      "586*7 : 4082  |  4102\n",
      "360*717 : 260520  |  258120\n",
      "665*79 : 51885  |  52535\n",
      "71*133 : 9513  |  9443\n",
      "889*965 : 846885  |  857885\n",
      "Epoch 6 train average loss = 0.4592023265201259\n",
      "Epoch 6 val average loss = 0.5033288066275088\n",
      "852*108 : 90808  |  92016\n",
      "115*37 : 4205  |  4255\n",
      "64*787 : 50608  |  50368\n",
      "615*90 : 55650  |  55350\n",
      "277*207 : 56619  |  57339\n",
      "Epoch 7 train average loss = 0.4583375978835891\n",
      "Epoch 7 val average loss = 0.4938096838270399\n",
      "819*25 : 20375  |  20475\n",
      "312*0 : 0  |  0\n",
      "963*9 : 8627  |  8667\n",
      "25*519 : 13025  |  12975\n",
      "139*781 : 108579  |  108559\n",
      "Epoch 8 train average loss = 0.4561200970289777\n",
      "Epoch 8 val average loss = 0.4944535387414766\n",
      "698*125 : 87550  |  87250\n",
      "876*266 : 233496  |  233016\n",
      "50*433 : 21550  |  21650\n",
      "837*927 : 771909  |  775899\n",
      "240*486 : 116240  |  116640\n",
      "Epoch 9 train average loss = 0.45597242433899166\n",
      "Epoch 9 val average loss = 0.49868918148056957\n",
      "470*243 : 113810  |  114210\n",
      "847*28 : 23836  |  23716\n",
      "389*31 : 12139  |  12059\n",
      "693*78 : 54174  |  54054\n",
      "798*40 : 31840  |  31920\n",
      "Epoch 10 train average loss = 0.45412228417416345\n",
      "Epoch 10 val average loss = 0.5019495833901072\n",
      "122*53 : 6386  |  6466\n",
      "370*38 : 14060  |  14060\n",
      "7*953 : 6701  |  6671\n",
      "69*564 : 38796  |  38916\n",
      "815*48 : 39320  |  39120\n",
      "Epoch 11 train average loss = 0.4517529170206729\n",
      "Epoch 11 val average loss = 0.49134978773468424\n",
      "700*28 : 19600  |  19600\n",
      "490*793 : 392170  |  388570\n",
      "617*518 : 318986  |  319606\n",
      "29*78 : 2242  |  2262\n",
      "41*569 : 23409  |  23329\n",
      "Epoch 12 train average loss = 0.4541804701572374\n",
      "Epoch 12 val average loss = 0.4889870143140424\n",
      "917*233 : 215011  |  213661\n",
      "575*5 : 2875  |  2875\n",
      "193*40 : 7840  |  7720\n",
      "14*248 : 3456  |  3472\n",
      "843*73 : 61749  |  61539\n",
      "Epoch 13 train average loss = 0.4516195361120608\n",
      "Epoch 13 val average loss = 0.4896484762187959\n",
      "53*214 : 11362  |  11342\n",
      "912*43 : 38876  |  39216\n",
      "75*969 : 73025  |  72675\n",
      "436*88 : 38368  |  38368\n",
      "580*782 : 455160  |  453560\n",
      "Epoch 14 train average loss = 0.4500939303421067\n",
      "Epoch 14 val average loss = 0.49557077823763207\n",
      "34*460 : 15640  |  15640\n",
      "34*129 : 4366  |  4386\n",
      "44*334 : 14696  |  14696\n",
      "756*65 : 48840  |  49140\n",
      "735*146 : 107830  |  107310\n",
      "Epoch 15 train average loss = 0.4473323403779041\n",
      "Epoch 15 val average loss = 0.4845798003150751\n",
      "121*27 : 3307  |  3267\n",
      "73*616 : 44748  |  44968\n",
      "357*27 : 9723  |  9639\n",
      "92*232 : 21184  |  21344\n",
      "445*24 : 10640  |  10680\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=1000\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "train_batches_per_epoch = 500\n",
    "val_batches_per_epoch = 50\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "train_source_seqs, val_source_seqs, train_target_seqs, val_target_seqs = train_test_split(source_seqs, target_seqs,\\\n",
    "                                                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        train_avg_cost = 0;\n",
    "        val_avg_cost = 0;\n",
    "\n",
    "        for _ in range(train_batches_per_epoch):\n",
    "            x,y = sample_batch(train_source_seqs, source_letter_to_ix, train_target_seqs, target_letter_to_ix, batch_size)\n",
    "            train_avg_cost += train(x, y).mean()\n",
    "        \n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            x,y = sample_batch(val_source_seqs, source_letter_to_ix, val_target_seqs, target_letter_to_ix, batch_size)\n",
    "            val_avg_cost += compute_cost(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} train average loss = {}\".format(epoch, train_avg_cost / train_batches_per_epoch))\n",
    "        print(\"Epoch {} val average loss = {}\".format(epoch, val_avg_cost / val_batches_per_epoch))\n",
    "        \n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(val_source_seqs))\n",
    "            print (val_source_seqs[ind],':', ''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(val_target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grand problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob1, prob_op1 = lambda a, b, c: (a + b) * c, '({}+{})*{}'\n",
    "prob2, prob_op2 = lambda a, b, c:  a * (b + c), '{}*({}+{})'\n",
    "prob3, prob_op3 = lambda a, b, c:  a + b * c, '{}+{}*{}'\n",
    "prob4, prob_op4 = lambda a, b, c:  a * b + c, '{}*{}+{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_grand_data(size, digits, problems = [prob1, prob2, prob3, prob4],\\\n",
    "                        problem_operators=[prob_op1, prob_op2, prob_op3, prob_op4]):\n",
    "    source_seqs = []\n",
    "    target_seqs = []\n",
    "    seen = set()\n",
    "    assert len(problem_operators) == len(problems)\n",
    "    print('Generating data...')\n",
    "    while len(source_seqs) < TRAINING_SIZE:\n",
    "        f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                        for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "        a, b, c = f(), f(), f()\n",
    "        # Skip any addition questions we've already seen\n",
    "        # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "        key = tuple(sorted((a, b, c)))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        \n",
    "        coin = np.random.randint(0, len(problems))\n",
    "        \n",
    "        # Pad the data with spaces such that it is always MAXLEN.\n",
    "        q = problem_operators[coin].format(a, b, c)\n",
    "        ans = str(problems[coin](a, b, c))\n",
    "\n",
    "        source_seqs.append(q)\n",
    "        target_seqs.append([\"START\"] + list(ans) + [\"END\"])\n",
    "\n",
    "    print('Total addition questions:', len(source_seqs))\n",
    "    \n",
    "    target_letters = list(set([token for ts in target_seqs for token in ts]))\n",
    "    target_letter_to_ix = {ph:i for i,ph in enumerate(target_letters)}\n",
    "    \n",
    "    source_letters = list(set([token for word in source_seqs for token in word]))\n",
    "    source_letter_to_ix = {l:i for i,l in enumerate(source_letters)}\n",
    "    \n",
    "    return np.array(source_seqs), source_letters, source_letter_to_ix, \\\n",
    "           np.array(target_seqs), target_letters, target_letter_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 2000\n",
    "DIGITS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 2000\n"
     ]
    }
   ],
   "source": [
    "source_seqs, source_letters, source_letter_to_ix, target_seqs, target_letters, target_letter_to_ix =\\\n",
    "                    generate_grand_data(TRAINING_SIZE, DIGITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93+3*79 : 330\n",
      "2*0+30 : 30\n",
      "1*(73+45) : 118\n",
      "6*(3+67) : 420\n",
      "69*8+3 : 555\n"
     ]
    }
   ],
   "source": [
    "for source, target in zip(source_seqs[:5],target_seqs[:5]):\n",
    "    print( source,':',\"\".join(target[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_SIZE = 16\n",
    "MEMORY_SIZE = 64\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out, memory = bazal_model(QUERY_SIZE, MEMORY_SIZE, HIDDEN_SIZE,  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 average loss = 1.8074171794116005\n",
      "3*(54+8) : 839  |  186\n",
      "44*7+7 : 229  |  315\n",
      "57+15*4 : 218  |  117\n",
      "43*1+2 : 108  |  45\n",
      "27*2+87 : 608  |  141\n",
      "Epoch 1 average loss = 1.6196723853008468\n",
      "18+7*62 : 303  |  452\n",
      "48+7*4 : 98  |  76\n",
      "8+89*8 : 403  |  720\n",
      "2*(59+96) : 269  |  310\n",
      "(58+6)*8 : 880  |  512\n",
      "Epoch 2 average loss = 1.4874540646206253\n",
      "92+53*92 : 5099  |  4968\n",
      "(78+75)*87 : 7765  |  13311\n",
      "54*(25+0) : 1452  |  1350\n",
      "(1+0)*64 : 40  |  64\n",
      "0*7+1 : 3  |  1\n",
      "Epoch 3 average loss = 1.3032164222313742\n",
      "97*(3+18) : 2709  |  2037\n",
      "57*39+80 : 4453  |  2303\n",
      "74*4+7 : 359  |  303\n",
      "0*(78+6) : 0  |  0\n",
      "(87+7)*48 : 2318  |  4512\n",
      "Epoch 4 average loss = 1.0245313576234003\n",
      "7*8+75 : 103  |  131\n",
      "6+6*93 : 469  |  564\n",
      "1+61*5 : 419  |  306\n",
      "2+59*30 : 2598  |  1772\n",
      "39*42+95 : 1373  |  1733\n",
      "Epoch 5 average loss = 0.6894229277183287\n",
      "74*(2+60) : 1428  |  4588\n",
      "9*6+23 : 79  |  77\n",
      "47*(49+2) : 4234  |  2397\n",
      "7+8*5 : 30  |  47\n",
      "2*69+98 : 208  |  236\n",
      "Epoch 6 average loss = 0.4108774484056355\n",
      "1+20*0 : 4  |  1\n",
      "15*44+65 : 320  |  725\n",
      "36+42*74 : 6432  |  3144\n",
      "93*62+88 : 4708  |  5854\n",
      "(75+92)*9 : 1583  |  1503\n",
      "Epoch 7 average loss = 0.21918465745583113\n",
      "7*52+36 : 400  |  400\n",
      "64+4*43 : 366  |  236\n",
      "52+56*0 : 52  |  52\n",
      "22*(6+44) : 1100  |  1100\n",
      "(85+36)*7 : 614  |  847\n",
      "Epoch 8 average loss = 0.11009148939910773\n",
      "(49+1)*31 : 1550  |  1550\n",
      "(8+1)*87 : 783  |  783\n",
      "21*(11+73) : 1764  |  1764\n",
      "(9+6)*88 : 1320  |  1320\n",
      "11*94+0 : 1034  |  1034\n",
      "Epoch 9 average loss = 0.053998595392924356\n",
      "(55+3)*77 : 4466  |  4466\n",
      "32*8+52 : 270  |  308\n",
      "7*(6+59) : 455  |  455\n",
      "21*(5+1) : 126  |  126\n",
      "19*2+4 : 42  |  42\n",
      "Epoch 10 average loss = 0.08680151892880913\n",
      "63*0+9 : 9  |  9\n",
      "(52+8)*9 : 510  |  540\n",
      "5*6+43 : 73  |  73\n",
      "(96+56)*9 : 1368  |  1368\n",
      "45*14+80 : 710  |  710\n",
      "Epoch 11 average loss = 0.019791177655911787\n",
      "4*(9+14) : 92  |  92\n",
      "55+8*6 : 103  |  103\n",
      "10*(33+6) : 390  |  390\n",
      "30+4*9 : 66  |  66\n",
      "(3+50)*1 : 53  |  53\n",
      "Epoch 12 average loss = 0.012232270473054021\n",
      "63*76+6 : 4794  |  4794\n",
      "(58+93)*2 : 302  |  302\n",
      "36+42*74 : 3144  |  3144\n",
      "68*4+3 : 275  |  275\n",
      "1+10*7 : 71  |  71\n",
      "Epoch 13 average loss = 0.008034974855735894\n",
      "69+2*0 : 69  |  69\n",
      "(3+13)*5 : 80  |  80\n",
      "26*(4+5) : 234  |  234\n",
      "89*(3+8) : 979  |  979\n",
      "(5+71)*38 : 2888  |  2888\n",
      "Epoch 14 average loss = 0.005506300733214235\n",
      "(0+46)*4 : 184  |  184\n",
      "5*(93+0) : 465  |  465\n",
      "97+93*4 : 469  |  469\n",
      "56+60*6 : 416  |  416\n",
      "73+16*8 : 201  |  201\n",
      "Epoch 15 average loss = 0.09774490923958623\n",
      "1*(8+60) : 68  |  68\n",
      "10*76+43 : 803  |  803\n",
      "69+68*0 : 69  |  69\n",
      "(0+34)*0 : 0  |  0\n",
      "63*78+50 : 4964  |  4964\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=30\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "train_batches_per_epoch = 500\n",
    "val_batches_per_epoch = 50\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "train_source_seqs, val_source_seqs, train_target_seqs, val_target_seqs = train_test_split(source_seqs, target_seqs,\\\n",
    "                                                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        train_avg_cost = 0;\n",
    "        val_avg_cost = 0;\n",
    "\n",
    "        for _ in range(train_batches_per_epoch):\n",
    "            x,y = sample_batch(train_source_seqs, source_letter_to_ix, train_target_seqs, target_letter_to_ix, batch_size)\n",
    "            train_avg_cost += train(x, y).mean()\n",
    "        \n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            x,y = sample_batch(val_source_seqs, source_letter_to_ix, val_target_seqs, target_letter_to_ix, batch_size)\n",
    "            val_avg_cost += compute_cost(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} train average loss = {}\".format(epoch, train_avg_cost / train_batches_per_epoch))\n",
    "        print(\"Epoch {} val average loss = {}\".format(epoch, val_avg_cost / val_batches_per_epoch))\n",
    "        \n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(val_source_seqs))\n",
    "            print (val_source_seqs[ind],':', ''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(val_target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
