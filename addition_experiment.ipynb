{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition experiment\n",
    "\n",
    "An implementation of sequence to sequence learning for performing addition\n",
    "\n",
    "* Input: \"535+61\"\n",
    "\n",
    "* Output: \"596\"\n",
    "\n",
    "Padding is handled by using a repeated sentinel character (space)\n",
    "Input may optionally be inverted, shown to increase performance in many tasks in:\n",
    "\"Learning to Execute\"\n",
    "http://arxiv.org/abs/1410.4615\n",
    "and\n",
    "\"Sequence to Sequence Learning with Neural Networks\"\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "Theoretically it introduces shorter term dependencies between source and target.\n",
    "Two digits inverted:\n",
    "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
    "Three digits inverted:\n",
    "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
    "Four digits inverted:\n",
    "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
    "Five digits inverted:\n",
    "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:\n",
      "b'C:\\\\Users\\\\tumanov\\\\AppData\\\\Local\\\\Temp\\\\try_flags_a5k1ufkw.c:4:19: fatal error: cudnn.h: No such file or directory\\r\\ncompilation terminated.\\r\\n'\n",
      "Mapped name None to device cuda0: GeForce GTX 1070 (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_trainable(net):\n",
    "    for tags in net.params.values():\n",
    "        tags -= {'trainable', 'regularizable'}\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "INVERT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try replacing GRU, or SimpleRNN.\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(size, digits, problem = lambda a, b: a+b, problem_operator='{}+{}'):\n",
    "    source_seqs = []\n",
    "    target_seqs = []\n",
    "    seen = set()\n",
    "    print('Generating data...')\n",
    "    while len(source_seqs) < TRAINING_SIZE:\n",
    "        f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                        for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "        a, b = f(), f()\n",
    "        # Skip any addition questions we've already seen\n",
    "        # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "        key = tuple(sorted((a, b)))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        # Pad the data with spaces such that it is always MAXLEN.\n",
    "        q = problem_operator.format(a, b)\n",
    "        ans = str(problem(a, b))\n",
    "\n",
    "        source_seqs.append(q)\n",
    "        target_seqs.append([\"START\"] + list(ans) + [\"END\"])\n",
    "\n",
    "    print('Total addition questions:', len(source_seqs))\n",
    "    \n",
    "    target_letters = list(set([token for ts in target_seqs for token in ts]))\n",
    "    target_letter_to_ix = {ph:i for i,ph in enumerate(target_letters)}\n",
    "    \n",
    "    source_letters = list(set([token for word in source_seqs for token in word]))\n",
    "    source_letter_to_ix = {l:i for i,l in enumerate(source_letters)}\n",
    "    \n",
    "    return np.array(source_seqs), source_letters, source_letter_to_ix, \\\n",
    "           np.array(target_seqs), target_letters, target_letter_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n"
     ]
    }
   ],
   "source": [
    "source_seqs, source_letters, source_letter_to_ix, target_seqs, target_letters, target_letter_to_ix =\\\n",
    "                    generate_data(TRAINING_SIZE, DIGITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+90 : 90\n",
      "452+917 : 1369\n",
      "98+8 : 106\n",
      "329+317 : 646\n",
      "59+84 : 143\n"
     ]
    }
   ],
   "source": [
    "for source, target in zip(source_seqs[:5],target_seqs[:5]):\n",
    "    print( source,':',\"\".join(target[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1447ba539b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(list(map(len,target_seqs)),bins=25);\n",
    "\n",
    "# Truncate names longer than MAX_LEN characters. This can be changed\n",
    "MAX_LEN = min([150,max(list(map(len, target_seqs)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast everything from symbols into matrix of int32. Pad with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_matrix(sequences, token_to_i, max_len=None, PAX_ix=-1):\n",
    "    \"\"\"\n",
    "    Converts several sequences of tokens to a matrix, edible a neural network.\n",
    "    Crops at max_len(if given), pads shorter sequences with -1 or PAD_ix.\n",
    "    \"\"\"\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences),max_len),dtype='int8') -1\n",
    "    for i,seq in enumerate(sequences):\n",
    "        \n",
    "        row_ix = [token_to_i.get(_, 0) for _ in seq[:max_len]]\n",
    "        matrix[i,:len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = T.matrix('token sequence', 'int32')\n",
    "output_sequence = T.matrix('target target_letters', 'int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##ENCODER\n",
    "l_in = InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "l_mask = InputLayer(shape=(None, None),input_var=T.neq(input_sequence,-1)) \n",
    "\n",
    "\n",
    "l_emb = non_trainable(EmbeddingLayer(l_in, len(source_letters),  len(source_letters), W=np.diag(np.ones(len(source_letters)))))\n",
    "l_rnn = LSTMLayer(l_emb, HIDDEN_SIZE, only_return_final=True, mask_input=l_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##DECODER\n",
    "dec_in = InputLayer(shape=(None, None),input_var=output_sequence)\n",
    "dec_mask = InputLayer(shape=(None, None),input_var=T.neq(output_sequence,-1))\n",
    "\n",
    "dec_emb = non_trainable(EmbeddingLayer(dec_in, len(target_letters), len(target_letters), W=np.diag(np.ones(len(target_letters)))))\n",
    "dec_rnn = LSTMLayer(dec_emb, num_units=HIDDEN_SIZE, cell_init=l_rnn, mask_input=dec_mask)\n",
    "# WARNING! if it's lstm use cell_init, not hid_init\n",
    "\n",
    "\n",
    "#flatten batch and time to be compatible with feedforward layers (will un-flatten later)\n",
    "dec_rnn_flat = reshape(dec_rnn, (-1,dec_rnn.output_shape[-1]))\n",
    "\n",
    "l_out = DenseLayer(dec_rnn_flat, len(target_letters), nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_model(nn, learning_rate=0.001):\n",
    "    # Model weights\n",
    "    weights = get_all_params(nn)\n",
    "    network_output = get_output(nn)\n",
    "    network_output = network_output.reshape([output_sequence.shape[0],\\\n",
    "                                         output_sequence.shape[1], -1])\n",
    "    predictions_flat = network_output[:,:-1,:].reshape([-1,len(target_letters)])\n",
    "    targets = output_sequence[:,1:].ravel()\n",
    "\n",
    "    #do not count loss for '-1' tokens\n",
    "    mask = T.nonzero(T.neq(targets,-1))\n",
    "\n",
    "    loss = lasagne.objectives.categorical_crossentropy(predictions_flat[mask], targets[mask]).mean()\n",
    "    updates = lasagne.updates.adam(loss, weights, learning_rate=learning_rate)\n",
    "    #training\n",
    "    train = theano.function([input_sequence, output_sequence], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    #computing loss without training\n",
    "    compute_cost = theano.function([input_sequence, output_sequence], loss, allow_input_downcast=True)\n",
    "    #compile the function that computes probabilities for next token given previous text.\n",
    "\n",
    "    last_probas =network_output[:, -1]\n",
    "\n",
    "    probs = theano.function([input_sequence, output_sequence], last_probas)\n",
    "    return train, compute_cost, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "We now need to implement a function that generates output sequence given input.\n",
    "\n",
    "Such function must work thusly:\n",
    "```\n",
    "Init:\n",
    "x = input\n",
    "y = [\"START\"]\n",
    "\n",
    "While not_too_long:\n",
    "  p(y_next|x,y) = probabilities of next letter for y\n",
    "  \n",
    "  y_next ~ p(y_next|x,y)\n",
    "  \n",
    "  y.append(y_next)\n",
    "  \n",
    "  if y_next == \"END\":\n",
    "      break\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_output(input, probs, target_letters, target_letter_to_ix, source_letter_to_ix,\n",
    "                    output_prefix = (\"START\",),\n",
    "                    END_token=\"END\",\n",
    "                    temperature=1,\n",
    "                    sample=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement a function that generates output sequence given input.\n",
    "    \n",
    "    We recommend (but not require) you to use the pseudo-code above and inline instructions.\n",
    "    \"\"\"\n",
    "    x = as_matrix([input], source_letter_to_ix) \n",
    "    output = list(output_prefix)\n",
    "    while True:\n",
    "        y = as_matrix([output], target_letter_to_ix)\n",
    "        next_y_probs = probs(x, y)\n",
    "        next_y_probs = (next_y_probs ** temperature) / (next_y_probs ** temperature).sum()\n",
    "        if sample:\n",
    "            next_y = np.random.choice(target_letters, p=next_y_probs[0])\n",
    "        else:\n",
    "            next_y = target_letters[next_y_probs[0].argmax()]\n",
    "        next_y = str(next_y)             \n",
    "        assert type(next_y) is str, \"please return token(string/character), not it's index\"\n",
    "        \n",
    "        output.append(next_y)\n",
    "\n",
    "        if next_y==END_token:\n",
    "            break\n",
    "            \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#source_seqs = np.array(source_seqs)\n",
    "#target_seqs = np.array(target_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(source_seqs, source_letter_to_ix, target_seqs, target_letter_to_ix, batch_size):\n",
    "    \"\"\"samples a random batch of source and target sequences, batch_size elements\"\"\"\n",
    "    batch_ix = np.random.randint(0,len(source_seqs),size=batch_size)\n",
    "    source_seqs_batch=as_matrix(source_seqs[batch_ix], source_letter_to_ix) \n",
    "    target_seqs_batch=as_matrix(target_seqs[batch_ix], target_letter_to_ix)\n",
    "    \n",
    "    return source_seqs_batch,target_seqs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train average loss = 0.08197419669138639\n",
      "Epoch 0 val average loss = 0.11527614729696303\n",
      "59+996 : 1055  |  1055\n",
      "534+873 : 1397  |  1407\n",
      "267+31 : 308  |  298\n",
      "5+515 : 520  |  520\n",
      "388+9 : 407  |  397\n",
      "Epoch 1 train average loss = 0.07638746983330547\n",
      "Epoch 1 val average loss = 0.11739519589507472\n",
      "272+5 : 277  |  277\n",
      "716+37 : 753  |  753\n",
      "681+81 : 762  |  762\n",
      "547+75 : 622  |  622\n",
      "29+857 : 886  |  886\n",
      "Epoch 2 train average loss = 0.07240082069820562\n",
      "Epoch 2 val average loss = 0.12240237601262764\n",
      "416+0 : 416  |  416\n",
      "803+670 : 1483  |  1473\n",
      "485+0 : 485  |  485\n",
      "97+406 : 503  |  503\n",
      "366+991 : 1357  |  1357\n",
      "Epoch 3 train average loss = 0.06832969206544318\n",
      "Epoch 3 val average loss = 0.09738800715904881\n",
      "924+232 : 1046  |  1156\n",
      "336+578 : 914  |  914\n",
      "798+963 : 1761  |  1761\n",
      "37+964 : 1001  |  1001\n",
      "549+66 : 615  |  615\n",
      "Epoch 4 train average loss = 0.06131126798776717\n",
      "Epoch 4 val average loss = 0.09616244593585298\n",
      "167+24 : 191  |  191\n",
      "62+89 : 151  |  151\n",
      "646+270 : 906  |  916\n",
      "1+952 : 953  |  953\n",
      "488+495 : 883  |  983\n",
      "Epoch 5 train average loss = 0.061328818428661364\n",
      "Epoch 5 val average loss = 0.0891181521692748\n",
      "639+3 : 642  |  642\n",
      "960+768 : 1728  |  1728\n",
      "454+301 : 755  |  755\n",
      "156+321 : 477  |  477\n",
      "26+434 : 460  |  460\n",
      "Epoch 6 train average loss = 0.056341027952983576\n",
      "Epoch 6 val average loss = 0.09772724745382756\n",
      "505+164 : 669  |  669\n",
      "16+231 : 247  |  247\n",
      "26+815 : 841  |  841\n",
      "777+158 : 935  |  935\n",
      "8+322 : 330  |  330\n",
      "Epoch 7 train average loss = 0.05554267240868105\n",
      "Epoch 7 val average loss = 0.09500756026966033\n",
      "552+65 : 617  |  617\n",
      "93+671 : 764  |  764\n",
      "198+35 : 233  |  233\n",
      "14+301 : 315  |  315\n",
      "630+74 : 694  |  704\n",
      "Epoch 8 train average loss = 0.049498748861292706\n",
      "Epoch 8 val average loss = 0.09078536681707769\n",
      "582+58 : 640  |  640\n",
      "947+162 : 1119  |  1109\n",
      "772+760 : 1532  |  1532\n",
      "332+644 : 976  |  976\n",
      "804+363 : 1167  |  1167\n",
      "Epoch 9 train average loss = 0.0466625551476665\n",
      "Epoch 9 val average loss = 0.07915094139023565\n",
      "86+446 : 532  |  532\n",
      "253+3 : 256  |  256\n",
      "417+94 : 511  |  511\n",
      "247+757 : 994  |  1004\n",
      "69+267 : 326  |  336\n",
      "Epoch 10 train average loss = 0.05303136426517701\n",
      "Epoch 10 val average loss = 0.09109121912388841\n",
      "165+698 : 863  |  863\n",
      "449+376 : 825  |  825\n",
      "959+19 : 988  |  978\n",
      "326+695 : 1021  |  1021\n",
      "16+169 : 185  |  185\n",
      "Epoch 11 train average loss = 0.04537219673986813\n",
      "Epoch 11 val average loss = 0.08914087321550763\n",
      "930+91 : 1022  |  1021\n",
      "15+71 : 86  |  86\n",
      "33+384 : 417  |  417\n",
      "942+227 : 1169  |  1169\n",
      "225+53 : 278  |  278\n",
      "Epoch 12 train average loss = 0.040925229031850224\n",
      "Epoch 12 val average loss = 0.06782731788134382\n",
      "76+821 : 907  |  897\n",
      "34+386 : 420  |  420\n",
      "425+21 : 446  |  446\n",
      "847+48 : 895  |  895\n",
      "93+68 : 161  |  161\n",
      "Epoch 13 train average loss = 0.044844899372401936\n",
      "Epoch 13 val average loss = 0.0715847643294914\n",
      "57+897 : 954  |  954\n",
      "668+30 : 698  |  698\n",
      "32+556 : 588  |  588\n",
      "48+389 : 437  |  437\n",
      "860+455 : 1315  |  1315\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=30\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "train_batches_per_epoch = 500\n",
    "val_batches_per_epoch = 50\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "train_source_seqs, val_source_seqs, train_target_seqs, val_target_seqs = train_test_split(source_seqs, target_seqs,\\\n",
    "                                                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        train_avg_cost = 0;\n",
    "        val_avg_cost = 0;\n",
    "\n",
    "        for _ in range(train_batches_per_epoch):\n",
    "            x,y = sample_batch(train_source_seqs, source_letter_to_ix, train_target_seqs, target_letter_to_ix, batch_size)\n",
    "            train_avg_cost += train(x, y).mean()\n",
    "        \n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            x,y = sample_batch(val_source_seqs, source_letter_to_ix, val_target_seqs, target_letter_to_ix, batch_size)\n",
    "            val_avg_cost += compute_cost(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} train average loss = {}\".format(epoch, train_avg_cost / train_batches_per_epoch))\n",
    "        print(\"Epoch {} val average loss = {}\".format(epoch, val_avg_cost / val_batches_per_epoch))\n",
    "        \n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(val_source_seqs))\n",
    "            print (val_source_seqs[ind],':', ''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(val_target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bazal module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CerMemory(lasagne.layers.Layer):\n",
    "    def __init__(self, incoming, memory_size, M=lasagne.init.Orthogonal(), **kwargs):\n",
    "        super(CerMemory, self).__init__(incoming, **kwargs)\n",
    "        self.query_shape = self.input_shape[1]\n",
    "        self.memory_size = memory_size\n",
    "        self.M = self.add_param(M, (self.query_shape, memory_size), name='M')\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        m = self.M / T.sqrt(T.sqr(self.M).sum(axis=0)).reshape(self.M.shape[1], 1)\n",
    "        weights =  T.dot(input, m)\n",
    "        return T.dot(weights, m.T)\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], self.query_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EvcNormalizer(lasagne.layers.Layer):\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return (input.T / T.sqrt(T.sqr(input).sum(axis=1)).reshape(input.shape[0], 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_SIZE = 16\n",
    "MEMORY_SIZE = 64\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bazal_model(query_size, memory_size, hidden_size, memory_benchmark=False, bidir_features=False):\n",
    "\n",
    "    ##ENCODER\n",
    "    l_in = InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "    l_mask = InputLayer(shape=(None, None),input_var=T.neq(input_sequence,-1)) \n",
    "\n",
    "\n",
    "    l_emb = non_trainable(EmbeddingLayer(l_in, len(source_letters),  len(source_letters), W=np.diag(np.ones(len(source_letters)))))\n",
    "\n",
    "    features = LSTMLayer(l_emb, HIDDEN_SIZE, only_return_final=True, mask_input=l_mask)\n",
    "    features_backward = LSTMLayer(l_emb, HIDDEN_SIZE, only_return_final=True, mask_input=l_mask, backwards=True)\n",
    "    if bidir_features:\n",
    "        features = ConcatLayer([features, features_backward])\n",
    "    \n",
    "    if not memory_benchmark:\n",
    "        ## QUERY BUILDER\n",
    "        query = DenseLayer(features, QUERY_SIZE, nonlinearity=None)\n",
    "        query = EvcNormalizer(query)\n",
    "        ## Memory\n",
    "        memory = CerMemory(query, MEMORY_SIZE)\n",
    "    else:\n",
    "        memory = DenseLayer(DenseLayer(features, QUERY_SIZE), QUERY_SIZE)\n",
    "        \n",
    "    to_decode = ConcatLayer([features, memory])\n",
    "    \n",
    "    ##DECODER\n",
    "    dec_in = InputLayer(shape=(None, None),input_var=output_sequence)\n",
    "    dec_mask = InputLayer(shape=(None, None),input_var=T.neq(output_sequence,-1))\n",
    "\n",
    "    dec_emb = non_trainable(EmbeddingLayer(dec_in, len(target_letters), len(target_letters), W=np.diag(np.ones(len(target_letters)))))\n",
    "    dec_rnn = LSTMLayer(dec_emb, num_units=to_decode.output_shape[-1], cell_init=to_decode, mask_input=dec_mask)\n",
    "    # WARNING! if it's lstm use cell_init, not hid_init\n",
    "\n",
    "\n",
    "    #flatten batch and time to be compatible with feedforward layers (will un-flatten later)\n",
    "    dec_rnn_flat = reshape(dec_rnn, (-1,dec_rnn.output_shape[-1]))\n",
    "\n",
    "    l_out = DenseLayer(dec_rnn_flat, len(target_letters), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out, memory = bazal_model(QUERY_SIZE, MEMORY_SIZE, HIDDEN_SIZE, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train average loss = 1.6945099367848775\n",
      "Epoch 0 val average loss = 1.5074621230403429\n",
      "16+162 : 596  |  178\n",
      "909+62 : 1186  |  971\n",
      "652+99 : 934  |  751\n",
      "942+3 : 611  |  945\n",
      "95+783 : 864  |  878\n",
      "Epoch 1 train average loss = 1.3506855359382486\n",
      "Epoch 1 val average loss = 1.2644231651026285\n",
      "34+648 : 522  |  682\n",
      "46+902 : 994  |  948\n",
      "95+453 : 510  |  548\n",
      "86+826 : 934  |  912\n",
      "8+179 : 159  |  187\n",
      "Epoch 2 train average loss = 1.1934498033549985\n",
      "Epoch 2 val average loss = 1.1549287399020165\n",
      "258+75 : 302  |  333\n",
      "83+789 : 899  |  872\n",
      "77+63 : 135  |  140\n",
      "62+49 : 85  |  111\n",
      "40+85 : 118  |  125\n",
      "Epoch 3 train average loss = 1.0759960223199752\n",
      "Epoch 3 val average loss = 1.0252210532587676\n",
      "1+246 : 255  |  247\n",
      "9+485 : 479  |  494\n",
      "13+987 : 1001  |  1000\n",
      "776+89 : 843  |  865\n",
      "819+449 : 1373  |  1268\n",
      "Epoch 4 train average loss = 0.9780166374766287\n",
      "Epoch 4 val average loss = 0.9553360436964097\n",
      "503+243 : 678  |  746\n",
      "361+993 : 1356  |  1354\n",
      "93+964 : 1029  |  1057\n",
      "77+562 : 633  |  639\n",
      "88+754 : 837  |  842\n",
      "Epoch 5 train average loss = 0.9023300818373581\n",
      "Epoch 5 val average loss = 0.8818523491654244\n",
      "82+305 : 396  |  387\n",
      "308+48 : 404  |  356\n",
      "65+777 : 841  |  842\n",
      "348+45 : 410  |  393\n",
      "418+434 : 848  |  852\n",
      "Epoch 6 train average loss = 0.824931680003359\n",
      "Epoch 6 val average loss = 0.7995710520590963\n",
      "672+77 : 759  |  749\n",
      "639+9 : 637  |  648\n",
      "259+73 : 324  |  332\n",
      "905+60 : 964  |  965\n",
      "461+717 : 1173  |  1178\n",
      "Epoch 7 train average loss = 0.7339451818325916\n",
      "Epoch 7 val average loss = 0.6740469434788885\n",
      "7+501 : 410  |  508\n",
      "2+600 : 601  |  602\n",
      "558+758 : 1414  |  1316\n",
      "60+865 : 926  |  925\n",
      "788+565 : 1343  |  1353\n",
      "Epoch 8 train average loss = 0.5513957743498399\n",
      "Epoch 8 val average loss = 0.48345586432400467\n",
      "20+80 : 91  |  100\n",
      "763+439 : 1192  |  1202\n",
      "303+6 : 310  |  309\n",
      "464+186 : 659  |  650\n",
      "567+54 : 621  |  621\n",
      "Epoch 9 train average loss = 0.39963861069525186\n",
      "Epoch 9 val average loss = 0.37164818168376257\n",
      "552+865 : 1427  |  1417\n",
      "719+40 : 760  |  759\n",
      "873+38 : 911  |  911\n",
      "223+92 : 315  |  315\n",
      "4+911 : 925  |  915\n",
      "Epoch 10 train average loss = 0.3166594728580184\n",
      "Epoch 10 val average loss = 0.3057222049007717\n",
      "174+239 : 412  |  413\n",
      "160+91 : 242  |  251\n",
      "16+197 : 214  |  213\n",
      "14+279 : 302  |  293\n",
      "851+9 : 870  |  860\n",
      "Epoch 11 train average loss = 0.26469414749526604\n",
      "Epoch 11 val average loss = 0.25429453256403406\n",
      "67+547 : 614  |  614\n",
      "4+175 : 179  |  179\n",
      "145+54 : 190  |  199\n",
      "464+562 : 1026  |  1026\n",
      "25+135 : 150  |  160\n",
      "Epoch 12 train average loss = 0.22193399861030197\n",
      "Epoch 12 val average loss = 0.222899579814403\n",
      "563+8 : 569  |  571\n",
      "70+23 : 93  |  93\n",
      "636+60 : 697  |  696\n",
      "39+17 : 62  |  56\n",
      "27+781 : 808  |  808\n",
      "Epoch 13 train average loss = 0.19049510327702032\n",
      "Epoch 13 val average loss = 0.19231423759878022\n",
      "35+143 : 188  |  178\n",
      "592+205 : 807  |  797\n",
      "50+570 : 621  |  620\n",
      "435+98 : 533  |  533\n",
      "531+175 : 716  |  706\n",
      "Epoch 14 train average loss = 0.16416616283301408\n",
      "Epoch 14 val average loss = 0.1662088476681247\n",
      "176+48 : 224  |  224\n",
      "84+73 : 157  |  157\n",
      "994+22 : 1016  |  1016\n",
      "776+859 : 1654  |  1635\n",
      "107+10 : 117  |  117\n",
      "Epoch 15 train average loss = 0.14625039378310214\n",
      "Epoch 15 val average loss = 0.14820288830408151\n",
      "230+499 : 829  |  729\n",
      "13+39 : 52  |  52\n",
      "34+500 : 534  |  534\n",
      "751+86 : 837  |  837\n",
      "358+338 : 706  |  696\n",
      "Epoch 16 train average loss = 0.12746339008296342\n",
      "Epoch 16 val average loss = 0.1489301747502429\n",
      "356+66 : 422  |  422\n",
      "298+57 : 355  |  355\n",
      "12+53 : 64  |  65\n",
      "385+299 : 684  |  684\n",
      "77+916 : 993  |  993\n",
      "Epoch 17 train average loss = 0.12350559415206168\n",
      "Epoch 17 val average loss = 0.1279107783927959\n",
      "249+202 : 451  |  451\n",
      "81+292 : 373  |  373\n",
      "759+81 : 840  |  840\n",
      "426+7 : 433  |  433\n",
      "828+305 : 1133  |  1133\n",
      "Epoch 18 train average loss = 0.10491778366516522\n",
      "Epoch 18 val average loss = 0.1286961144158265\n",
      "328+62 : 390  |  390\n",
      "6+876 : 882  |  882\n",
      "822+739 : 1561  |  1561\n",
      "22+300 : 322  |  322\n",
      "502+3 : 505  |  505\n",
      "Epoch 19 train average loss = 0.10867035604968149\n",
      "Epoch 19 val average loss = 0.1314757160453732\n",
      "55+27 : 81  |  82\n",
      "82+171 : 253  |  253\n",
      "698+112 : 800  |  810\n",
      "63+944 : 1007  |  1007\n",
      "85+242 : 327  |  327\n",
      "Epoch 20 train average loss = 0.08372820228534313\n",
      "Epoch 20 val average loss = 0.08974837402974989\n",
      "7+508 : 525  |  515\n",
      "6+279 : 285  |  285\n",
      "264+961 : 1225  |  1225\n",
      "25+86 : 111  |  111\n",
      "538+43 : 581  |  581\n",
      "Epoch 21 train average loss = 0.08110477866824256\n",
      "Epoch 21 val average loss = 0.09981427569957456\n",
      "401+639 : 1040  |  1040\n",
      "899+370 : 1269  |  1269\n",
      "88+902 : 990  |  990\n",
      "330+574 : 904  |  904\n",
      "803+2 : 805  |  805\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=30\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "train_batches_per_epoch = 500\n",
    "val_batches_per_epoch = 50\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "train_source_seqs, val_source_seqs, train_target_seqs, val_target_seqs = train_test_split(source_seqs, target_seqs,\\\n",
    "                                                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        train_avg_cost = 0;\n",
    "        val_avg_cost = 0;\n",
    "\n",
    "        for _ in range(train_batches_per_epoch):\n",
    "            x,y = sample_batch(train_source_seqs, source_letter_to_ix, train_target_seqs, target_letter_to_ix, batch_size)\n",
    "            train_avg_cost += train(x, y).mean()\n",
    "        \n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            x,y = sample_batch(val_source_seqs, source_letter_to_ix, val_target_seqs, target_letter_to_ix, batch_size)\n",
    "            val_avg_cost += compute_cost(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} train average loss = {}\".format(epoch, train_avg_cost / train_batches_per_epoch))\n",
    "        print(\"Epoch {} val average loss = {}\".format(epoch, val_avg_cost / val_batches_per_epoch))\n",
    "        \n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(val_source_seqs))\n",
    "            print (val_source_seqs[ind],':', ''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(val_target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M_add = np.array(memory.M.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./zoo/addprob_memory_after30epochs.npy', M_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same for prod problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 100000\n",
    "DIGITS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 100000\n"
     ]
    }
   ],
   "source": [
    "source_seqs, source_letters, source_letter_to_ix, target_seqs, target_letters, target_letter_to_ix =\\\n",
    "                    generate_data(TRAINING_SIZE, DIGITS, lambda a, b: a * b, '{}*{}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54*911 : 49194\n",
      "8*690 : 5520\n",
      "465*0 : 0\n",
      "714*109 : 77826\n",
      "7*13 : 91\n"
     ]
    }
   ],
   "source": [
    "for source, target in zip(source_seqs[:5],target_seqs[:5]):\n",
    "    print( source,':',\"\".join(target[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_SIZE = 16\n",
    "MEMORY_SIZE = 64\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out, memory = bazal_model(QUERY_SIZE, MEMORY_SIZE, HIDDEN_SIZE,  False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train average loss = 1.7809327858483859\n",
      "Epoch 0 val average loss = 1.6148372492990466\n",
      "90*186 : 32540  |  16740\n",
      "269*27 : 26816  |  7263\n",
      "285*76 : 17690  |  21660\n",
      "62*814 : 47884  |  50468\n",
      "71*278 : 46426  |  19738\n",
      "Epoch 1 train average loss = 1.550690292434635\n",
      "Epoch 1 val average loss = 1.478128944848121\n",
      "578*653 : 309402  |  377434\n",
      "573*8 : 4376  |  4584\n",
      "467*86 : 31928  |  40162\n",
      "120*3 : 030  |  360\n",
      "738*487 : 385866  |  359406\n",
      "Epoch 2 train average loss = 1.4268369515115071\n",
      "Epoch 2 val average loss = 1.384444662047536\n",
      "14*26 : 556  |  364\n",
      "586*134 : 46204  |  78524\n",
      "821*227 : 141499  |  186367\n",
      "832*5 : 4450  |  4160\n",
      "5*699 : 5875  |  3495\n",
      "Epoch 3 train average loss = 1.3388051017622669\n",
      "Epoch 3 val average loss = 1.2883489420631062\n",
      "946*313 : 312434  |  296098\n",
      "34*657 : 12598  |  22338\n",
      "56*807 : 39556  |  45192\n",
      "39*482 : 19394  |  18798\n",
      "210*374 : 43190  |  78540\n",
      "Epoch 4 train average loss = 1.2542447322617154\n",
      "Epoch 4 val average loss = 1.2269998502710724\n",
      "160*288 : 41160  |  46080\n",
      "770*412 : 301080  |  317240\n",
      "142*934 : 127144  |  132628\n",
      "121*86 : 10582  |  10406\n",
      "660*913 : 811640  |  602580\n",
      "Epoch 5 train average loss = 1.1961758061019188\n",
      "Epoch 5 val average loss = 1.1782971265595763\n",
      "932*19 : 11252  |  17708\n",
      "73*704 : 51212  |  51392\n",
      "814*894 : 736604  |  727716\n",
      "1*923 : 1939  |  923\n",
      "647*698 : 465934  |  451606\n",
      "Epoch 6 train average loss = 1.146865787263563\n",
      "Epoch 6 val average loss = 1.1384976973198864\n",
      "288*89 : 23632  |  25632\n",
      "581*444 : 245584  |  257964\n",
      "249*16 : 5048  |  3984\n",
      "469*996 : 423476  |  467124\n",
      "637*3 : 1951  |  1911\n",
      "Epoch 7 train average loss = 1.0884484024662402\n",
      "Epoch 7 val average loss = 1.074366965329761\n",
      "9*212 : 1912  |  1908\n",
      "586*97 : 58374  |  56842\n",
      "118*49 : 6854  |  5782\n",
      "87*193 : 17109  |  16791\n",
      "271*92 : 24628  |  24932\n",
      "Epoch 8 train average loss = 1.0195161495907081\n",
      "Epoch 8 val average loss = 1.00042301480989\n",
      "854*96 : 70832  |  81984\n",
      "970*92 : 79240  |  89240\n",
      "222*22 : 6324  |  4884\n",
      "684*221 : 149344  |  151164\n",
      "63*96 : 6452  |  6048\n",
      "Epoch 9 train average loss = 0.9651495977172331\n",
      "Epoch 9 val average loss = 0.9765618396931527\n",
      "870*16 : 13480  |  13920\n",
      "518*15 : 9090  |  7770\n",
      "95*73 : 6825  |  6935\n",
      "57*56 : 3236  |  3192\n",
      "134*718 : 95588  |  96212\n",
      "Epoch 10 train average loss = 0.9244296275851274\n",
      "Epoch 10 val average loss = 0.9211881420831801\n",
      "335*350 : 116250  |  117250\n",
      "41*50 : 2050  |  2050\n",
      "126*68 : 8608  |  8568\n",
      "19*818 : 14902  |  15542\n",
      "429*55 : 22855  |  23595\n",
      "Epoch 11 train average loss = 0.887073355606777\n",
      "Epoch 11 val average loss = 0.8807974672617948\n",
      "870*802 : 703140  |  697740\n",
      "263*563 : 148939  |  148069\n",
      "517*614 : 317838  |  317438\n",
      "263*543 : 149299  |  142809\n",
      "232*323 : 49506  |  74936\n",
      "Epoch 12 train average loss = 0.8521344730669261\n",
      "Epoch 12 val average loss = 0.8513462692266834\n",
      "299*39 : 12081  |  11661\n",
      "531*670 : 348270  |  355770\n",
      "713*81 : 58443  |  57753\n",
      "370*8 : 2920  |  2960\n",
      "817*476 : 401572  |  388892\n",
      "Epoch 13 train average loss = 0.8240589237118288\n",
      "Epoch 13 val average loss = 0.8478362057288514\n",
      "40*595 : 23800  |  23800\n",
      "201*350 : 66150  |  70350\n",
      "124*873 : 111672  |  108252\n",
      "22*455 : 9510  |  10010\n",
      "79*78 : 6322  |  6162\n",
      "Epoch 14 train average loss = 0.7929240343287127\n",
      "Epoch 14 val average loss = 0.8051037459797008\n",
      "277*390 : 104530  |  108030\n",
      "49*378 : 19062  |  18522\n",
      "4*494 : 1976  |  1976\n",
      "682*12 : 9584  |  8184\n",
      "90*646 : 56620  |  58140\n",
      "Epoch 15 train average loss = 0.7732099692112991\n",
      "Epoch 15 val average loss = 0.797048375159779\n",
      "478*559 : 259562  |  267202\n",
      "633*853 : 528729  |  539949\n",
      "26*376 : 8912  |  9776\n",
      "844*13 : 11532  |  10972\n",
      "810*12 : 9720  |  9720\n",
      "Epoch 16 train average loss = 0.751487696121562\n",
      "Epoch 16 val average loss = 0.7590114346643851\n",
      "529*663 : 342807  |  350727\n",
      "48*953 : 46024  |  45744\n",
      "817*67 : 52929  |  54739\n",
      "49*299 : 14911  |  14651\n",
      "367*703 : 256751  |  258001\n",
      "Epoch 17 train average loss = 0.7357106872853555\n",
      "Epoch 17 val average loss = 0.78910929088225\n",
      "718*58 : 41764  |  41644\n",
      "359*68 : 24532  |  24412\n",
      "635*182 : 116310  |  115570\n",
      "441*78 : 34898  |  34398\n",
      "91*986 : 91666  |  89726\n",
      "Epoch 18 train average loss = 0.7183586930465333\n",
      "Epoch 18 val average loss = 0.7420452350137823\n",
      "429*660 : 277820  |  283140\n",
      "27*979 : 26153  |  26433\n",
      "52*122 : 6392  |  6344\n",
      "56*712 : 40056  |  39872\n",
      "325*17 : 6315  |  5525\n",
      "Epoch 19 train average loss = 0.701970784301091\n",
      "Epoch 19 val average loss = 0.7300190825923165\n",
      "58*698 : 40444  |  40484\n",
      "30*254 : 5980  |  7620\n",
      "325*94 : 31050  |  30550\n",
      "184*211 : 38544  |  38824\n",
      "285*286 : 79630  |  81510\n",
      "Epoch 20 train average loss = 0.6918051144031077\n",
      "Epoch 20 val average loss = 0.7121971827315347\n",
      "59*658 : 39002  |  38822\n",
      "25*181 : 3825  |  4525\n",
      "309*64 : 20036  |  19776\n",
      "155*272 : 44720  |  42160\n",
      "589*102 : 60038  |  60078\n",
      "Epoch 21 train average loss = 0.6734802889295056\n",
      "Epoch 21 val average loss = 0.7196543995517389\n",
      "103*31 : 3393  |  3193\n",
      "87*549 : 46973  |  47763\n",
      "542*9 : 4778  |  4878\n",
      "212*679 : 142748  |  143948\n",
      "15*40 : 700  |  600\n",
      "Epoch 22 train average loss = 0.6635797669461483\n",
      "Epoch 22 val average loss = 0.6960390546718888\n",
      "769*45 : 34355  |  34605\n",
      "470*516 : 239920  |  242520\n",
      "782*503 : 395966  |  393346\n",
      "506*204 : 102264  |  103224\n",
      "768*7 : 4976  |  5376\n",
      "Epoch 23 train average loss = 0.6511172941480892\n",
      "Epoch 23 val average loss = 0.691361810508234\n",
      "98*755 : 74370  |  73990\n",
      "449*83 : 36777  |  37267\n",
      "788*735 : 600020  |  579180\n",
      "246*428 : 104888  |  105288\n",
      "458*268 : 121264  |  122744\n",
      "Epoch 24 train average loss = 0.6363387980041247\n",
      "Epoch 24 val average loss = 0.6852939571267366\n",
      "258*121 : 37098  |  31218\n",
      "261*654 : 167874  |  170694\n",
      "638*38 : 25124  |  24244\n",
      "55*912 : 50760  |  50160\n",
      "814*693 : 573682  |  564102\n",
      "Epoch 25 train average loss = 0.6321751973625743\n",
      "Epoch 25 val average loss = 0.712771874818957\n",
      "605*59 : 35335  |  35695\n",
      "521*117 : 62917  |  60957\n",
      "598*610 : 358980  |  364780\n",
      "105*12 : 1140  |  1260\n",
      "433*0 : 0  |  0\n",
      "Epoch 26 train average loss = 0.6182999723398285\n",
      "Epoch 26 val average loss = 0.6739769889891756\n",
      "68*693 : 47484  |  47124\n",
      "496*27 : 12992  |  13392\n",
      "182*978 : 169836  |  177996\n",
      "787*79 : 70013  |  62173\n",
      "433*4 : 1752  |  1732\n",
      "Epoch 27 train average loss = 0.6079097354322406\n",
      "Epoch 27 val average loss = 0.6738310664538826\n",
      "989*527 : 523893  |  521203\n",
      "42*428 : 18096  |  17976\n",
      "645*449 : 289215  |  289605\n",
      "45*167 : 6895  |  7515\n",
      "24*95 : 2180  |  2280\n",
      "Epoch 28 train average loss = 0.6051825770430418\n",
      "Epoch 28 val average loss = 0.6650592996408364\n",
      "5*759 : 3825  |  3795\n",
      "835*32 : 28020  |  26720\n",
      "762*191 : 145962  |  145542\n",
      "59*845 : 49435  |  49855\n",
      "17*754 : 13218  |  12818\n",
      "Epoch 29 train average loss = 0.5901854397914131\n",
      "Epoch 29 val average loss = 0.6599390135691813\n",
      "728*919 : 673712  |  669032\n",
      "220*599 : 132860  |  131780\n",
      "41*622 : 25442  |  25502\n",
      "40*248 : 9920  |  9920\n",
      "243*1 : 243  |  243\n",
      "Epoch 30 train average loss = 0.5823995040789269\n",
      "Epoch 30 val average loss = 0.6472260038214468\n",
      "379*480 : 187920  |  181920\n",
      "678*2 : 1356  |  1356\n",
      "78*303 : 24374  |  23634\n",
      "375*695 : 267325  |  260625\n",
      "470*702 : 327140  |  329940\n",
      "Epoch 31 train average loss = 0.5736133109748137\n",
      "Epoch 31 val average loss = 0.6153783626723611\n",
      "13*884 : 11452  |  11492\n",
      "598*278 : 166404  |  166244\n",
      "0*868 : 0  |  0\n",
      "5*217 : 1035  |  1085\n",
      "346*22 : 6972  |  7612\n",
      "Epoch 32 train average loss = 0.5624972317210316\n",
      "Epoch 32 val average loss = 0.7469220971462832\n",
      "553*34 : 18598  |  18802\n",
      "958*211 : 199318  |  202138\n",
      "281*251 : 71771  |  70531\n",
      "343*68 : 23364  |  23324\n",
      "222*78 : 17376  |  17316\n",
      "Epoch 33 train average loss = 0.5511906184543067\n",
      "Epoch 33 val average loss = 0.6161977530563937\n",
      "2*211 : 442  |  422\n",
      "55*224 : 12520  |  12320\n",
      "664*562 : 371328  |  373168\n",
      "26*482 : 12412  |  12532\n",
      "596*126 : 75036  |  75096\n",
      "Epoch 34 train average loss = 0.5452938699394516\n",
      "Epoch 34 val average loss = 0.6140450949597044\n",
      "772*2 : 1544  |  1544\n",
      "192*391 : 69672  |  75072\n",
      "27*195 : 4905  |  5265\n",
      "135*692 : 100720  |  93420\n",
      "37*68 : 2496  |  2516\n",
      "Epoch 35 train average loss = 0.5311642486107081\n",
      "Epoch 35 val average loss = 0.6440399484427611\n",
      "733*140 : 103180  |  102620\n",
      "700*73 : 51100  |  51100\n",
      "816*413 : 343028  |  337008\n",
      "420*72 : 30240  |  30240\n",
      "520*811 : 424840  |  421720\n",
      "Epoch 36 train average loss = 0.5301976784892567\n",
      "Epoch 36 val average loss = 0.6072345405748684\n",
      "57*535 : 30295  |  30495\n",
      "738*41 : 30338  |  30258\n",
      "162*70 : 11540  |  11340\n",
      "212*61 : 12752  |  12932\n",
      "228*74 : 16952  |  16872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 train average loss = 0.5241485718456392\n",
      "Epoch 37 val average loss = 0.6234277130928519\n",
      "971*72 : 69972  |  69912\n",
      "481*47 : 21967  |  22607\n",
      "57*59 : 3191  |  3363\n",
      "596*560 : 337360  |  333760\n",
      "779*3 : 2407  |  2337\n",
      "Epoch 38 train average loss = 0.516863194289448\n",
      "Epoch 38 val average loss = 0.609415417801293\n",
      "419*856 : 358244  |  358664\n",
      "49*299 : 14531  |  14651\n",
      "905*76 : 67980  |  68780\n",
      "40*352 : 13680  |  14080\n",
      "488*79 : 38792  |  38552\n",
      "Epoch 39 train average loss = 0.5100114681903684\n",
      "Epoch 39 val average loss = 0.6251170410488122\n",
      "24*481 : 11544  |  11544\n",
      "686*64 : 42904  |  43904\n",
      "965*551 : 537815  |  531715\n",
      "70*481 : 33470  |  33670\n",
      "173*7 : 1191  |  1211\n",
      "Epoch 40 train average loss = 0.5002524844366676\n",
      "Epoch 40 val average loss = 0.59990950224516\n",
      "307*41 : 12527  |  12587\n",
      "877*33 : 29161  |  28941\n",
      "11*847 : 9337  |  9317\n",
      "20*327 : 6540  |  6540\n",
      "917*801 : 730217  |  734517\n",
      "Epoch 41 train average loss = 0.5028900717794844\n",
      "Epoch 41 val average loss = 0.595912978532161\n",
      "15*300 : 4100  |  4500\n",
      "7*896 : 6252  |  6272\n",
      "3*687 : 2081  |  2061\n",
      "112*994 : 109528  |  111328\n",
      "367*773 : 283511  |  283691\n",
      "Epoch 42 train average loss = 0.49341716797348206\n",
      "Epoch 42 val average loss = 0.5956828481826435\n",
      "767*56 : 42872  |  42952\n",
      "12*14 : 164  |  168\n",
      "592*719 : 425088  |  425648\n",
      "815*399 : 324985  |  325185\n",
      "735*662 : 487470  |  486570\n",
      "Epoch 43 train average loss = 0.48365313731517545\n",
      "Epoch 43 val average loss = 0.5884810514014651\n",
      "755*57 : 43235  |  43035\n",
      "386*53 : 20578  |  20458\n",
      "865*26 : 23030  |  22490\n",
      "166*21 : 3466  |  3486\n",
      "364*20 : 6880  |  7280\n",
      "Epoch 44 train average loss = 0.48644662123711896\n",
      "Epoch 44 val average loss = 0.5941083542401737\n",
      "67*993 : 65931  |  66531\n",
      "19*760 : 14440  |  14440\n",
      "90*163 : 14470  |  14670\n",
      "268*94 : 25232  |  25192\n",
      "493*93 : 45869  |  45849\n",
      "Epoch 45 train average loss = 0.4782899631644679\n",
      "Epoch 45 val average loss = 0.5820024843514701\n",
      "519*987 : 511533  |  512253\n",
      "387*83 : 32301  |  32121\n",
      "841*688 : 569968  |  578608\n",
      "786*93 : 73278  |  73098\n",
      "886*148 : 131568  |  131128\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=1000\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "train_batches_per_epoch = 500\n",
    "val_batches_per_epoch = 50\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "train_source_seqs, val_source_seqs, train_target_seqs, val_target_seqs = train_test_split(source_seqs, target_seqs,\\\n",
    "                                                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        train_avg_cost = 0;\n",
    "        val_avg_cost = 0;\n",
    "\n",
    "        for _ in range(train_batches_per_epoch):\n",
    "            x,y = sample_batch(train_source_seqs, source_letter_to_ix, train_target_seqs, target_letter_to_ix, batch_size)\n",
    "            train_avg_cost += train(x, y).mean()\n",
    "        \n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            x,y = sample_batch(val_source_seqs, source_letter_to_ix, val_target_seqs, target_letter_to_ix, batch_size)\n",
    "            val_avg_cost += compute_cost(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} train average loss = {}\".format(epoch, train_avg_cost / train_batches_per_epoch))\n",
    "        print(\"Epoch {} val average loss = {}\".format(epoch, val_avg_cost / val_batches_per_epoch))\n",
    "        \n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(val_source_seqs))\n",
    "            print (val_source_seqs[ind],':', ''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(val_target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grand problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob1, prob_op1 = lambda a, b, c: (a + b) * c, '({}+{})*{}'\n",
    "prob2, prob_op2 = lambda a, b, c:  a * (b + c), '{}*({}+{})'\n",
    "prob3, prob_op3 = lambda a, b, c:  a + b * c, '{}+{}*{}'\n",
    "prob4, prob_op4 = lambda a, b, c:  a * b + c, '{}*{}+{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_grand_data(size, digits, problems = [prob1, prob2, prob3, prob4],\\\n",
    "                        problem_operators=[prob_op1, prob_op2, prob_op3, prob_op4]):\n",
    "    source_seqs = []\n",
    "    target_seqs = []\n",
    "    seen = set()\n",
    "    assert len(problem_operators) == len(problems)\n",
    "    print('Generating data...')\n",
    "    while len(source_seqs) < TRAINING_SIZE:\n",
    "        f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                        for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "        a, b, c = f(), f(), f()\n",
    "        # Skip any addition questions we've already seen\n",
    "        # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "        key = tuple(sorted((a, b, c)))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        \n",
    "        coin = np.random.randint(0, len(problems))\n",
    "        \n",
    "        # Pad the data with spaces such that it is always MAXLEN.\n",
    "        q = problem_operators[coin].format(a, b, c)\n",
    "        ans = str(problems[coin](a, b, c))\n",
    "\n",
    "        source_seqs.append(q)\n",
    "        target_seqs.append([\"START\"] + list(ans) + [\"END\"])\n",
    "\n",
    "    print('Total addition questions:', len(source_seqs))\n",
    "    \n",
    "    target_letters = list(set([token for ts in target_seqs for token in ts]))\n",
    "    target_letter_to_ix = {ph:i for i,ph in enumerate(target_letters)}\n",
    "    \n",
    "    source_letters = list(set([token for word in source_seqs for token in word]))\n",
    "    source_letter_to_ix = {l:i for i,l in enumerate(source_letters)}\n",
    "    \n",
    "    return np.array(source_seqs), source_letters, source_letter_to_ix, \\\n",
    "           np.array(target_seqs), target_letters, target_letter_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 2000\n",
    "DIGITS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 2000\n"
     ]
    }
   ],
   "source": [
    "source_seqs, source_letters, source_letter_to_ix, target_seqs, target_letters, target_letter_to_ix =\\\n",
    "                    generate_grand_data(TRAINING_SIZE, DIGITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93+3*79 : 330\n",
      "2*0+30 : 30\n",
      "1*(73+45) : 118\n",
      "6*(3+67) : 420\n",
      "69*8+3 : 555\n"
     ]
    }
   ],
   "source": [
    "for source, target in zip(source_seqs[:5],target_seqs[:5]):\n",
    "    print( source,':',\"\".join(target[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_SIZE = 16\n",
    "MEMORY_SIZE = 64\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out, memory = bazal_model(QUERY_SIZE, MEMORY_SIZE, HIDDEN_SIZE,  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 average loss = 1.8074171794116005\n",
      "3*(54+8) : 839  |  186\n",
      "44*7+7 : 229  |  315\n",
      "57+15*4 : 218  |  117\n",
      "43*1+2 : 108  |  45\n",
      "27*2+87 : 608  |  141\n",
      "Epoch 1 average loss = 1.6196723853008468\n",
      "18+7*62 : 303  |  452\n",
      "48+7*4 : 98  |  76\n",
      "8+89*8 : 403  |  720\n",
      "2*(59+96) : 269  |  310\n",
      "(58+6)*8 : 880  |  512\n",
      "Epoch 2 average loss = 1.4874540646206253\n",
      "92+53*92 : 5099  |  4968\n",
      "(78+75)*87 : 7765  |  13311\n",
      "54*(25+0) : 1452  |  1350\n",
      "(1+0)*64 : 40  |  64\n",
      "0*7+1 : 3  |  1\n",
      "Epoch 3 average loss = 1.3032164222313742\n",
      "97*(3+18) : 2709  |  2037\n",
      "57*39+80 : 4453  |  2303\n",
      "74*4+7 : 359  |  303\n",
      "0*(78+6) : 0  |  0\n",
      "(87+7)*48 : 2318  |  4512\n",
      "Epoch 4 average loss = 1.0245313576234003\n",
      "7*8+75 : 103  |  131\n",
      "6+6*93 : 469  |  564\n",
      "1+61*5 : 419  |  306\n",
      "2+59*30 : 2598  |  1772\n",
      "39*42+95 : 1373  |  1733\n",
      "Epoch 5 average loss = 0.6894229277183287\n",
      "74*(2+60) : 1428  |  4588\n",
      "9*6+23 : 79  |  77\n",
      "47*(49+2) : 4234  |  2397\n",
      "7+8*5 : 30  |  47\n",
      "2*69+98 : 208  |  236\n",
      "Epoch 6 average loss = 0.4108774484056355\n",
      "1+20*0 : 4  |  1\n",
      "15*44+65 : 320  |  725\n",
      "36+42*74 : 6432  |  3144\n",
      "93*62+88 : 4708  |  5854\n",
      "(75+92)*9 : 1583  |  1503\n",
      "Epoch 7 average loss = 0.21918465745583113\n",
      "7*52+36 : 400  |  400\n",
      "64+4*43 : 366  |  236\n",
      "52+56*0 : 52  |  52\n",
      "22*(6+44) : 1100  |  1100\n",
      "(85+36)*7 : 614  |  847\n",
      "Epoch 8 average loss = 0.11009148939910773\n",
      "(49+1)*31 : 1550  |  1550\n",
      "(8+1)*87 : 783  |  783\n",
      "21*(11+73) : 1764  |  1764\n",
      "(9+6)*88 : 1320  |  1320\n",
      "11*94+0 : 1034  |  1034\n",
      "Epoch 9 average loss = 0.053998595392924356\n",
      "(55+3)*77 : 4466  |  4466\n",
      "32*8+52 : 270  |  308\n",
      "7*(6+59) : 455  |  455\n",
      "21*(5+1) : 126  |  126\n",
      "19*2+4 : 42  |  42\n",
      "Epoch 10 average loss = 0.08680151892880913\n",
      "63*0+9 : 9  |  9\n",
      "(52+8)*9 : 510  |  540\n",
      "5*6+43 : 73  |  73\n",
      "(96+56)*9 : 1368  |  1368\n",
      "45*14+80 : 710  |  710\n",
      "Epoch 11 average loss = 0.019791177655911787\n",
      "4*(9+14) : 92  |  92\n",
      "55+8*6 : 103  |  103\n",
      "10*(33+6) : 390  |  390\n",
      "30+4*9 : 66  |  66\n",
      "(3+50)*1 : 53  |  53\n",
      "Epoch 12 average loss = 0.012232270473054021\n",
      "63*76+6 : 4794  |  4794\n",
      "(58+93)*2 : 302  |  302\n",
      "36+42*74 : 3144  |  3144\n",
      "68*4+3 : 275  |  275\n",
      "1+10*7 : 71  |  71\n",
      "Epoch 13 average loss = 0.008034974855735894\n",
      "69+2*0 : 69  |  69\n",
      "(3+13)*5 : 80  |  80\n",
      "26*(4+5) : 234  |  234\n",
      "89*(3+8) : 979  |  979\n",
      "(5+71)*38 : 2888  |  2888\n",
      "Epoch 14 average loss = 0.005506300733214235\n",
      "(0+46)*4 : 184  |  184\n",
      "5*(93+0) : 465  |  465\n",
      "97+93*4 : 469  |  469\n",
      "56+60*6 : 416  |  416\n",
      "73+16*8 : 201  |  201\n",
      "Epoch 15 average loss = 0.09774490923958623\n",
      "1*(8+60) : 68  |  68\n",
      "10*76+43 : 803  |  803\n",
      "69+68*0 : 69  |  69\n",
      "(0+34)*0 : 0  |  0\n",
      "63*78+50 : 4964  |  4964\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=30\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "train_batches_per_epoch = 500\n",
    "val_batches_per_epoch = 50\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "train_source_seqs, val_source_seqs, train_target_seqs, val_target_seqs = train_test_split(source_seqs, target_seqs,\\\n",
    "                                                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        train_avg_cost = 0;\n",
    "        val_avg_cost = 0;\n",
    "\n",
    "        for _ in range(train_batches_per_epoch):\n",
    "            x,y = sample_batch(train_source_seqs, source_letter_to_ix, train_target_seqs, target_letter_to_ix, batch_size)\n",
    "            train_avg_cost += train(x, y).mean()\n",
    "        \n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            x,y = sample_batch(val_source_seqs, source_letter_to_ix, val_target_seqs, target_letter_to_ix, batch_size)\n",
    "            val_avg_cost += compute_cost(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} train average loss = {}\".format(epoch, train_avg_cost / train_batches_per_epoch))\n",
    "        print(\"Epoch {} val average loss = {}\".format(epoch, val_avg_cost / val_batches_per_epoch))\n",
    "        \n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(val_source_seqs))\n",
    "            print (val_source_seqs[ind],':', ''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(val_target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
