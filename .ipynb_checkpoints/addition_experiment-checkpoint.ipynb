{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition experiment\n",
    "\n",
    "An implementation of sequence to sequence learning for performing addition\n",
    "\n",
    "* Input: \"535+61\"\n",
    "\n",
    "* Output: \"596\"\n",
    "\n",
    "Padding is handled by using a repeated sentinel character (space)\n",
    "Input may optionally be inverted, shown to increase performance in many tasks in:\n",
    "\"Learning to Execute\"\n",
    "http://arxiv.org/abs/1410.4615\n",
    "and\n",
    "\"Sequence to Sequence Learning with Neural Networks\"\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "Theoretically it introduces shorter term dependencies between source and target.\n",
    "Two digits inverted:\n",
    "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
    "Three digits inverted:\n",
    "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
    "Four digits inverted:\n",
    "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
    "Five digits inverted:\n",
    "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:\n",
      "b'C:\\\\Users\\\\tumanov\\\\AppData\\\\Local\\\\Temp\\\\try_flags_ltil45h8.c:4:19: fatal error: cudnn.h: No such file or directory\\r\\ncompilation terminated.\\r\\n'\n",
      "Mapped name None to device cuda0: GeForce GTX 1070 (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_trainable(net):\n",
    "    for tags in net.params.values():\n",
    "        tags -= {'trainable', 'regularizable'}\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 11\n",
    "INVERT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try replacing GRU, or SimpleRNN.\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(size, digits, problem = lambda a, b: a+b, problem_operator='{}+{}'):\n",
    "    source_seqs = []\n",
    "    target_seqs = []\n",
    "    seen = set()\n",
    "    print('Generating data...')\n",
    "    while len(source_seqs) < TRAINING_SIZE:\n",
    "        f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                        for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "        a, b = f(), f()\n",
    "        # Skip any addition questions we've already seen\n",
    "        # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "        key = tuple(sorted((a, b)))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        # Pad the data with spaces such that it is always MAXLEN.\n",
    "        q = problem_operator.format(a, b)\n",
    "        ans = str(problem(a, b))\n",
    "\n",
    "        source_seqs.append(q)\n",
    "        target_seqs.append([\"START\"] + list(ans) + [\"END\"])\n",
    "\n",
    "    print('Total addition questions:', len(source_seqs))\n",
    "    \n",
    "    target_letters = list(set([token for ts in target_seqs for token in ts]))\n",
    "    target_letter_to_ix = {ph:i for i,ph in enumerate(target_letters)}\n",
    "    \n",
    "    source_letters = list(set([token for word in source_seqs for token in word]))\n",
    "    source_letter_to_ix = {l:i for i,l in enumerate(source_letters)}\n",
    "    \n",
    "    return np.array(source_seqs), source_letters, source_letter_to_ix, \\\n",
    "           np.array(target_seqs), target_letters, target_letter_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n"
     ]
    }
   ],
   "source": [
    "source_seqs, source_letters, source_letter_to_ix, target_seqs, target_letters, target_letter_to_ix =\\\n",
    "                    generate_data(TRAINING_SIZE, DIGITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00101110110+01110010011 : 10100001001\n",
      "00000111010+00000000001 : 00000111011\n",
      "00000000110+00001101110 : 00001110100\n",
      "00000000100+00001111101 : 00010000001\n",
      "00000000001+00000001000 : 00000001001\n"
     ]
    }
   ],
   "source": [
    "for source, target in zip(source_seqs[:5],target_seqs[:5]):\n",
    "    print( source,':',\"\".join(target[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhoMkqWM4SJI6/w9oDrrNqiPKeQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a5d0a8aa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(list(map(len,target_seqs)),bins=25);\n",
    "\n",
    "# Truncate names longer than MAX_LEN characters. This can be changed\n",
    "MAX_LEN = min([150,max(list(map(len, target_seqs)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast everything from symbols into matrix of int32. Pad with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_matrix(sequences, token_to_i, max_len=None, PAX_ix=-1):\n",
    "    \"\"\"\n",
    "    Converts several sequences of tokens to a matrix, edible a neural network.\n",
    "    Crops at max_len(if given), pads shorter sequences with -1 or PAD_ix.\n",
    "    \"\"\"\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences),max_len),dtype='int8') -1\n",
    "    for i,seq in enumerate(sequences):\n",
    "        \n",
    "        row_ix = [token_to_i.get(_, 0) for _ in seq[:max_len]]\n",
    "        matrix[i,:len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = T.matrix('token sequence', 'int32')\n",
    "output_sequence = T.matrix('target target_letters', 'int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##ENCODER\n",
    "l_in = InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "l_mask = InputLayer(shape=(None, None),input_var=T.neq(input_sequence,-1)) \n",
    "\n",
    "\n",
    "l_emb = non_trainable(EmbeddingLayer(l_in, len(source_letters),  len(source_letters), W=np.diag(np.ones(len(source_letters)))))\n",
    "l_rnn = LSTMLayer(l_emb, HIDDEN_SIZE, only_return_final=True, mask_input=l_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##DECODER\n",
    "dec_in = InputLayer(shape=(None, None),input_var=output_sequence)\n",
    "dec_mask = InputLayer(shape=(None, None),input_var=T.neq(output_sequence,-1))\n",
    "\n",
    "dec_emb = non_trainable(EmbeddingLayer(dec_in, len(target_letters), len(target_letters), W=np.diag(np.ones(len(target_letters)))))\n",
    "dec_rnn = LSTMLayer(dec_emb, num_units=HIDDEN_SIZE, cell_init=l_rnn, mask_input=dec_mask)\n",
    "# WARNING! if it's lstm use cell_init, not hid_init\n",
    "\n",
    "\n",
    "#flatten batch and time to be compatible with feedforward layers (will un-flatten later)\n",
    "dec_rnn_flat = reshape(dec_rnn, (-1,dec_rnn.output_shape[-1]))\n",
    "\n",
    "l_out = DenseLayer(dec_rnn_flat, len(target_letters), nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_model(nn, learning_rate=0.001):\n",
    "    # Model weights\n",
    "    weights = get_all_params(nn)\n",
    "    network_output = get_output(nn)\n",
    "    network_output = network_output.reshape([output_sequence.shape[0],\\\n",
    "                                         output_sequence.shape[1], -1])\n",
    "    predictions_flat = network_output[:,:-1,:].reshape([-1,len(target_letters)])\n",
    "    targets = output_sequence[:,1:].ravel()\n",
    "\n",
    "    #do not count loss for '-1' tokens\n",
    "    mask = T.nonzero(T.neq(targets,-1))\n",
    "\n",
    "    loss = lasagne.objectives.categorical_crossentropy(predictions_flat[mask], targets[mask]).mean()\n",
    "    updates = lasagne.updates.adam(loss, weights, learning_rate=learning_rate)\n",
    "    #training\n",
    "    train = theano.function([input_sequence, output_sequence], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    #computing loss without training\n",
    "    compute_cost = theano.function([input_sequence, output_sequence], loss, allow_input_downcast=True)\n",
    "    #compile the function that computes probabilities for next token given previous text.\n",
    "\n",
    "    last_probas =network_output[:, -1]\n",
    "\n",
    "    probs = theano.function([input_sequence, output_sequence], last_probas)\n",
    "    return train, compute_cost, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "We now need to implement a function that generates output sequence given input.\n",
    "\n",
    "Such function must work thusly:\n",
    "```\n",
    "Init:\n",
    "x = input\n",
    "y = [\"START\"]\n",
    "\n",
    "While not_too_long:\n",
    "  p(y_next|x,y) = probabilities of next letter for y\n",
    "  \n",
    "  y_next ~ p(y_next|x,y)\n",
    "  \n",
    "  y.append(y_next)\n",
    "  \n",
    "  if y_next == \"END\":\n",
    "      break\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_output(input, probs, target_letters, target_letter_to_ix, source_letter_to_ix,\n",
    "                    output_prefix = (\"START\",),\n",
    "                    END_token=\"END\",\n",
    "                    temperature=1,\n",
    "                    sample=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement a function that generates output sequence given input.\n",
    "    \n",
    "    We recommend (but not require) you to use the pseudo-code above and inline instructions.\n",
    "    \"\"\"\n",
    "    x = as_matrix([input], source_letter_to_ix) \n",
    "    output = list(output_prefix)\n",
    "    while True:\n",
    "        y = as_matrix([output], target_letter_to_ix)\n",
    "        next_y_probs = probs(x, y)\n",
    "        next_y_probs = (next_y_probs ** temperature) / (next_y_probs ** temperature).sum()\n",
    "        if sample:\n",
    "            next_y = np.random.choice(target_letters, p=next_y_probs[0])\n",
    "        else:\n",
    "            next_y = target_letters[next_y_probs[0].argmax()]\n",
    "        next_y = str(next_y)             \n",
    "        assert type(next_y) is str, \"please return token(string/character), not it's index\"\n",
    "        \n",
    "        output.append(next_y)\n",
    "\n",
    "        if next_y==END_token:\n",
    "            break\n",
    "            \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#source_seqs = np.array(source_seqs)\n",
    "#target_seqs = np.array(target_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(source_seqs, source_letter_to_ix, target_seqs, target_letter_to_ix, batch_size):\n",
    "    \"\"\"samples a random batch of source and target sequences, batch_size elements\"\"\"\n",
    "    batch_ix = np.random.randint(0,len(source_seqs),size=batch_size)\n",
    "    source_seqs_batch=as_matrix(source_seqs[batch_ix], source_letter_to_ix) \n",
    "    target_seqs_batch=as_matrix(target_seqs[batch_ix], target_letter_to_ix)\n",
    "    \n",
    "    return source_seqs_batch,target_seqs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train average loss = 0.6724414123654577\n",
      "Epoch 0 val average loss = 0.6359176307661706\n",
      "00011000011+00000010111 : 01110010001  |  00011011010\n",
      "00000000111+00011011010 : 10101000101  |  00011100001\n",
      "01010100111+00011111001 : 01001111111  |  01110100000\n",
      "00101010001+00000100010 : 00111101100  |  00101110011\n",
      "01011111101+00001111100 : 10010011111  |  01101111001\n",
      "Epoch 1 train average loss = 0.6332304225555356\n",
      "Epoch 1 val average loss = 0.6151244237697028\n",
      "01100101011+00000011100 : 10100011010  |  01101000111\n",
      "00111000100+11101100010 : 00100010001  |  100100100110\n",
      "00000011111+10110101101 : 01111010001  |  10111001100\n",
      "00011101010+00111110011 : 01100101100  |  01011011101\n",
      "00000001100+00000000110 : 00011110000  |  00000010010\n",
      "Epoch 2 train average loss = 0.5569978549373689\n",
      "Epoch 2 val average loss = 0.5014640153982893\n",
      "00000001000+01001110111 : 00111100001  |  01001111111\n",
      "00011010110+00000100001 : 00110011010  |  00011110111\n",
      "00001010011+00000011100 : 00010110101  |  00001101111\n",
      "10101010111+00000011110 : 10010111000  |  10101110101\n",
      "00000001101+01110101111 : 10010111110  |  01110111100\n",
      "Epoch 3 train average loss = 0.47466123178140385\n",
      "Epoch 3 val average loss = 0.4424183154426903\n",
      "00010010101+00000011001 : 00010100111  |  00010101110\n",
      "00010000100+00000000101 : 00010001110  |  00010001001\n",
      "00010110101+00000101000 : 00010111111  |  00011011101\n",
      "00001111010+00111111110 : 01100000110  |  01001111000\n",
      "00000000010+01000110111 : 01000110111  |  01000111001\n",
      "Epoch 4 train average loss = 0.4326897842024958\n",
      "Epoch 4 val average loss = 0.3979989301490469\n",
      "00000000111+00101101111 : 00110010111  |  00101110110\n",
      "00000000110+00101000110 : 00011111111  |  00101001100\n",
      "00000000010+11010101111 : 11001011101  |  11010110001\n",
      "00000001000+00111110101 : 01000001101  |  00111111101\n",
      "11100001111+00000000101 : 11010111111  |  11100010100\n",
      "Epoch 5 train average loss = 0.41143500422458734\n",
      "Epoch 5 val average loss = 0.3770848824900366\n",
      "00001110001+00100001001 : 00101101001  |  00101111010\n",
      "00000011000+00011100101 : 00100011001  |  00011111101\n",
      "00000000001+01101110010 : 01110000001  |  01101110011\n",
      "00100000001+01101111111 : 01111111100  |  10010000000\n",
      "01111110011+00000010101 : 01111111101  |  10000001000\n",
      "Epoch 6 train average loss = 0.403828408121409\n",
      "Epoch 6 val average loss = 0.3722001015331074\n",
      "00010000010+01110000001 : 10000100110  |  10000000011\n",
      "00000000101+11101110010 : 11101110110  |  11101110111\n",
      "00000110101+11010010110 : 11011011100  |  11011001011\n",
      "00001110000+00001111101 : 00011111101  |  00011101101\n",
      "00000011110+00101000101 : 00101101010  |  00101100011\n",
      "Epoch 7 train average loss = 0.3948708603305863\n",
      "Epoch 7 val average loss = 0.35003286594141075\n",
      "01101100010+00000000110 : 01101111100  |  01101101000\n",
      "00011111100+00001111010 : 00101110001  |  00101110110\n",
      "00001101011+00011001011 : 00101000011  |  00100110110\n",
      "00110001110+00000011010 : 00110100000  |  00110101000\n",
      "01010100010+00000001101 : 01010101110  |  01010101111\n",
      "Epoch 8 train average loss = 0.3703969493850978\n",
      "Epoch 8 val average loss = 0.3855575133894803\n",
      "10010101111+00000011111 : 10100100001  |  10011001110\n",
      "00000010110+00001101111 : 00010000101  |  00010000101\n",
      "00011010010+00001100001 : 00101001111  |  00100110011\n",
      "00000111110+00000000000 : 00001000001  |  00000111110\n",
      "00000110100+00001100011 : 00010010011  |  00010010111\n",
      "Epoch 9 train average loss = 0.3629303757345759\n",
      "Epoch 9 val average loss = 0.3345096911330166\n",
      "00000010111+01101010010 : 01101110000  |  01101101001\n",
      "10010000101+00001100100 : 10011111010  |  10011101001\n",
      "01011101010+00000000010 : 01100000010  |  01011101100\n",
      "00000000100+00101010101 : 00101100011  |  00101011001\n",
      "00001001010+00011101100 : 00100111010  |  00100110110\n",
      "Epoch 10 train average loss = 0.35700173191685963\n",
      "Epoch 10 val average loss = 0.360228792967068\n",
      "11101000001+01000010111 : 11111011111  |  100101011000\n",
      "00000000111+01010011101 : 01010110100  |  01010100100\n",
      "00101001101+00000000111 : 00101011000  |  00101010100\n",
      "00000001001+00110100010 : 00111000110  |  00110101011\n",
      "00000111010+01100011110 : 01101100110  |  01101011000\n",
      "Epoch 11 train average loss = 0.34915144658465286\n",
      "Epoch 11 val average loss = 0.3844289477780699\n",
      "01101100101+00011101000 : 10000111110  |  10001001101\n",
      "00111111111+11001111100 : 11111111110  |  100001111011\n",
      "00011001111+00001000101 : 00100100001  |  00100010100\n",
      "11011110001+00000000010 : 11100111101  |  11011110011\n",
      "00001011101+00111000011 : 01000100010  |  01000100000\n",
      "Epoch 12 train average loss = 0.34552410021606805\n",
      "Epoch 12 val average loss = 0.34636630903414173\n",
      "00000011101+00001000101 : 00001101011  |  00001100010\n",
      "00111001001+01111010010 : 11000000011  |  10110011011\n",
      "00001110100+00000001001 : 00001110101  |  00001111101\n",
      "01000110101+00000000101 : 01000101010  |  01000111010\n",
      "00001111001+00101100111 : 01000001100  |  00111100000\n",
      "Epoch 13 train average loss = 0.33777122774373863\n",
      "Epoch 13 val average loss = 0.33132301488169524\n",
      "00000000000+10000011100 : 10000011011  |  10000011100\n",
      "01011100010+01100100010 : 10111111100  |  11000000100\n",
      "00100010011+00110010001 : 01010011101  |  01010100100\n",
      "00000000111+00110001101 : 01000000000  |  00110010100\n",
      "00001110001+00000001101 : 00001110001  |  00001111110\n",
      "Epoch 14 train average loss = 0.32833026712319796\n",
      "Epoch 14 val average loss = 0.3988324791681348\n",
      "00110001001+00000101110 : 00110111100  |  00110110111\n",
      "00100000111+00000000011 : 00011111111  |  00100001010\n",
      "00010011111+00101000001 : 00111110100  |  00111100000\n",
      "10100101001+00101111010 : 11011110111  |  11010100011\n",
      "10111110000+00000100000 : 10111111001  |  11000010000\n",
      "Epoch 15 train average loss = 0.32723771835100307\n",
      "Epoch 15 val average loss = 0.3334226392982268\n",
      "00000111000+00001001001 : 00001110110  |  00010000001\n",
      "00000111010+00011011000 : 00100000100  |  00100010010\n",
      "00010111110+00000101010 : 00011100010  |  00011101000\n",
      "00001100000+00100001001 : 00101011111  |  00101101001\n",
      "00000101011+00001101011 : 00010001100  |  00010010110\n",
      "Epoch 16 train average loss = 0.33448844208421913\n",
      "Epoch 16 val average loss = 0.328875574708421\n",
      "00000000010+01001011110 : 01001100001  |  01001100000\n",
      "00101001100+10100101101 : 11001001111  |  11001111001\n",
      "00000100100+00100001111 : 00100100010  |  00100110011\n",
      "00001100001+01110110111 : 10000100111  |  10000011000\n",
      "00001011101+00000010111 : 00001110110  |  00001110100\n",
      "Epoch 17 train average loss = 0.3244604184950947\n",
      "Epoch 17 val average loss = 0.33551716929856795\n",
      "00000110001+00011111100 : 00100110110  |  00100101101\n",
      "00000100001+01000100000 : 01001101000  |  01001000001\n",
      "00000000101+11000010100 : 11001010011  |  11000011001\n",
      "00001001110+00110111011 : 00111111010  |  01000001001\n",
      "00000001010+10111111010 : 11000111000  |  11000000100\n",
      "Epoch 18 train average loss = 0.3173264617960711\n",
      "Epoch 18 val average loss = 0.2983767606607282\n",
      "00101100110+00000001110 : 00101110101  |  00101110100\n",
      "00000101110+00011111011 : 00100100100  |  00100101001\n",
      "00001001110+01010110101 : 01100000011  |  01100000011\n",
      "00001010011+00001000000 : 00010001100  |  00010010011\n",
      "11000101001+00000011010 : 11001010000  |  11001000011\n",
      "Epoch 19 train average loss = 0.31306769024071834\n",
      "Epoch 19 val average loss = 0.31256049327207647\n",
      "00010010101+00000011111 : 00010110010  |  00010110100\n",
      "00001100011+00100101001 : 00110000011  |  00110001100\n",
      "00000001100+01111011101 : 01111110110  |  01111101001\n",
      "11110000000+01100010101 : 11111110110  |  101010010101\n",
      "00000000111+11011110001 : 11100111100  |  11011111000\n",
      "Epoch 20 train average loss = 0.31186502312816305\n",
      "Epoch 20 val average loss = 0.36651906748175883\n",
      "11110110101+00010100011 : 11111111001  |  100001011000\n",
      "01110101010+00001110001 : 10000110010  |  10000011011\n",
      "00000110111+00000010011 : 00001001010  |  00001001010\n",
      "00101011111+00000001111 : 00110000001  |  00101101110\n",
      "00000111000+01011001001 : 01100001011  |  01100000001\n",
      "Epoch 21 train average loss = 0.31324271806149284\n",
      "Epoch 21 val average loss = 0.29534484484093687\n",
      "00010111010+00000000110 : 00011000100  |  00011000000\n",
      "10101001001+10110110110 : 110000001010  |  101011111111\n",
      "00000110111+00111000100 : 00111110001  |  00111111011\n",
      "00011011110+01101001011 : 10000001111  |  10000101001\n",
      "00000000101+10011011000 : 10100000111  |  10011011101\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=30\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "train_batches_per_epoch = 500\n",
    "val_batches_per_epoch = 50\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "train_source_seqs, val_source_seqs, train_target_seqs, val_target_seqs = train_test_split(source_seqs, target_seqs,\\\n",
    "                                                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        train_avg_cost = 0;\n",
    "        val_avg_cost = 0;\n",
    "\n",
    "        for _ in range(train_batches_per_epoch):\n",
    "            x,y = sample_batch(train_source_seqs, source_letter_to_ix, train_target_seqs, target_letter_to_ix, batch_size)\n",
    "            train_avg_cost += train(x, y).mean()\n",
    "        \n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            x,y = sample_batch(val_source_seqs, source_letter_to_ix, val_target_seqs, target_letter_to_ix, batch_size)\n",
    "            val_avg_cost += compute_cost(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} train average loss = {}\".format(epoch, train_avg_cost / train_batches_per_epoch))\n",
    "        print(\"Epoch {} val average loss = {}\".format(epoch, val_avg_cost / val_batches_per_epoch))\n",
    "        \n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(val_source_seqs))\n",
    "            print (val_source_seqs[ind],':', ''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(val_target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bazal module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CerMemory(lasagne.layers.Layer):\n",
    "    def __init__(self, incoming, memory_size, M=lasagne.init.Orthogonal(), **kwargs):\n",
    "        super(CerMemory, self).__init__(incoming, **kwargs)\n",
    "        self.query_shape = self.input_shape[1]\n",
    "        self.memory_size = memory_size\n",
    "        self.M = self.add_param(M, (self.query_shape, memory_size), name='M')\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        m = self.M / T.sqrt(T.sqr(self.M).sum(axis=0)).reshape(self.M.shape[1], 1)\n",
    "        weights =  T.dot(input, m)\n",
    "        return T.dot(weights, m.T)\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], self.query_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EvcNormalizer(lasagne.layers.Layer):\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return (input.T / T.sqrt(T.sqr(input).sum(axis=1)).reshape(input.shape[0], 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_SIZE = 16\n",
    "MEMORY_SIZE = 64\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bazal_model(query_size, memory_size, hidden_size, memory_benchmark=False, bidir_features=False):\n",
    "\n",
    "    ##ENCODER\n",
    "    l_in = InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "    l_mask = InputLayer(shape=(None, None),input_var=T.neq(input_sequence,-1)) \n",
    "\n",
    "\n",
    "    l_emb = non_trainable(EmbeddingLayer(l_in, len(source_letters),  len(source_letters), W=np.diag(np.ones(len(source_letters)))))\n",
    "\n",
    "    features = LSTMLayer(l_emb, HIDDEN_SIZE, only_return_final=True, mask_input=l_mask)\n",
    "    features_backward = LSTMLayer(l_emb, HIDDEN_SIZE, only_return_final=True, mask_input=l_mask, backwards=True)\n",
    "    if bidir_features:\n",
    "        features = ConcatLayer([features, features_backward])\n",
    "    \n",
    "    if not memory_benchmark:\n",
    "        ## QUERY BUILDER\n",
    "        query = DenseLayer(features, QUERY_SIZE, nonlinearity=None)\n",
    "        query = EvcNormalizer(query)\n",
    "        ## Memory\n",
    "        memory = CerMemory(query, MEMORY_SIZE)\n",
    "    else:\n",
    "        memory = DenseLayer(DenseLayer(features, QUERY_SIZE), QUERY_SIZE)\n",
    "        \n",
    "    to_decode = ConcatLayer([features, memory])\n",
    "    \n",
    "    ##DECODER\n",
    "    dec_in = InputLayer(shape=(None, None),input_var=output_sequence)\n",
    "    dec_mask = InputLayer(shape=(None, None),input_var=T.neq(output_sequence,-1))\n",
    "\n",
    "    dec_emb = non_trainable(EmbeddingLayer(dec_in, len(target_letters), len(target_letters), W=np.diag(np.ones(len(target_letters)))))\n",
    "    dec_rnn = LSTMLayer(dec_emb, num_units=to_decode.output_shape[-1], cell_init=to_decode, mask_input=dec_mask)\n",
    "    # WARNING! if it's lstm use cell_init, not hid_init\n",
    "\n",
    "\n",
    "    #flatten batch and time to be compatible with feedforward layers (will un-flatten later)\n",
    "    dec_rnn_flat = reshape(dec_rnn, (-1,dec_rnn.output_shape[-1]))\n",
    "\n",
    "    l_out = DenseLayer(dec_rnn_flat, len(target_letters), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out, memory = bazal_model(QUERY_SIZE, MEMORY_SIZE, HIDDEN_SIZE, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train average loss = 0.6678547107710676\n",
      "Epoch 0 val average loss = 0.639111345864788\n",
      "01110001100+00000000001 : 01010110000  |  01110001101\n",
      "00111010100+01111001011 : 11011110000  |  10110011111\n",
      "01010100110+00000000101 : 00001011010  |  01010101011\n",
      "00000100111+00001110001 : 10001101001  |  00010011000\n",
      "10110101111+00000000001 : 00111000001  |  10110110000\n",
      "Epoch 1 train average loss = 0.6409741155574062\n",
      "Epoch 1 val average loss = 0.6351519512813915\n",
      "00100010001+11111001111 : 01111110110  |  100011100000\n",
      "00111010010+01111110001 : 00011110011  |  10111000011\n",
      "00001001100+00001010000 : 10101000011  |  00010011100\n",
      "00000001010+10101101001 : 00101111110  |  10101110011\n",
      "00011000010+00011000001 : 01011111000  |  00110000011\n",
      "Epoch 2 train average loss = 0.6256393621391105\n",
      "Epoch 2 val average loss = 0.5706847803796581\n",
      "11100100111+00000111001 : 100011101010  |  11101100000\n",
      "00000111010+01011011101 : 01100110111  |  01100010111\n",
      "00100011110+00001111110 : 00010111011  |  00110011100\n",
      "00000100111+00111110010 : 01110110011  |  01000011001\n",
      "00000000010+10001111001 : 01010100010  |  10001111011\n",
      "Epoch 3 train average loss = 0.5002466205205514\n",
      "Epoch 3 val average loss = 0.4599768276864621\n",
      "00000010001+11100011011 : 10101100100  |  11100101100\n",
      "00010110001+00101001110 : 01000100100  |  00111111111\n",
      "01001101101+00011111011 : 01011110100  |  01101101000\n",
      "01111100001+00000001010 : 10000010111  |  01111101011\n",
      "00000001001+11110010011 : 11110110011  |  11110011100\n",
      "Epoch 4 train average loss = 0.4419299483941976\n",
      "Epoch 4 val average loss = 0.43295300304638057\n",
      "00001111110+00000101010 : 00010000110  |  00010101000\n",
      "00110011011+00001001001 : 00110110111  |  00111100100\n",
      "00000011110+00000011111 : 00001000111  |  00000111101\n",
      "00000010111+00100111101 : 00100100000  |  00101010100\n",
      "01010000000+00011011001 : 01101000110  |  01101011001\n",
      "Epoch 5 train average loss = 0.39109811334074\n",
      "Epoch 5 val average loss = 0.3996712720840575\n",
      "00010000101+00000110111 : 00010011011  |  00010111100\n",
      "00001000111+00000001000 : 00001001011  |  00001001111\n",
      "01000001001+00000000101 : 00111110101  |  01000001110\n",
      "11110000100+00000110010 : 11110110100  |  11110110110\n",
      "00001000110+00101000011 : 00101110011  |  00110001001\n",
      "Epoch 6 train average loss = 0.3636756230188843\n",
      "Epoch 6 val average loss = 0.33853537153353785\n",
      "00000010110+00001101111 : 00010001011  |  00010000101\n",
      "00110001001+00001110000 : 00111110000  |  00111111001\n",
      "00000100110+10001111010 : 10001111110  |  10010100000\n",
      "00000010001+10101111101 : 10110010101  |  10110001110\n",
      "00010101010+00000011111 : 00011000110  |  00011001001\n",
      "Epoch 7 train average loss = 0.34511461020626033\n",
      "Epoch 7 val average loss = 0.31020758665925086\n",
      "00001101111+11100111010 : 11110100111  |  11110101001\n",
      "00111001110+00011001100 : 01010001011  |  01010011010\n",
      "11101010001+10011101001 : 101010001011  |  110000111010\n",
      "01100111010+00000000010 : 01101000110  |  01100111100\n",
      "01010001011+00000000000 : 01010000110  |  01010001011\n",
      "Epoch 8 train average loss = 0.3313227189829503\n",
      "Epoch 8 val average loss = 0.3060239491128298\n",
      "00011001101+00100100001 : 00111101010  |  00111101110\n",
      "00010011101+00000110110 : 00011001110  |  00011010011\n",
      "00000000000+01111011110 : 01111011011  |  01111011110\n",
      "00001111011+00001111001 : 00100000000  |  00011110100\n",
      "01010011111+00000011000 : 01010111100  |  01010110111\n",
      "Epoch 9 train average loss = 0.32294239647474926\n",
      "Epoch 9 val average loss = 0.3506438957485704\n",
      "10110100110+00000001000 : 11000000101  |  10110101110\n",
      "00000001000+00100110000 : 00100100010  |  00100111000\n",
      "01111000001+00000111110 : 10000001001  |  01111111111\n",
      "01100111001+00000001000 : 01100101101  |  01101000001\n",
      "00000110111+00100001001 : 00100100110  |  00101000000\n",
      "Epoch 10 train average loss = 0.3179151285921401\n",
      "Epoch 10 val average loss = 0.36426420293822903\n",
      "01010101100+01001100101 : 10011010110  |  10100010001\n",
      "00000000000+11010111001 : 11010101010  |  11010111001\n",
      "01001100110+00000011100 : 01010000001  |  01010000010\n",
      "00000011010+01100011111 : 01100100101  |  01100111001\n",
      "00100110000+00000111101 : 00101101100  |  00101101101\n",
      "Epoch 11 train average loss = 0.3083929457914375\n",
      "Epoch 11 val average loss = 0.2938423502166725\n",
      "01000100111+00000000001 : 01000100100  |  01000101000\n",
      "01011101101+01101111111 : 10111111100  |  11001101100\n",
      "00000011000+10001111111 : 10010010011  |  10010010111\n",
      "01011011100+10000001000 : 11011101111  |  11011100100\n",
      "00000101001+00010011010 : 00011000000  |  00011000011\n",
      "Epoch 12 train average loss = 0.30338151567614063\n",
      "Epoch 12 val average loss = 0.2825078227140534\n",
      "00001101110+00010011110 : 00100010001  |  00100001100\n",
      "00000001011+00010011011 : 00010100001  |  00010100110\n",
      "00000000110+00010110010 : 00010111010  |  00010111000\n",
      "01001111000+00011101110 : 01101100011  |  01101100110\n",
      "11000100010+00000010000 : 10111101110  |  11000110010\n",
      "Epoch 13 train average loss = 0.2978446280874887\n",
      "Epoch 13 val average loss = 0.32765811255924127\n",
      "11111011100+00111111010 : 100110000001  |  100111010110\n",
      "00011111110+00000000000 : 00011111101  |  00011111110\n",
      "00000000011+01101111111 : 01101101110  |  01110000010\n",
      "01101000101+00001011100 : 01110010010  |  01110100001\n",
      "00001100000+00000101111 : 00010001001  |  00010001111\n",
      "Epoch 14 train average loss = 0.2972146610933252\n",
      "Epoch 14 val average loss = 0.29113748752851243\n",
      "00100111100+00000001100 : 00101000000  |  00101001000\n",
      "00111100111+00000010011 : 00111110111  |  00111111010\n",
      "00001111110+00000101010 : 00010101110  |  00010101000\n",
      "00001110101+11001010000 : 11010110001  |  11011000101\n",
      "01110010010+00000000100 : 01110010000  |  01110010110\n",
      "Epoch 15 train average loss = 0.2931464783397919\n",
      "Epoch 15 val average loss = 0.2653364868408116\n",
      "00000010100+00001011010 : 00001101100  |  00001101110\n",
      "01101011110+00010001111 : 01111101100  |  01111101101\n",
      "00101101110+00000000000 : 00101100101  |  00101101110\n",
      "00000101110+00000010000 : 00000111110  |  00000111110\n",
      "00110101111+10001011110 : 10111110100  |  11000001101\n",
      "Epoch 16 train average loss = 0.28299233639145\n",
      "Epoch 16 val average loss = 0.25356735122417706\n",
      "00011101001+00000001110 : 00011110111  |  00011110111\n",
      "00110001100+00100100001 : 01010110001  |  01010101101\n",
      "00000000000+10111111010 : 11000000100  |  10111111010\n",
      "00000010000+01001010101 : 01001101010  |  01001100101\n",
      "00000011000+00010101010 : 00011000101  |  00011000010\n",
      "Epoch 17 train average loss = 0.27961944004507044\n",
      "Epoch 17 val average loss = 0.25420653155991263\n",
      "00011100001+00001111001 : 00101010111  |  00101011010\n",
      "00010111010+00001000000 : 00011111111  |  00011111010\n",
      "00110011111+00011111101 : 01010010100  |  01010011100\n",
      "01000101001+00000011011 : 01000111110  |  01001000100\n",
      "00000101110+11101100011 : 11110010001  |  11110010001\n",
      "Epoch 18 train average loss = 0.28132925345638565\n",
      "Epoch 18 val average loss = 0.24549325977864891\n",
      "00000011010+00011101101 : 00100001001  |  00100000111\n",
      "00000000000+00110010010 : 00110010110  |  00110010010\n",
      "00011000100+00101010101 : 01000100000  |  01000011001\n",
      "00000001010+00100001011 : 00100011001  |  00100010101\n",
      "00011110011+01110010011 : 10010010010  |  10010000110\n",
      "Epoch 19 train average loss = 0.27862354930503147\n",
      "Epoch 19 val average loss = 0.23479656198452503\n",
      "11100001010+00111111011 : 100011101111  |  100100000101\n",
      "00000111000+00011011010 : 00100010111  |  00100010010\n",
      "00110001101+00000000101 : 00110010001  |  00110010010\n",
      "00000111111+00100000100 : 00101000100  |  00101000011\n",
      "00100010001+00111100111 : 01011110011  |  01011111000\n",
      "Epoch 20 train average loss = 0.2809566177658407\n",
      "Epoch 20 val average loss = 0.23848795089495667\n",
      "00000001010+00001000010 : 00001001101  |  00001001100\n",
      "11111011101+00100011000 : 100100110100  |  100011110101\n",
      "00111110000+00000111010 : 01000100110  |  01000101010\n",
      "00111001101+01000110010 : 10000001111  |  01111111111\n",
      "00010011111+00000000001 : 00010100000  |  00010100000\n",
      "Epoch 21 train average loss = 0.2756210646675564\n",
      "Epoch 21 val average loss = 0.281513419985703\n",
      "11101110110+00000000110 : 11110000001  |  11101111100\n",
      "00000001000+00100101011 : 00100110101  |  00100110011\n",
      "00000110111+01100100000 : 01101100101  |  01101010111\n",
      "00001110010+00000110000 : 00010100000  |  00010100010\n",
      "00111001010+00000001001 : 00111001110  |  00111010011\n",
      "Epoch 22 train average loss = 0.26640089835302444\n",
      "Epoch 22 val average loss = 0.24994886793653495\n",
      "00101010110+00000100110 : 00110000001  |  00101111100\n",
      "00000000101+00001011010 : 00001011111  |  00001011111\n",
      "01000111000+00000000000 : 01000110001  |  01000111000\n",
      "00111001010+00000001100 : 00111010010  |  00111010110\n",
      "00010001001+00000110011 : 00010111110  |  00010111100\n",
      "Epoch 23 train average loss = 0.2668023015658794\n",
      "Epoch 23 val average loss = 0.24715399324646664\n",
      "00001111110+11110101010 : 100000010011  |  100000101000\n",
      "00001001000+01010001000 : 01011000011  |  01011010000\n",
      "11110011111+00000000110 : 11110001111  |  11110100101\n",
      "01010100011+00011010101 : 01101111001  |  01101111000\n",
      "01001000101+00110100100 : 01111101001  |  01111101001\n",
      "Epoch 24 train average loss = 0.2631480669730499\n",
      "Epoch 24 val average loss = 0.2370601147044365\n",
      "00000100100+00001111101 : 00010101000  |  00010100001\n",
      "00011001010+00001100111 : 00100101101  |  00100110001\n",
      "10100101100+00000101010 : 10101000111  |  10101010110\n",
      "00110011011+00001101010 : 01000000111  |  01000000101\n",
      "00011011111+00100000010 : 00111100001  |  00111100001\n",
      "Epoch 25 train average loss = 0.2659147596467033\n",
      "Epoch 25 val average loss = 0.2530526240871042\n",
      "11101010111+01001110111 : 100110000001  |  100111001110\n",
      "00000011110+00000101101 : 00001000111  |  00001001011\n",
      "01001011110+00010111011 : 01100011100  |  01100011001\n",
      "00011010000+01111111011 : 10011011111  |  10011001011\n",
      "01010001111+00000001011 : 01010100011  |  01010011010\n",
      "Epoch 26 train average loss = 0.26150573311657666\n",
      "Epoch 26 val average loss = 0.2691687671048836\n",
      "00010000111+00011000100 : 00101000100  |  00101001011\n",
      "00000100110+00011111111 : 00100100001  |  00100100101\n",
      "10111110001+00000000000 : 10111100101  |  10111110001\n",
      "11001101001+00110011100 : 100000001000  |  100000000101\n",
      "00000100101+10000011100 : 10001000000  |  10001000001\n",
      "Epoch 27 train average loss = 0.2675474589661272\n",
      "Epoch 27 val average loss = 0.2395847137107382\n",
      "00000110111+00011110110 : 00100101101  |  00100101101\n",
      "01101100100+00000001111 : 01101101111  |  01101110011\n",
      "00000000010+01000110111 : 01000111010  |  01000111001\n",
      "01100101011+00001111110 : 01110101101  |  01110101001\n",
      "00000011110+00001000001 : 00001011101  |  00001011111\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=30\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "train_batches_per_epoch = 500\n",
    "val_batches_per_epoch = 50\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "train_source_seqs, val_source_seqs, train_target_seqs, val_target_seqs = train_test_split(source_seqs, target_seqs,\\\n",
    "                                                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        train_avg_cost = 0;\n",
    "        val_avg_cost = 0;\n",
    "\n",
    "        for _ in range(train_batches_per_epoch):\n",
    "            x,y = sample_batch(train_source_seqs, source_letter_to_ix, train_target_seqs, target_letter_to_ix, batch_size)\n",
    "            train_avg_cost += train(x, y).mean()\n",
    "        \n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            x,y = sample_batch(val_source_seqs, source_letter_to_ix, val_target_seqs, target_letter_to_ix, batch_size)\n",
    "            val_avg_cost += compute_cost(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} train average loss = {}\".format(epoch, train_avg_cost / train_batches_per_epoch))\n",
    "        print(\"Epoch {} val average loss = {}\".format(epoch, val_avg_cost / val_batches_per_epoch))\n",
    "        \n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(val_source_seqs))\n",
    "            print (val_source_seqs[ind],':', ''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(val_target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M_add = np.array(memory.M.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./zoo/addprob_memory_after30epochs.npy', M_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same for prod problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 100000\n",
    "DIGITS = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 100000\n"
     ]
    }
   ],
   "source": [
    "source_seqs, source_letters, source_letter_to_ix, target_seqs, target_letters, target_letter_to_ix =\\\n",
    "                    generate_data(TRAINING_SIZE, DIGITS, lambda a, b: a * b, '{}*{}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00001100100*00000000010 : 00011001000\n",
      "00010001000*00001111000 : 11111111000000\n",
      "00001001110*00000000111 : 01000100010\n",
      "00001111011*00000010111 : 101100001101\n",
      "00000000011*00000000001 : 00000000011\n"
     ]
    }
   ],
   "source": [
    "for source, target in zip(source_seqs[:5],target_seqs[:5]):\n",
    "    print( source,':',\"\".join(target[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_SIZE = 16\n",
    "MEMORY_SIZE = 64\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out, memory = bazal_model(QUERY_SIZE, MEMORY_SIZE, HIDDEN_SIZE,  False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train average loss = 0.5591780107772621\n",
      "Epoch 0 val average loss = 0.602365886189771\n",
      "False\n",
      "00001101100*00000001110 : 01101001110  |  10111101000\n",
      "False\n",
      "00000000010*10011100100 : 111100010000  |  100111001000\n",
      "False\n",
      "00000010101*01111001001 : 110011111100100  |  100111101111101\n",
      "False\n",
      "01011001000*00010100011 : 11100101001110000  |  11100010101011000\n",
      "False\n",
      "00000111100*01010111101 : 1000001110010010  |  1010010001001100\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=1000\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "train_batches_per_epoch = 500\n",
    "val_batches_per_epoch = 50\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "train_source_seqs, val_source_seqs, train_target_seqs, val_target_seqs = train_test_split(source_seqs, target_seqs,\\\n",
    "                                                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        train_avg_cost = 0;\n",
    "        val_avg_cost = 0;\n",
    "\n",
    "        for _ in range(train_batches_per_epoch):\n",
    "            x,y = sample_batch(train_source_seqs, source_letter_to_ix, train_target_seqs, target_letter_to_ix, batch_size)\n",
    "            train_avg_cost += train(x, y).mean()\n",
    "        \n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            x,y = sample_batch(val_source_seqs, source_letter_to_ix, val_target_seqs, target_letter_to_ix, batch_size)\n",
    "            val_avg_cost += compute_cost(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} train average loss = {}\".format(epoch, train_avg_cost / train_batches_per_epoch))\n",
    "        print(\"Epoch {} val average loss = {}\".format(epoch, val_avg_cost / val_batches_per_epoch))\n",
    "        \n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(val_source_seqs))\n",
    "            print (''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1])==''.join(val_target_seqs[ind][1:-1]))\n",
    "            print (val_source_seqs[ind],':', ''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(val_target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'719*101'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'*'.join(map(lambda x: str(int(x, 2)), val_source_seqs[ind].split('*')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grand problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob1, prob_op1 = lambda a, b, c: (a + b) * c, '({}+{})*{}'\n",
    "prob2, prob_op2 = lambda a, b, c:  a * (b + c), '{}*({}+{})'\n",
    "prob3, prob_op3 = lambda a, b, c:  a + b * c, '{}+{}*{}'\n",
    "prob4, prob_op4 = lambda a, b, c:  a * b + c, '{}*{}+{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_grand_data(size, digits, problems = [prob1, prob2, prob3, prob4],\\\n",
    "                        problem_operators=[prob_op1, prob_op2, prob_op3, prob_op4]):\n",
    "    source_seqs = []\n",
    "    target_seqs = []\n",
    "    seen = set()\n",
    "    assert len(problem_operators) == len(problems)\n",
    "    print('Generating data...')\n",
    "    while len(source_seqs) < TRAINING_SIZE:\n",
    "        f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                        for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "        a, b, c = f(), f(), f()\n",
    "        # Skip any addition questions we've already seen\n",
    "        # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "        key = tuple(sorted((a, b, c)))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        \n",
    "        coin = np.random.randint(0, len(problems))\n",
    "        \n",
    "        # Pad the data with spaces such that it is always MAXLEN.\n",
    "        q = problem_operators[coin].format(a, b, c)\n",
    "        ans = str(problems[coin](a, b, c))\n",
    "\n",
    "        source_seqs.append(q)\n",
    "        target_seqs.append([\"START\"] + list(ans) + [\"END\"])\n",
    "\n",
    "    print('Total addition questions:', len(source_seqs))\n",
    "    \n",
    "    target_letters = list(set([token for ts in target_seqs for token in ts]))\n",
    "    target_letter_to_ix = {ph:i for i,ph in enumerate(target_letters)}\n",
    "    \n",
    "    source_letters = list(set([token for word in source_seqs for token in word]))\n",
    "    source_letter_to_ix = {l:i for i,l in enumerate(source_letters)}\n",
    "    \n",
    "    return np.array(source_seqs), source_letters, source_letter_to_ix, \\\n",
    "           np.array(target_seqs), target_letters, target_letter_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 2000\n",
    "DIGITS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 2000\n"
     ]
    }
   ],
   "source": [
    "source_seqs, source_letters, source_letter_to_ix, target_seqs, target_letters, target_letter_to_ix =\\\n",
    "                    generate_grand_data(TRAINING_SIZE, DIGITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93+3*79 : 330\n",
      "2*0+30 : 30\n",
      "1*(73+45) : 118\n",
      "6*(3+67) : 420\n",
      "69*8+3 : 555\n"
     ]
    }
   ],
   "source": [
    "for source, target in zip(source_seqs[:5],target_seqs[:5]):\n",
    "    print( source,':',\"\".join(target[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_SIZE = 16\n",
    "MEMORY_SIZE = 64\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out, memory = bazal_model(QUERY_SIZE, MEMORY_SIZE, HIDDEN_SIZE,  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 average loss = 1.8074171794116005\n",
      "3*(54+8) : 839  |  186\n",
      "44*7+7 : 229  |  315\n",
      "57+15*4 : 218  |  117\n",
      "43*1+2 : 108  |  45\n",
      "27*2+87 : 608  |  141\n",
      "Epoch 1 average loss = 1.6196723853008468\n",
      "18+7*62 : 303  |  452\n",
      "48+7*4 : 98  |  76\n",
      "8+89*8 : 403  |  720\n",
      "2*(59+96) : 269  |  310\n",
      "(58+6)*8 : 880  |  512\n",
      "Epoch 2 average loss = 1.4874540646206253\n",
      "92+53*92 : 5099  |  4968\n",
      "(78+75)*87 : 7765  |  13311\n",
      "54*(25+0) : 1452  |  1350\n",
      "(1+0)*64 : 40  |  64\n",
      "0*7+1 : 3  |  1\n",
      "Epoch 3 average loss = 1.3032164222313742\n",
      "97*(3+18) : 2709  |  2037\n",
      "57*39+80 : 4453  |  2303\n",
      "74*4+7 : 359  |  303\n",
      "0*(78+6) : 0  |  0\n",
      "(87+7)*48 : 2318  |  4512\n",
      "Epoch 4 average loss = 1.0245313576234003\n",
      "7*8+75 : 103  |  131\n",
      "6+6*93 : 469  |  564\n",
      "1+61*5 : 419  |  306\n",
      "2+59*30 : 2598  |  1772\n",
      "39*42+95 : 1373  |  1733\n",
      "Epoch 5 average loss = 0.6894229277183287\n",
      "74*(2+60) : 1428  |  4588\n",
      "9*6+23 : 79  |  77\n",
      "47*(49+2) : 4234  |  2397\n",
      "7+8*5 : 30  |  47\n",
      "2*69+98 : 208  |  236\n",
      "Epoch 6 average loss = 0.4108774484056355\n",
      "1+20*0 : 4  |  1\n",
      "15*44+65 : 320  |  725\n",
      "36+42*74 : 6432  |  3144\n",
      "93*62+88 : 4708  |  5854\n",
      "(75+92)*9 : 1583  |  1503\n",
      "Epoch 7 average loss = 0.21918465745583113\n",
      "7*52+36 : 400  |  400\n",
      "64+4*43 : 366  |  236\n",
      "52+56*0 : 52  |  52\n",
      "22*(6+44) : 1100  |  1100\n",
      "(85+36)*7 : 614  |  847\n",
      "Epoch 8 average loss = 0.11009148939910773\n",
      "(49+1)*31 : 1550  |  1550\n",
      "(8+1)*87 : 783  |  783\n",
      "21*(11+73) : 1764  |  1764\n",
      "(9+6)*88 : 1320  |  1320\n",
      "11*94+0 : 1034  |  1034\n",
      "Epoch 9 average loss = 0.053998595392924356\n",
      "(55+3)*77 : 4466  |  4466\n",
      "32*8+52 : 270  |  308\n",
      "7*(6+59) : 455  |  455\n",
      "21*(5+1) : 126  |  126\n",
      "19*2+4 : 42  |  42\n",
      "Epoch 10 average loss = 0.08680151892880913\n",
      "63*0+9 : 9  |  9\n",
      "(52+8)*9 : 510  |  540\n",
      "5*6+43 : 73  |  73\n",
      "(96+56)*9 : 1368  |  1368\n",
      "45*14+80 : 710  |  710\n",
      "Epoch 11 average loss = 0.019791177655911787\n",
      "4*(9+14) : 92  |  92\n",
      "55+8*6 : 103  |  103\n",
      "10*(33+6) : 390  |  390\n",
      "30+4*9 : 66  |  66\n",
      "(3+50)*1 : 53  |  53\n",
      "Epoch 12 average loss = 0.012232270473054021\n",
      "63*76+6 : 4794  |  4794\n",
      "(58+93)*2 : 302  |  302\n",
      "36+42*74 : 3144  |  3144\n",
      "68*4+3 : 275  |  275\n",
      "1+10*7 : 71  |  71\n",
      "Epoch 13 average loss = 0.008034974855735894\n",
      "69+2*0 : 69  |  69\n",
      "(3+13)*5 : 80  |  80\n",
      "26*(4+5) : 234  |  234\n",
      "89*(3+8) : 979  |  979\n",
      "(5+71)*38 : 2888  |  2888\n",
      "Epoch 14 average loss = 0.005506300733214235\n",
      "(0+46)*4 : 184  |  184\n",
      "5*(93+0) : 465  |  465\n",
      "97+93*4 : 469  |  469\n",
      "56+60*6 : 416  |  416\n",
      "73+16*8 : 201  |  201\n",
      "Epoch 15 average loss = 0.09774490923958623\n",
      "1*(8+60) : 68  |  68\n",
      "10*76+43 : 803  |  803\n",
      "69+68*0 : 69  |  69\n",
      "(0+34)*0 : 0  |  0\n",
      "63*78+50 : 4964  |  4964\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=30\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "train_batches_per_epoch = 500\n",
    "val_batches_per_epoch = 50\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "train_source_seqs, val_source_seqs, train_target_seqs, val_target_seqs = train_test_split(source_seqs, target_seqs,\\\n",
    "                                                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        train_avg_cost = 0;\n",
    "        val_avg_cost = 0;\n",
    "\n",
    "        for _ in range(train_batches_per_epoch):\n",
    "            x,y = sample_batch(train_source_seqs, source_letter_to_ix, train_target_seqs, target_letter_to_ix, batch_size)\n",
    "            train_avg_cost += train(x, y).mean()\n",
    "        \n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            x,y = sample_batch(val_source_seqs, source_letter_to_ix, val_target_seqs, target_letter_to_ix, batch_size)\n",
    "            val_avg_cost += compute_cost(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} train average loss = {}\".format(epoch, train_avg_cost / train_batches_per_epoch))\n",
    "        print(\"Epoch {} val average loss = {}\".format(epoch, val_avg_cost / val_batches_per_epoch))\n",
    "        \n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(val_source_seqs))\n",
    "            print (val_source_seqs[ind],':', ''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(val_target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
