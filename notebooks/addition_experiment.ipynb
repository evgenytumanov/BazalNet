{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition experiment\n",
    "\n",
    "An implementation of sequence to sequence learning for performing addition\n",
    "\n",
    "* Input: \"535+61\"\n",
    "\n",
    "* Output: \"596\"\n",
    "\n",
    "Padding is handled by using a repeated sentinel character (space)\n",
    "Input may optionally be inverted, shown to increase performance in many tasks in:\n",
    "\"Learning to Execute\"\n",
    "http://arxiv.org/abs/1410.4615\n",
    "and\n",
    "\"Sequence to Sequence Learning with Neural Networks\"\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "Theoretically it introduces shorter term dependencies between source and target.\n",
    "Two digits inverted:\n",
    "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
    "Three digits inverted:\n",
    "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
    "Four digits inverted:\n",
    "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
    "Five digits inverted:\n",
    "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:\n",
      "b'C:\\\\Users\\\\tumanov\\\\AppData\\\\Local\\\\Temp\\\\try_flags_ltil45h8.c:4:19: fatal error: cudnn.h: No such file or directory\\r\\ncompilation terminated.\\r\\n'\n",
      "Mapped name None to device cuda0: GeForce GTX 1070 (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_trainable(net):\n",
    "    for tags in net.params.values():\n",
    "        tags -= {'trainable', 'regularizable'}\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "INVERT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try replacing GRU, or SimpleRNN.\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(size, digits, problem = lambda a, b: a+b, problem_operator='{}+{}'):\n",
    "    source_seqs = []\n",
    "    target_seqs = []\n",
    "    seen = set()\n",
    "    print('Generating data...')\n",
    "    while len(source_seqs) < TRAINING_SIZE:\n",
    "        f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                        for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "        a, b = f(), f()\n",
    "        # Skip any addition questions we've already seen\n",
    "        # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "        key = tuple(sorted((a, b)))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        # Pad the data with spaces such that it is always MAXLEN.\n",
    "        q = problem_operator.format(a, b)\n",
    "        ans = str(problem(a, b))\n",
    "\n",
    "        source_seqs.append(q)\n",
    "        target_seqs.append([\"START\"] + list(ans) + [\"END\"])\n",
    "\n",
    "    print('Total addition questions:', len(source_seqs))\n",
    "    \n",
    "    target_letters = list(set([token for ts in target_seqs for token in ts]))\n",
    "    target_letter_to_ix = {ph:i for i,ph in enumerate(target_letters)}\n",
    "    \n",
    "    source_letters = list(set([token for word in source_seqs for token in word]))\n",
    "    source_letter_to_ix = {l:i for i,l in enumerate(source_letters)}\n",
    "    \n",
    "    return np.array(source_seqs), source_letters, source_letter_to_ix, \\\n",
    "           np.array(target_seqs), target_letters, target_letter_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n"
     ]
    }
   ],
   "source": [
    "source_seqs, source_letters, source_letter_to_ix, target_seqs, target_letters, target_letter_to_ix =\\\n",
    "                    generate_data(TRAINING_SIZE, DIGITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266+547 : 813\n",
      "52+12 : 64\n",
      "59+98 : 157\n",
      "2+809 : 811\n",
      "30+5 : 35\n"
     ]
    }
   ],
   "source": [
    "for source, target in zip(source_seqs[:5],target_seqs[:5]):\n",
    "    print( source,':',\"\".join(target[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFpVJREFUeJzt3X+snuV93/H3JzYjXhoIgTPPsp2ZCGuSQYtTLM9bqiqN\n1eL8UE0kiBypwaosnA0vSrVKHfSPNfnDEvyR0rENT6RkGJoWLNoMK8GZKGTqIs12DynB2ATlKIDw\nkcEOENxsw52d7/54rlM9Pvexz+Nzjn3Osd8v6dZzPd/7up7nunRH+fj+8RxSVUiS1O89sz0BSdLc\nYzhIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdQwcDkkWJPmbJN9u7z+Y5KkkP26vV/X1vSvJSJKX\nktzUV78xyYG2774kafXLkzzW6vuSrJi5JUqSztW5nDl8GXix7/2dwNNVtRJ4ur0nySpgE3A9sAG4\nP8mCNmYHcDuwsm0bWn0L8HZVXQfcC9wzpdVIkmbEwkE6JVkGfBrYDvzbVt4IfLy1dwL/A/h3rf5o\nVZ0AXk4yAqxN8gpwRVXtbZ/5MHAzsKeN+Ur7rMeB/5QkdZafb19zzTW1YsWKQaYvSWqeffbZn1bV\n0GT9BgoH4I+A3wPe31dbXFVHWvt1YHFrLwX29vU73Gr/r7XH18fGvAZQVSeTvANcDfz0TBNasWIF\nw8PDA05fkgSQ5NVB+k16WSnJZ4CjVfXsmfq0f+Gf9z/SlGRrkuEkw8eOHTvfXydJl6xB7jl8DPjN\ndlnoUeATSf4EeCPJEoD2erT1HwWW941f1mqjrT2+ftqYJAuBK4E3x0+kqh6oqjVVtWZoaNKzIknS\nFE0aDlV1V1Utq6oV9G40P1NVvwXsBja3bpuBJ1p7N7CpPYF0Lb0bz/vbJajjSda1p5RuGzdm7LNu\nad/hn4uVpFky6D2HidwN7EqyBXgV+BxAVR1Msgs4BJwEtlXVqTbmDuAhYBG9G9F7Wv1B4JF28/ot\neiEkSZolma//QF+zZk15Q1qSzk2SZ6tqzWT9/IW0JKnDcJAkdRgOkqQOw0GS1DGdp5UkzXMr7vzO\nOfV/5e5Pn6eZaK7xzEGS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiS\nOgwHSVKH4SBJ6jAcJEkdk4ZDkvcm2Z/kh0kOJvlqq38lyWiS59r2qb4xdyUZSfJSkpv66jcmOdD2\n3ZckrX55ksdafV+SFTO/VEnSoAY5czgBfKKqPgKsBjYkWdf23VtVq9v2JECSVcAm4HpgA3B/kgWt\n/w7gdmBl2za0+hbg7aq6DrgXuGf6S5MkTdWk4VA9P29vL2tbnWXIRuDRqjpRVS8DI8DaJEuAK6pq\nb1UV8DBwc9+Yna39OLB+7KxCknThDXTPIcmCJM8BR4Gnqmpf2/WlJM8n+UaSq1ptKfBa3/DDrba0\ntcfXTxtTVSeBd4Crp7AeSdIMGCgcqupUVa0GltE7C7iB3iWiD9O71HQE+Np5m2WTZGuS4STDx44d\nO99fJ0mXrHN6WqmqfgZ8D9hQVW+00PgF8HVgbes2CizvG7as1UZbe3z9tDFJFgJXAm9O8P0PVNWa\nqlozNDR0LlOXJJ2DQZ5WGkrygdZeBPw68KN2D2HMZ4EXWns3sKk9gXQtvRvP+6vqCHA8ybp2P+E2\n4Im+MZtb+xbgmXZfQpI0CxYO0GcJsLM9cfQeYFdVfTvJI0lW07s5/QrwRYCqOphkF3AIOAlsq6pT\n7bPuAB4CFgF72gbwIPBIkhHgLXpPO0mSZsmk4VBVzwMfnaD+hbOM2Q5sn6A+DNwwQf1d4NbJ5iJJ\nujD8hbQkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS\n1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHVMGg5J3ptkf5IfJjmY5Kut/sEkTyX5cXu9qm/M\nXUlGkryU5Ka++o1JDrR99yVJq1+e5LFW35dkxcwvVZI0qEHOHE4An6iqjwCrgQ1J1gF3Ak9X1Urg\n6faeJKuATcD1wAbg/iQL2mftAG4HVrZtQ6tvAd6uquuAe4F7ZmBtkqQpmjQcqufn7e1lbStgI7Cz\n1XcCN7f2RuDRqjpRVS8DI8DaJEuAK6pqb1UV8PC4MWOf9TiwfuysQpJ04Q10zyHJgiTPAUeBp6pq\nH7C4qo60Lq8Di1t7KfBa3/DDrba0tcfXTxtTVSeBd4Crz3k1kqQZMVA4VNWpqloNLKN3FnDDuP1F\n72zivEqyNclwkuFjx46d76+TpEvWOT2tVFU/A75H717BG+1SEe31aOs2CizvG7as1UZbe3z9tDFJ\nFgJXAm9O8P0PVNWaqlozNDR0LlOXJJ2DQZ5WGkrygdZeBPw68CNgN7C5ddsMPNHau4FN7Qmka+nd\neN7fLkEdT7Ku3U+4bdyYsc+6BXimnY1IkmbBwgH6LAF2tieO3gPsqqpvJ/lfwK4kW4BXgc8BVNXB\nJLuAQ8BJYFtVnWqfdQfwELAI2NM2gAeBR5KMAG/Re9pJkjRLJg2Hqnoe+OgE9TeB9WcYsx3YPkF9\nGLhhgvq7wK0DzFeSdAH4C2lJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH\n4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeqYNBySLE/yvSSHkhxM8uVW\n/0qS0STPte1TfWPuSjKS5KUkN/XVb0xyoO27L0la/fIkj7X6viQrZn6pkqRBDXLmcBL43apaBawD\ntiVZ1fbdW1Wr2/YkQNu3Cbge2ADcn2RB678DuB1Y2bYNrb4FeLuqrgPuBe6Z/tIkSVM1aThU1ZGq\n+kFr/y3wIrD0LEM2Ao9W1YmqehkYAdYmWQJcUVV7q6qAh4Gb+8bsbO3HgfVjZxWSpAvvnO45tMs9\nHwX2tdKXkjyf5BtJrmq1pcBrfcMOt9rS1h5fP21MVZ0E3gGuPpe5SZJmzsDhkOSXgD8HfqeqjtO7\nRPRhYDVwBPjaeZnh6XPYmmQ4yfCxY8fO99dJ0iVroHBIchm9YPhmVf0FQFW9UVWnquoXwNeBta37\nKLC8b/iyVhtt7fH108YkWQhcCbw5fh5V9UBVramqNUNDQ4OtUJJ0zgZ5WinAg8CLVfWHffUlfd0+\nC7zQ2ruBTe0JpGvp3XjeX1VHgONJ1rXPvA14om/M5ta+BXim3ZeQJM2ChQP0+RjwBeBAkuda7feB\nzydZDRTwCvBFgKo6mGQXcIjek07bqupUG3cH8BCwCNjTNuiFzyNJRoC36D3tJEmaJZOGQ1V9H5jo\nyaEnzzJmO7B9gvowcMME9XeBWyebiyTpwvAX0pKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMk\nqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1DFpOCRZ\nnuR7SQ4lOZjky63+wSRPJflxe72qb8xdSUaSvJTkpr76jUkOtH33JUmrX57ksVbfl2TFzC9VkjSo\nQc4cTgK/W1WrgHXAtiSrgDuBp6tqJfB0e0/btwm4HtgA3J9kQfusHcDtwMq2bWj1LcDbVXUdcC9w\nzwysTZI0RZOGQ1UdqaoftPbfAi8CS4GNwM7WbSdwc2tvBB6tqhNV9TIwAqxNsgS4oqr2VlUBD48b\nM/ZZjwPrx84qJEkX3jndc2iXez4K7AMWV9WRtut1YHFrLwVe6xt2uNWWtvb4+mljquok8A5w9bnM\nTZI0cwYOhyS/BPw58DtVdbx/XzsTqBme20Rz2JpkOMnwsWPHzvfXSdIla6BwSHIZvWD4ZlX9RSu/\n0S4V0V6PtvoosLxv+LJWG23t8fXTxiRZCFwJvDl+HlX1QFWtqao1Q0NDg0xdkjQFgzytFOBB4MWq\n+sO+XbuBza29GXiir76pPYF0Lb0bz/vbJajjSda1z7xt3Jixz7oFeKadjUiSZsHCAfp8DPgCcCDJ\nc632+8DdwK4kW4BXgc8BVNXBJLuAQ/SedNpWVafauDuAh4BFwJ62QS98HkkyArxF72knSdIsmTQc\nqur7wJmeHFp/hjHbge0T1IeBGyaovwvcOtlcJEkXhr+QliR1GA6SpA7DQZLUYThIkjoMB0lSh+Eg\nSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKk\njknDIck3khxN8kJf7StJRpM817ZP9e27K8lIkpeS3NRXvzHJgbbvviRp9cuTPNbq+5KsmNklSpLO\n1SBnDg8BGyao31tVq9v2JECSVcAm4Po25v4kC1r/HcDtwMq2jX3mFuDtqroOuBe4Z4prkSTNkEnD\noar+CnhrwM/bCDxaVSeq6mVgBFibZAlwRVXtraoCHgZu7huzs7UfB9aPnVVIkmbHdO45fCnJ8+2y\n01WtthR4ra/P4VZb2trj66eNqaqTwDvA1dOYlyRpmqYaDjuADwOrgSPA12ZsRmeRZGuS4STDx44d\nuxBfKUmXpCmFQ1W9UVWnquoXwNeBtW3XKLC8r+uyVhtt7fH108YkWQhcCbx5hu99oKrWVNWaoaGh\nqUxdkjSAKYVDu4cw5rPA2JNMu4FN7Qmka+ndeN5fVUeA40nWtfsJtwFP9I3Z3Nq3AM+0+xKSpFmy\ncLIOSf4M+DhwTZLDwB8AH0+yGijgFeCLAFV1MMku4BBwEthWVafaR91B78mnRcCetgE8CDySZITe\nje9NM7EwSdLUTRoOVfX5CcoPnqX/dmD7BPVh4IYJ6u8Ct042D0nSheMvpCVJHYaDJKnDcJAkdRgO\nkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ\n6jAcJEkdhoMkqWPScEjyjSRHk7zQV/tgkqeS/Li9XtW3764kI0leSnJTX/3GJAfavvuSpNUvT/JY\nq+9LsmJmlyhJOleDnDk8BGwYV7sTeLqqVgJPt/ckWQVsAq5vY+5PsqCN2QHcDqxs29hnbgHerqrr\ngHuBe6a6GEnSzJg0HKrqr4C3xpU3Ajtbeydwc1/90ao6UVUvAyPA2iRLgCuqam9VFfDwuDFjn/U4\nsH7srEKSNDumes9hcVUdae3XgcWtvRR4ra/f4VZb2trj66eNqaqTwDvA1VOclyRpBkz7hnQ7E6gZ\nmMukkmxNMpxk+NixYxfiKyXpkjTVcHijXSqivR5t9VFgeV+/Za022trj66eNSbIQuBJ4c6IvraoH\nqmpNVa0ZGhqa4tQlSZOZajjsBja39mbgib76pvYE0rX0bjzvb5egjidZ1+4n3DZuzNhn3QI8085G\nJEmzZOFkHZL8GfBx4Jokh4E/AO4GdiXZArwKfA6gqg4m2QUcAk4C26rqVPuoO+g9+bQI2NM2gAeB\nR5KM0LvxvWlGViZJmrJJw6GqPn+GXevP0H87sH2C+jBwwwT1d4FbJ5uHJF2MVtz5nXMe88rdnz4P\nMzmdv5CWJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThI\nkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqSOaYVDkleSHEjyXJLhVvtgkqeS/Li9XtXX/64k\nI0leSnJTX/3G9jkjSe5LkunMS5I0PTNx5vBrVbW6qta093cCT1fVSuDp9p4kq4BNwPXABuD+JAva\nmB3A7cDKtm2YgXlJkqbofFxW2gjsbO2dwM199Uer6kRVvQyMAGuTLAGuqKq9VVXAw31jJEmzYLrh\nUMBfJnk2ydZWW1xVR1r7dWBxay8FXusbe7jVlrb2+LokaZYsnOb4X6mq0ST/CHgqyY/6d1ZVJalp\nfsffawG0FeBDH/rQTH2sJGmcaZ05VNVoez0KfAtYC7zRLhXRXo+27qPA8r7hy1pttLXH1yf6vgeq\nak1VrRkaGprO1CVJZzHlcEjyviTvH2sDvwG8AOwGNrdum4EnWns3sCnJ5UmupXfjeX+7BHU8ybr2\nlNJtfWMkSbNgOpeVFgPfak+dLgT+tKq+m+SvgV1JtgCvAp8DqKqDSXYBh4CTwLaqOtU+6w7gIWAR\nsKdtkqRZMuVwqKqfAB+ZoP4msP4MY7YD2yeoDwM3THUukqSZ5S+kJUkdhoMkqcNwkCR1GA6SpA7D\nQZLUYThIkjoMB0lSh+EgSeqY7h/ek6ZsxZ3fOaf+r9z96fM0E0njeeYgSeowHCRJHYaDJKnDcJAk\ndRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqmDPhkGRDkpeSjCS5c7bnI0mXsjkRDkkWAP8Z+CSwCvh8\nklWzOytJunTNiXAA1gIjVfWTqvo74FFg4yzPSZIuWXMlHJYCr/W9P9xqkqRZMK/+KmuSrcDW9vbn\nSV6a4kddA/x0ZmY16y6ZteSeCziT6bloj8k8OgYTuWiOS+6Z1lr+ySCd5ko4jALL+94va7XTVNUD\nwAPT/bIkw1W1ZrqfMxe4lrnnYlkHuJa56kKsZa5cVvprYGWSa5P8A2ATsHuW5yRJl6w5ceZQVSeT\n/BvgvwMLgG9U1cFZnpYkXbLmRDgAVNWTwJMX6OumfWlqDnEtc8/Fsg5wLXPVeV9Lqup8f4ckaZ6Z\nK/ccJElzyEUbDknem2R/kh8mOZjkqxP0SZL72p/seD7JL8/GXCcz4Fo+nuSdJM+17d/PxlwHkWRB\nkr9J8u0J9s2LYzJmkrXMp2PySpIDbZ7DE+yfN8dlgLXMi+OS5ANJHk/yoyQvJvkX4/af12MyZ+45\nnAcngE9U1c+TXAZ8P8meqtrb1+eTwMq2/XNgR3udawZZC8D/rKrPzML8ztWXgReBKybYN1+OyZiz\nrQXmzzEB+LWqOtOz8/PtuJxtLTA/jst/AL5bVbe0pzj/4bj95/WYXLRnDtXz8/b2sraNv8GyEXi4\n9d0LfCDJkgs5z0EMuJZ5Icky4NPAH5+hy7w4JjDQWi4m8+a4XAySXAn8KvAgQFX9XVX9bFy383pM\nLtpwgL8/5X8OOAo8VVX7xnWZN3+2Y4C1APzLdnq5J8n1F3iKg/oj4PeAX5xh/7w5Jky+FpgfxwR6\n/9j4yyTPtr9EMN58Oi6TrQXm/nG5FjgG/Nd22fKPk7xvXJ/zekwu6nCoqlNVtZreL67XJrlhtuc0\nVQOs5QfAh6rqnwH/EfhvF3qOk0nyGeBoVT0723OZrgHXMuePSZ9faf/7+iSwLcmvzvaEpmGytcyH\n47IQ+GVgR1V9FPjfwAX9Txlc1OEwpp2OfQ/YMG7XQH+2Yy4501qq6vjYpaf2m5HLklwzC1M8m48B\nv5nkFXp/efcTSf5kXJ/5ckwmXcs8OSYAVNVoez0KfIveX0ruN1+Oy6RrmSfH5TBwuO8KweP0wqLf\neT0mF204JBlK8oHWXgT8OvCjcd12A7e1u/7rgHeq6sgFnuqkBllLkn+cJK29lt6xffNCz/Vsququ\nqlpWVSvo/YmUZ6rqt8Z1mxfHZJC1zIdjApDkfUneP9YGfgN4YVy3eXFcBlnLfDguVfU68FqSf9pK\n64FD47qd12NyMT+ttATYmd5/SOg9wK6q+naSfwVQVf+F3i+yPwWMAP8H+O3ZmuwkBlnLLcC/TnIS\n+L/Apponv3Ccp8dkQvP0mCwGvtX+/3Ih8KdV9d15elwGWct8OS5fAr7ZnlT6CfDbF/KY+AtpSVLH\nRXtZSZI0dYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnq+P/Knb5xqRFacQAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a5d0a8aac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(list(map(len,target_seqs)),bins=25);\n",
    "\n",
    "# Truncate names longer than MAX_LEN characters. This can be changed\n",
    "MAX_LEN = min([150,max(list(map(len, target_seqs)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast everything from symbols into matrix of int32. Pad with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_matrix(sequences, token_to_i, max_len=None, PAX_ix=-1):\n",
    "    \"\"\"\n",
    "    Converts several sequences of tokens to a matrix, edible a neural network.\n",
    "    Crops at max_len(if given), pads shorter sequences with -1 or PAD_ix.\n",
    "    \"\"\"\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences),max_len),dtype='int8') -1\n",
    "    for i,seq in enumerate(sequences):\n",
    "        \n",
    "        row_ix = [token_to_i.get(_, 0) for _ in seq[:max_len]]\n",
    "        matrix[i,:len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = T.matrix('token sequence', 'int32')\n",
    "output_sequence = T.matrix('target target_letters', 'int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##ENCODER\n",
    "l_in = InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "l_mask = InputLayer(shape=(None, None),input_var=T.neq(input_sequence,-1)) \n",
    "\n",
    "\n",
    "l_emb = non_trainable(EmbeddingLayer(l_in, len(source_letters),  len(source_letters), W=np.diag(np.ones(len(source_letters)))))\n",
    "l_rnn = LSTMLayer(l_emb, HIDDEN_SIZE, only_return_final=True, mask_input=l_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##DECODER\n",
    "dec_in = InputLayer(shape=(None, None),input_var=output_sequence)\n",
    "dec_mask = InputLayer(shape=(None, None),input_var=T.neq(output_sequence,-1))\n",
    "\n",
    "dec_emb = non_trainable(EmbeddingLayer(dec_in, len(target_letters), len(target_letters), W=np.diag(np.ones(len(target_letters)))))\n",
    "dec_rnn = LSTMLayer(dec_emb, num_units=HIDDEN_SIZE, cell_init=l_rnn, mask_input=dec_mask)\n",
    "# WARNING! if it's lstm use cell_init, not hid_init\n",
    "\n",
    "\n",
    "#flatten batch and time to be compatible with feedforward layers (will un-flatten later)\n",
    "dec_rnn_flat = reshape(dec_rnn, (-1,dec_rnn.output_shape[-1]))\n",
    "\n",
    "l_out = DenseLayer(dec_rnn_flat, len(target_letters), nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_model(nn, learning_rate=0.001):\n",
    "    # Model weights\n",
    "    weights = get_all_params(nn)\n",
    "    network_output = get_output(nn)\n",
    "    network_output = network_output.reshape([output_sequence.shape[0],\\\n",
    "                                         output_sequence.shape[1], -1])\n",
    "    predictions_flat = network_output[:,:-1,:].reshape([-1,len(target_letters)])\n",
    "    targets = output_sequence[:,1:].ravel()\n",
    "\n",
    "    #do not count loss for '-1' tokens\n",
    "    mask = T.nonzero(T.neq(targets,-1))\n",
    "\n",
    "    loss = lasagne.objectives.categorical_crossentropy(predictions_flat[mask], targets[mask]).mean()\n",
    "    updates = lasagne.updates.adam(loss, weights, learning_rate=learning_rate)\n",
    "    #training\n",
    "    train = theano.function([input_sequence, output_sequence], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    #computing loss without training\n",
    "    compute_cost = theano.function([input_sequence, output_sequence], loss, allow_input_downcast=True)\n",
    "    #compile the function that computes probabilities for next token given previous text.\n",
    "\n",
    "    last_probas =network_output[:, -1]\n",
    "\n",
    "    probs = theano.function([input_sequence, output_sequence], last_probas)\n",
    "    return train, compute_cost, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "We now need to implement a function that generates output sequence given input.\n",
    "\n",
    "Such function must work thusly:\n",
    "```\n",
    "Init:\n",
    "x = input\n",
    "y = [\"START\"]\n",
    "\n",
    "While not_too_long:\n",
    "  p(y_next|x,y) = probabilities of next letter for y\n",
    "  \n",
    "  y_next ~ p(y_next|x,y)\n",
    "  \n",
    "  y.append(y_next)\n",
    "  \n",
    "  if y_next == \"END\":\n",
    "      break\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_output(input, probs, target_letters, target_letter_to_ix, source_letter_to_ix,\n",
    "                    output_prefix = (\"START\",),\n",
    "                    END_token=\"END\",\n",
    "                    temperature=1,\n",
    "                    sample=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement a function that generates output sequence given input.\n",
    "    \n",
    "    We recommend (but not require) you to use the pseudo-code above and inline instructions.\n",
    "    \"\"\"\n",
    "    x = as_matrix([input], source_letter_to_ix) \n",
    "    output = list(output_prefix)\n",
    "    while True:\n",
    "        y = as_matrix([output], target_letter_to_ix)\n",
    "        next_y_probs = probs(x, y)\n",
    "        next_y_probs = (next_y_probs ** temperature) / (next_y_probs ** temperature).sum()\n",
    "        if sample:\n",
    "            next_y = np.random.choice(target_letters, p=next_y_probs[0])\n",
    "        else:\n",
    "            next_y = target_letters[next_y_probs[0].argmax()]\n",
    "        next_y = str(next_y)             \n",
    "        assert type(next_y) is str, \"please return token(string/character), not it's index\"\n",
    "        \n",
    "        output.append(next_y)\n",
    "\n",
    "        if next_y==END_token:\n",
    "            break\n",
    "            \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#source_seqs = np.array(source_seqs)\n",
    "#target_seqs = np.array(target_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(source_seqs, source_letter_to_ix, target_seqs, target_letter_to_ix, batch_size):\n",
    "    \"\"\"samples a random batch of source and target sequences, batch_size elements\"\"\"\n",
    "    batch_ix = np.random.randint(0,len(source_seqs),size=batch_size)\n",
    "    source_seqs_batch=as_matrix(source_seqs[batch_ix], source_letter_to_ix) \n",
    "    target_seqs_batch=as_matrix(target_seqs[batch_ix], target_letter_to_ix)\n",
    "    \n",
    "    return source_seqs_batch,target_seqs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train average loss = 1.724408619115944\n",
      "Epoch 0 val average loss = 1.6007625962506895\n",
      "625+968 : 632  |  1593\n",
      "5+539 : 161  |  544\n",
      "568+248 : 1049  |  816\n",
      "734+723 : 1420  |  1457\n",
      "86+350 : 594  |  436\n",
      "Epoch 1 train average loss = 1.4482155305233033\n",
      "Epoch 1 val average loss = 1.3316337735395296\n",
      "10+463 : 536  |  473\n",
      "4+549 : 517  |  553\n",
      "33+990 : 1039  |  1023\n",
      "326+58 : 311  |  384\n",
      "767+18 : 833  |  785\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=30\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "train_batches_per_epoch = 500\n",
    "val_batches_per_epoch = 50\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "train_source_seqs, val_source_seqs, train_target_seqs, val_target_seqs = train_test_split(source_seqs, target_seqs,\\\n",
    "                                                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        train_avg_cost = 0;\n",
    "        val_avg_cost = 0;\n",
    "\n",
    "        for _ in range(train_batches_per_epoch):\n",
    "            x,y = sample_batch(train_source_seqs, source_letter_to_ix, train_target_seqs, target_letter_to_ix, batch_size)\n",
    "            train_avg_cost += train(x, y).mean()\n",
    "        \n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            x,y = sample_batch(val_source_seqs, source_letter_to_ix, val_target_seqs, target_letter_to_ix, batch_size)\n",
    "            val_avg_cost += compute_cost(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} train average loss = {}\".format(epoch, train_avg_cost / train_batches_per_epoch))\n",
    "        print(\"Epoch {} val average loss = {}\".format(epoch, val_avg_cost / val_batches_per_epoch))\n",
    "        \n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(val_source_seqs))\n",
    "            print (val_source_seqs[ind],':', ''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(val_target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bazal module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CerMemory(lasagne.layers.Layer):\n",
    "    def __init__(self, incoming, memory_size, M=lasagne.init.Orthogonal(), **kwargs):\n",
    "        super(CerMemory, self).__init__(incoming, **kwargs)\n",
    "        self.query_shape = self.input_shape[1]\n",
    "        self.memory_size = memory_size\n",
    "        self.M = self.add_param(M, (self.query_shape, memory_size), name='M')\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        m = self.M / T.sqrt(T.sqr(self.M).sum(axis=0)).reshape(self.M.shape[1], 1)\n",
    "        weights =  T.dot(input, m)\n",
    "        return T.dot(weights, m.T)\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], self.query_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EvcNormalizer(lasagne.layers.Layer):\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return (input.T / T.sqrt(T.sqr(input).sum(axis=1)).reshape(input.shape[0], 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_SIZE = 16\n",
    "MEMORY_SIZE = 64\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bazal_model(query_size, memory_size, hidden_size, memory_benchmark=False, bidir_features=False):\n",
    "\n",
    "    ##ENCODER\n",
    "    l_in = InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "    l_mask = InputLayer(shape=(None, None),input_var=T.neq(input_sequence,-1)) \n",
    "\n",
    "\n",
    "    l_emb = non_trainable(EmbeddingLayer(l_in, len(source_letters),  len(source_letters), W=np.diag(np.ones(len(source_letters)))))\n",
    "\n",
    "    features = LSTMLayer(l_emb, HIDDEN_SIZE, only_return_final=True, mask_input=l_mask)\n",
    "    features_backward = LSTMLayer(l_emb, HIDDEN_SIZE, only_return_final=True, mask_input=l_mask, backwards=True)\n",
    "    if bidir_features:\n",
    "        features = ConcatLayer([features, features_backward])\n",
    "    \n",
    "    if not memory_benchmark:\n",
    "        ## QUERY BUILDER\n",
    "        query = DenseLayer(features, QUERY_SIZE, nonlinearity=None)\n",
    "        query = EvcNormalizer(query)\n",
    "        ## Memory\n",
    "        memory = CerMemory(query, MEMORY_SIZE)\n",
    "    else:\n",
    "        memory = DenseLayer(DenseLayer(features, QUERY_SIZE), QUERY_SIZE)\n",
    "        \n",
    "    to_decode = ConcatLayer([features, memory])\n",
    "    \n",
    "    ##DECODER\n",
    "    dec_in = InputLayer(shape=(None, None),input_var=output_sequence)\n",
    "    dec_mask = InputLayer(shape=(None, None),input_var=T.neq(output_sequence,-1))\n",
    "\n",
    "    dec_emb = non_trainable(EmbeddingLayer(dec_in, len(target_letters), len(target_letters), W=np.diag(np.ones(len(target_letters)))))\n",
    "    dec_rnn = LSTMLayer(dec_emb, num_units=to_decode.output_shape[-1], cell_init=to_decode, mask_input=dec_mask)\n",
    "    # WARNING! if it's lstm use cell_init, not hid_init\n",
    "\n",
    "\n",
    "    #flatten batch and time to be compatible with feedforward layers (will un-flatten later)\n",
    "    dec_rnn_flat = reshape(dec_rnn, (-1,dec_rnn.output_shape[-1]))\n",
    "\n",
    "    l_out = DenseLayer(dec_rnn_flat, len(target_letters), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out, memory = bazal_model(QUERY_SIZE, MEMORY_SIZE, HIDDEN_SIZE, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train average loss = 1.6742143869203903\n",
      "Epoch 0 val average loss = 1.4339063474449143\n",
      "773+25 : 799  |  798\n",
      "81+445 : 916  |  526\n",
      "28+352 : 261  |  380\n",
      "39+578 : 465  |  617\n",
      "878+646 : 1523  |  1524\n",
      "Epoch 1 train average loss = 1.317323833876064\n",
      "Epoch 1 val average loss = 1.2285492045532798\n",
      "351+682 : 1599  |  1033\n",
      "640+67 : 640  |  707\n",
      "96+906 : 1011  |  1002\n",
      "406+521 : 792  |  927\n",
      "715+561 : 1607  |  1276\n",
      "Epoch 2 train average loss = 1.1632730232336268\n",
      "Epoch 2 val average loss = 1.1099605287226941\n",
      "86+350 : 399  |  436\n",
      "286+3 : 336  |  289\n",
      "8+342 : 332  |  350\n",
      "759+18 : 788  |  777\n",
      "290+290 : 513  |  580\n",
      "Epoch 3 train average loss = 1.045522278657902\n",
      "Epoch 3 val average loss = 0.9992109939609599\n",
      "438+11 : 437  |  449\n",
      "56+392 : 428  |  448\n",
      "17+300 : 305  |  317\n",
      "33+557 : 523  |  590\n",
      "80+45 : 129  |  125\n",
      "Epoch 4 train average loss = 0.9506064003860062\n",
      "Epoch 4 val average loss = 0.9316100418117856\n",
      "3+951 : 957  |  954\n",
      "629+8 : 635  |  637\n",
      "639+39 : 693  |  678\n",
      "153+4 : 176  |  157\n",
      "219+104 : 320  |  323\n",
      "Epoch 5 train average loss = 0.8757097786927143\n",
      "Epoch 5 val average loss = 0.8693076887067153\n",
      "204+38 : 225  |  242\n",
      "838+729 : 1538  |  1567\n",
      "725+252 : 970  |  977\n",
      "99+272 : 357  |  371\n",
      "49+871 : 913  |  920\n",
      "Epoch 6 train average loss = 0.8145957107974854\n",
      "Epoch 6 val average loss = 0.7948836647755283\n",
      "787+357 : 1074  |  1144\n",
      "807+116 : 929  |  923\n",
      "21+874 : 902  |  895\n",
      "241+321 : 564  |  562\n",
      "244+310 : 549  |  554\n",
      "Epoch 7 train average loss = 0.7565183972433357\n",
      "Epoch 7 val average loss = 0.7774048949681345\n",
      "213+8 : 229  |  221\n",
      "27+74 : 100  |  101\n",
      "10+109 : 120  |  119\n",
      "38+83 : 117  |  121\n",
      "174+49 : 251  |  223\n",
      "Epoch 8 train average loss = 0.6965506421161222\n",
      "Epoch 8 val average loss = 0.6929601927143549\n",
      "570+36 : 606  |  606\n",
      "743+301 : 981  |  1044\n",
      "147+200 : 337  |  347\n",
      "24+83 : 107  |  107\n",
      "24+28 : 82  |  52\n",
      "Epoch 9 train average loss = 0.6030169775917447\n",
      "Epoch 9 val average loss = 0.5555107549757917\n",
      "4+164 : 177  |  168\n",
      "713+32 : 736  |  745\n",
      "984+2 : 986  |  986\n",
      "909+30 : 940  |  939\n",
      "80+636 : 717  |  716\n",
      "Epoch 10 train average loss = 0.45829638499859116\n",
      "Epoch 10 val average loss = 0.4161474850774145\n",
      "351+60 : 392  |  411\n",
      "825+88 : 912  |  913\n",
      "349+47 : 385  |  396\n",
      "512+97 : 600  |  609\n",
      "33+622 : 664  |  655\n",
      "Epoch 11 train average loss = 0.34812304906515223\n",
      "Epoch 11 val average loss = 0.32292285384458913\n",
      "953+72 : 1035  |  1025\n",
      "6+395 : 392  |  401\n",
      "403+753 : 1076  |  1156\n",
      "91+657 : 750  |  748\n",
      "881+341 : 1222  |  1222\n",
      "Epoch 12 train average loss = 0.27178860340563343\n",
      "Epoch 12 val average loss = 0.28019212825705186\n",
      "660+317 : 978  |  977\n",
      "593+58 : 651  |  651\n",
      "592+0 : 591  |  592\n",
      "271+600 : 870  |  871\n",
      "78+757 : 825  |  835\n",
      "Epoch 13 train average loss = 0.2202755831323927\n",
      "Epoch 13 val average loss = 0.23145012039003948\n",
      "394+535 : 919  |  929\n",
      "1+667 : 668  |  668\n",
      "6+182 : 188  |  188\n",
      "37+935 : 972  |  972\n",
      "826+745 : 1572  |  1571\n",
      "Epoch 14 train average loss = 0.18254756290455096\n",
      "Epoch 14 val average loss = 0.18621768310433656\n",
      "0+464 : 464  |  464\n",
      "360+8 : 370  |  368\n",
      "24+39 : 59  |  63\n",
      "0+777 : 777  |  777\n",
      "604+358 : 952  |  962\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=30\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "train_batches_per_epoch = 500\n",
    "val_batches_per_epoch = 50\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "train_source_seqs, val_source_seqs, train_target_seqs, val_target_seqs = train_test_split(source_seqs, target_seqs,\\\n",
    "                                                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        train_avg_cost = 0;\n",
    "        val_avg_cost = 0;\n",
    "\n",
    "        for _ in range(train_batches_per_epoch):\n",
    "            x,y = sample_batch(train_source_seqs, source_letter_to_ix, train_target_seqs, target_letter_to_ix, batch_size)\n",
    "            train_avg_cost += train(x, y).mean()\n",
    "        \n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            x,y = sample_batch(val_source_seqs, source_letter_to_ix, val_target_seqs, target_letter_to_ix, batch_size)\n",
    "            val_avg_cost += compute_cost(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} train average loss = {}\".format(epoch, train_avg_cost / train_batches_per_epoch))\n",
    "        print(\"Epoch {} val average loss = {}\".format(epoch, val_avg_cost / val_batches_per_epoch))\n",
    "        \n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(val_source_seqs))\n",
    "            print (val_source_seqs[ind],':', ''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(val_target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M_add = np.array(memory.M.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./zoo/addprob_memory_after30epochs.npy', M_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same for prod problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 100000\n",
    "DIGITS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 100000\n"
     ]
    }
   ],
   "source": [
    "source_seqs, source_letters, source_letter_to_ix, target_seqs, target_letters, target_letter_to_ix =\\\n",
    "                    generate_data(TRAINING_SIZE, DIGITS, lambda a, b: a * b, '{}*{}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25*43 : 1075\n",
      "61*4 : 244\n",
      "3*184 : 552\n",
      "7*5 : 35\n",
      "51*7 : 357\n"
     ]
    }
   ],
   "source": [
    "for source, target in zip(source_seqs[:5],target_seqs[:5]):\n",
    "    print( source,':',\"\".join(target[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_SIZE = 16\n",
    "MEMORY_SIZE = 64\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bazal_model(query_size, memory_size, hidden_size, memory_benchmark=False, bidir_features=False):\n",
    "\n",
    "    ##ENCODER\n",
    "    l_in = InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "    l_mask = InputLayer(shape=(None, None),input_var=T.neq(input_sequence,-1)) \n",
    "\n",
    "\n",
    "    l_emb = non_trainable(EmbeddingLayer(l_in, len(source_letters),  len(source_letters), W=np.diag(np.ones(len(source_letters)))))\n",
    "\n",
    "    features = LSTMLayer(l_emb, HIDDEN_SIZE*5, only_return_final=True, mask_input=l_mask)\n",
    "    features_backward = LSTMLayer(l_emb, HIDDEN_SIZE*5, only_return_final=True, mask_input=l_mask, backwards=True)\n",
    "    if bidir_features:\n",
    "        features = ConcatLayer([features, features_backward])\n",
    "    \n",
    "    if not memory_benchmark:\n",
    "        ## QUERY BUILDER\n",
    "        query = DenseLayer(features, QUERY_SIZE, nonlinearity=None)\n",
    "        query = EvcNormalizer(query)\n",
    "        ## Memory\n",
    "        memory = CerMemory(query, MEMORY_SIZE)\n",
    "    else:\n",
    "        memory = DenseLayer(DenseLayer(features, QUERY_SIZE), QUERY_SIZE)\n",
    "        \n",
    "    to_decode = ConcatLayer([features, memory])\n",
    "    \n",
    "    ##DECODER\n",
    "    dec_in = InputLayer(shape=(None, None),input_var=output_sequence)\n",
    "    dec_mask = InputLayer(shape=(None, None),input_var=T.neq(output_sequence,-1))\n",
    "\n",
    "    dec_emb = non_trainable(EmbeddingLayer(dec_in, len(target_letters), len(target_letters), W=np.diag(np.ones(len(target_letters)))))\n",
    "    dec_rnn = LSTMLayer(dec_emb, num_units=to_decode.output_shape[-1], cell_init=to_decode, mask_input=dec_mask)\n",
    "    # WARNING! if it's lstm use cell_init, not hid_init\n",
    "\n",
    "\n",
    "    #flatten batch and time to be compatible with feedforward layers (will un-flatten later)\n",
    "    dec_rnn_flat = reshape(dec_rnn, (-1,dec_rnn.output_shape[-1]))\n",
    "\n",
    "    l_out = DenseLayer(dec_rnn_flat, len(target_letters), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out, memory = bazal_model(QUERY_SIZE, MEMORY_SIZE, HIDDEN_SIZE,  False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 67/500 [00:00<00:00, 668.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 155/500 [00:00<00:00, 772.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████▉     | 249/500 [00:00<00:00, 827.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 334/500 [00:00<00:00, 832.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 417/500 [00:00<00:00, 831.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 839.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm.tqdm(range(train_batches_per_epoch)):\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:43<00:00,  2.24it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train average loss = 1.5986278103094766\n",
      "Epoch 0 val average loss = 1.450215123473087\n",
      "False\n",
      "121*338 : 40364  |  40898\n",
      "False\n",
      "133*7 : 7559  |  931\n",
      "False\n",
      "701*586 : 374536  |  410786\n",
      "False\n",
      "50*113 : 6920  |  5650\n",
      "False\n",
      "690*96 : 63766  |  66240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:46<00:00,  2.20it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train average loss = 1.236026344000003\n",
      "Epoch 1 val average loss = 1.139667141037998\n",
      "False\n",
      "33*895 : 30405  |  29535\n",
      "False\n",
      "200*182 : 43200  |  36400\n",
      "False\n",
      "709*459 : 334403  |  325431\n",
      "False\n",
      "78*323 : 26606  |  25194\n",
      "False\n",
      "333*39 : 11693  |  12987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:45<00:00,  2.22it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train average loss = 1.0441027867636372\n",
      "Epoch 2 val average loss = 0.9213208069088478\n",
      "False\n",
      "716*440 : 321240  |  315040\n",
      "False\n",
      "125*5 : 775  |  625\n",
      "False\n",
      "65*854 : 56130  |  55510\n",
      "False\n",
      "743*16 : 11428  |  11888\n",
      "False\n",
      "940*57 : 52660  |  53580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train average loss = 0.8256853999431272\n",
      "Epoch 3 val average loss = 0.7919077669526083\n",
      "False\n",
      "2*548 : 1152  |  1096\n",
      "False\n",
      "302*40 : 12480  |  12080\n",
      "True\n",
      "240*80 : 20200  |  19200\n",
      "False\n",
      "786*41 : 33926  |  32226\n",
      "False\n",
      "830*29 : 23070  |  24070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train average loss = 0.7360362974718457\n",
      "Epoch 4 val average loss = 0.7360364157752874\n",
      "False\n",
      "23*676 : 14908  |  15548\n",
      "False\n",
      "9*114 : 1026  |  1026\n",
      "False\n",
      "256*242 : 62912  |  61952\n",
      "False\n",
      "84*125 : 10200  |  10500\n",
      "True\n",
      "75*15 : 1125  |  1125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train average loss = 0.6805974253053574\n",
      "Epoch 5 val average loss = 0.7256294333303288\n",
      "False\n",
      "276*599 : 169804  |  165324\n",
      "False\n",
      "640*618 : 392720  |  395520\n",
      "False\n",
      "940*38 : 36120  |  35720\n",
      "False\n",
      "663*436 : 290908  |  289068\n",
      "False\n",
      "19*161 : 3059  |  3059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:44<00:00,  2.22it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train average loss = 0.6114800740830969\n",
      "Epoch 6 val average loss = 0.6801952689806824\n",
      "False\n",
      "137*89 : 12313  |  12193\n",
      "False\n",
      "6*956 : 5716  |  5736\n",
      "True\n",
      "957*4 : 3668  |  3828\n",
      "False\n",
      "729*283 : 205167  |  206307\n",
      "False\n",
      "920*622 : 576240  |  572240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train average loss = 0.5639696583594016\n",
      "Epoch 7 val average loss = 0.672156206813137\n",
      "False\n",
      "801*126 : 101606  |  100926\n",
      "False\n",
      "698*946 : 664628  |  660308\n",
      "False\n",
      "198*68 : 13504  |  13464\n",
      "False\n",
      "533*542 : 284546  |  288886\n",
      "False\n",
      "609*631 : 381299  |  384279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train average loss = 0.5022730654032395\n",
      "Epoch 8 val average loss = 0.7005400344738804\n",
      "False\n",
      "53*799 : 42527  |  42347\n",
      "False\n",
      "743*9 : 6801  |  6687\n",
      "False\n",
      "539*219 : 118681  |  118041\n",
      "False\n",
      "564*12 : 6688  |  6768\n",
      "False\n",
      "115*876 : 99940  |  100740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train average loss = 0.45616437141699645\n",
      "Epoch 9 val average loss = 0.7480416283685315\n",
      "False\n",
      "38*333 : 12934  |  12654\n",
      "True\n",
      "45*5 : 225  |  225\n",
      "True\n",
      "498*31 : 15458  |  15438\n",
      "False\n",
      "784*316 : 243984  |  247744\n",
      "False\n",
      "493*2 : 1186  |  986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train average loss = 0.3911521420229151\n",
      "Epoch 10 val average loss = 0.7464897061921154\n",
      "False\n",
      "132*803 : 109296  |  105996\n",
      "True\n",
      "110*22 : 2420  |  2420\n",
      "False\n",
      "10*963 : 9430  |  9630\n",
      "True\n",
      "230*3 : 730  |  690\n",
      "True\n",
      "533*5 : 2665  |  2665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 train average loss = 0.32892448618882847\n",
      "Epoch 11 val average loss = 0.788659161913402\n",
      "False\n",
      "112*740 : 83680  |  82880\n",
      "False\n",
      "226*361 : 84346  |  81586\n",
      "True\n",
      "539*50 : 26950  |  26950\n",
      "False\n",
      "442*35 : 15670  |  15470\n",
      "False\n",
      "24*4 : 96  |  96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 train average loss = 0.27522573551255686\n",
      "Epoch 12 val average loss = 0.8165971594834391\n",
      "False\n",
      "333*397 : 133341  |  132201\n",
      "False\n",
      "77*897 : 68709  |  69069\n",
      "False\n",
      "609*45 : 26905  |  27405\n",
      "False\n",
      "431*748 : 320548  |  322388\n",
      "True\n",
      "48*900 : 43200  |  43200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 train average loss = 0.2248516813827948\n",
      "Epoch 13 val average loss = 0.8696660276904791\n",
      "False\n",
      "134*98 : 13332  |  13132\n",
      "True\n",
      "434*7 : 3038  |  3038\n",
      "False\n",
      "437*64 : 27888  |  27968\n",
      "False\n",
      "85*973 : 84165  |  82705\n",
      "False\n",
      "819*91 : 72809  |  74529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 train average loss = 0.17378955791924697\n",
      "Epoch 14 val average loss = 0.9828420893965977\n",
      "True\n",
      "24*6 : 124  |  144\n",
      "False\n",
      "158*861 : 135238  |  136038\n",
      "False\n",
      "612*961 : 603772  |  588132\n",
      "False\n",
      "71*510 : 36610  |  36210\n",
      "False\n",
      "747*963 : 734621  |  719361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 train average loss = 0.14437570233733316\n",
      "Epoch 15 val average loss = 0.9732686831372485\n",
      "False\n",
      "362*25 : 9150  |  9050\n",
      "True\n",
      "804*5 : 3620  |  4020\n",
      "False\n",
      "677*46 : 31482  |  31142\n",
      "False\n",
      "506*83 : 43078  |  41998\n",
      "False\n",
      "312*64 : 20328  |  19968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 train average loss = 0.11004019454270396\n",
      "Epoch 16 val average loss = 1.0248796990522435\n",
      "False\n",
      "59*893 : 51907  |  52687\n",
      "False\n",
      "78*307 : 23406  |  23946\n",
      "True\n",
      "32*63 : 2016  |  2016\n",
      "False\n",
      "648*133 : 88184  |  86184\n",
      "False\n",
      "878*641 : 562458  |  562798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 train average loss = 0.10076144074356086\n",
      "Epoch 17 val average loss = 1.0786626571807068\n",
      "True\n",
      "43*190 : 8570  |  8170\n",
      "False\n",
      "876*72 : 64112  |  63072\n",
      "False\n",
      "85*910 : 78350  |  77350\n",
      "False\n",
      "434*26 : 11164  |  11284\n",
      "False\n",
      "250*661 : 165750  |  165250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 316/500 [02:21<01:22,  2.23it/s]"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=1000\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "train_batches_per_epoch = 500\n",
    "val_batches_per_epoch = 50\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "train_source_seqs, val_source_seqs, train_target_seqs, val_target_seqs = train_test_split(source_seqs, target_seqs,\\\n",
    "                                                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        train_avg_cost = 0;\n",
    "        val_avg_cost = 0;\n",
    "\n",
    "        for _ in tqdm.tqdm(range(train_batches_per_epoch)):\n",
    "            x,y = sample_batch(train_source_seqs, source_letter_to_ix, train_target_seqs, target_letter_to_ix, batch_size)\n",
    "            train_avg_cost += train(x, y).mean()\n",
    "        \n",
    "        for _ in tqdm.tqdm(range(val_batches_per_epoch)):\n",
    "            x,y = sample_batch(val_source_seqs, source_letter_to_ix, val_target_seqs, target_letter_to_ix, batch_size)\n",
    "            val_avg_cost += compute_cost(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} train average loss = {}\".format(epoch, train_avg_cost / train_batches_per_epoch))\n",
    "        print(\"Epoch {} val average loss = {}\".format(epoch, val_avg_cost / val_batches_per_epoch))\n",
    "        \n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(val_source_seqs))\n",
    "            print (''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1])==''.join(val_target_seqs[ind][1:-1]))\n",
    "            print (val_source_seqs[ind],':', ''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(val_target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'719*101'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'*'.join(map(lambda x: str(int(x, 2)), val_source_seqs[ind].split('*')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grand problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob1, prob_op1 = lambda a, b, c: (a + b) * c, '({}+{})*{}'\n",
    "prob2, prob_op2 = lambda a, b, c:  a * (b + c), '{}*({}+{})'\n",
    "prob3, prob_op3 = lambda a, b, c:  a + b * c, '{}+{}*{}'\n",
    "prob4, prob_op4 = lambda a, b, c:  a * b + c, '{}*{}+{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_grand_data(size, digits, problems = [prob1, prob2, prob3, prob4],\\\n",
    "                        problem_operators=[prob_op1, prob_op2, prob_op3, prob_op4]):\n",
    "    source_seqs = []\n",
    "    target_seqs = []\n",
    "    seen = set()\n",
    "    assert len(problem_operators) == len(problems)\n",
    "    print('Generating data...')\n",
    "    while len(source_seqs) < TRAINING_SIZE:\n",
    "        f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                        for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "        a, b, c = f(), f(), f()\n",
    "        # Skip any addition questions we've already seen\n",
    "        # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "        key = tuple(sorted((a, b, c)))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        \n",
    "        coin = np.random.randint(0, len(problems))\n",
    "        \n",
    "        # Pad the data with spaces such that it is always MAXLEN.\n",
    "        q = problem_operators[coin].format(a, b, c)\n",
    "        ans = str(problems[coin](a, b, c))\n",
    "\n",
    "        source_seqs.append(q)\n",
    "        target_seqs.append([\"START\"] + list(ans) + [\"END\"])\n",
    "\n",
    "    print('Total addition questions:', len(source_seqs))\n",
    "    \n",
    "    target_letters = list(set([token for ts in target_seqs for token in ts]))\n",
    "    target_letter_to_ix = {ph:i for i,ph in enumerate(target_letters)}\n",
    "    \n",
    "    source_letters = list(set([token for word in source_seqs for token in word]))\n",
    "    source_letter_to_ix = {l:i for i,l in enumerate(source_letters)}\n",
    "    \n",
    "    return np.array(source_seqs), source_letters, source_letter_to_ix, \\\n",
    "           np.array(target_seqs), target_letters, target_letter_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 2000\n",
    "DIGITS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 2000\n"
     ]
    }
   ],
   "source": [
    "source_seqs, source_letters, source_letter_to_ix, target_seqs, target_letters, target_letter_to_ix =\\\n",
    "                    generate_grand_data(TRAINING_SIZE, DIGITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93+3*79 : 330\n",
      "2*0+30 : 30\n",
      "1*(73+45) : 118\n",
      "6*(3+67) : 420\n",
      "69*8+3 : 555\n"
     ]
    }
   ],
   "source": [
    "for source, target in zip(source_seqs[:5],target_seqs[:5]):\n",
    "    print( source,':',\"\".join(target[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_SIZE = 16\n",
    "MEMORY_SIZE = 64\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out, memory = bazal_model(QUERY_SIZE, MEMORY_SIZE, HIDDEN_SIZE,  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 average loss = 1.8074171794116005\n",
      "3*(54+8) : 839  |  186\n",
      "44*7+7 : 229  |  315\n",
      "57+15*4 : 218  |  117\n",
      "43*1+2 : 108  |  45\n",
      "27*2+87 : 608  |  141\n",
      "Epoch 1 average loss = 1.6196723853008468\n",
      "18+7*62 : 303  |  452\n",
      "48+7*4 : 98  |  76\n",
      "8+89*8 : 403  |  720\n",
      "2*(59+96) : 269  |  310\n",
      "(58+6)*8 : 880  |  512\n",
      "Epoch 2 average loss = 1.4874540646206253\n",
      "92+53*92 : 5099  |  4968\n",
      "(78+75)*87 : 7765  |  13311\n",
      "54*(25+0) : 1452  |  1350\n",
      "(1+0)*64 : 40  |  64\n",
      "0*7+1 : 3  |  1\n",
      "Epoch 3 average loss = 1.3032164222313742\n",
      "97*(3+18) : 2709  |  2037\n",
      "57*39+80 : 4453  |  2303\n",
      "74*4+7 : 359  |  303\n",
      "0*(78+6) : 0  |  0\n",
      "(87+7)*48 : 2318  |  4512\n",
      "Epoch 4 average loss = 1.0245313576234003\n",
      "7*8+75 : 103  |  131\n",
      "6+6*93 : 469  |  564\n",
      "1+61*5 : 419  |  306\n",
      "2+59*30 : 2598  |  1772\n",
      "39*42+95 : 1373  |  1733\n",
      "Epoch 5 average loss = 0.6894229277183287\n",
      "74*(2+60) : 1428  |  4588\n",
      "9*6+23 : 79  |  77\n",
      "47*(49+2) : 4234  |  2397\n",
      "7+8*5 : 30  |  47\n",
      "2*69+98 : 208  |  236\n",
      "Epoch 6 average loss = 0.4108774484056355\n",
      "1+20*0 : 4  |  1\n",
      "15*44+65 : 320  |  725\n",
      "36+42*74 : 6432  |  3144\n",
      "93*62+88 : 4708  |  5854\n",
      "(75+92)*9 : 1583  |  1503\n",
      "Epoch 7 average loss = 0.21918465745583113\n",
      "7*52+36 : 400  |  400\n",
      "64+4*43 : 366  |  236\n",
      "52+56*0 : 52  |  52\n",
      "22*(6+44) : 1100  |  1100\n",
      "(85+36)*7 : 614  |  847\n",
      "Epoch 8 average loss = 0.11009148939910773\n",
      "(49+1)*31 : 1550  |  1550\n",
      "(8+1)*87 : 783  |  783\n",
      "21*(11+73) : 1764  |  1764\n",
      "(9+6)*88 : 1320  |  1320\n",
      "11*94+0 : 1034  |  1034\n",
      "Epoch 9 average loss = 0.053998595392924356\n",
      "(55+3)*77 : 4466  |  4466\n",
      "32*8+52 : 270  |  308\n",
      "7*(6+59) : 455  |  455\n",
      "21*(5+1) : 126  |  126\n",
      "19*2+4 : 42  |  42\n",
      "Epoch 10 average loss = 0.08680151892880913\n",
      "63*0+9 : 9  |  9\n",
      "(52+8)*9 : 510  |  540\n",
      "5*6+43 : 73  |  73\n",
      "(96+56)*9 : 1368  |  1368\n",
      "45*14+80 : 710  |  710\n",
      "Epoch 11 average loss = 0.019791177655911787\n",
      "4*(9+14) : 92  |  92\n",
      "55+8*6 : 103  |  103\n",
      "10*(33+6) : 390  |  390\n",
      "30+4*9 : 66  |  66\n",
      "(3+50)*1 : 53  |  53\n",
      "Epoch 12 average loss = 0.012232270473054021\n",
      "63*76+6 : 4794  |  4794\n",
      "(58+93)*2 : 302  |  302\n",
      "36+42*74 : 3144  |  3144\n",
      "68*4+3 : 275  |  275\n",
      "1+10*7 : 71  |  71\n",
      "Epoch 13 average loss = 0.008034974855735894\n",
      "69+2*0 : 69  |  69\n",
      "(3+13)*5 : 80  |  80\n",
      "26*(4+5) : 234  |  234\n",
      "89*(3+8) : 979  |  979\n",
      "(5+71)*38 : 2888  |  2888\n",
      "Epoch 14 average loss = 0.005506300733214235\n",
      "(0+46)*4 : 184  |  184\n",
      "5*(93+0) : 465  |  465\n",
      "97+93*4 : 469  |  469\n",
      "56+60*6 : 416  |  416\n",
      "73+16*8 : 201  |  201\n",
      "Epoch 15 average loss = 0.09774490923958623\n",
      "1*(8+60) : 68  |  68\n",
      "10*76+43 : 803  |  803\n",
      "69+68*0 : 69  |  69\n",
      "(0+34)*0 : 0  |  0\n",
      "63*78+50 : 4964  |  4964\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=30\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "train_batches_per_epoch = 500\n",
    "val_batches_per_epoch = 50\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "train_source_seqs, val_source_seqs, train_target_seqs, val_target_seqs = train_test_split(source_seqs, target_seqs,\\\n",
    "                                                                                          test_size=0.33, random_state=42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        train_avg_cost = 0;\n",
    "        val_avg_cost = 0;\n",
    "\n",
    "        for _ in range(train_batches_per_epoch):\n",
    "            x,y = sample_batch(train_source_seqs, source_letter_to_ix, train_target_seqs, target_letter_to_ix, batch_size)\n",
    "            train_avg_cost += train(x, y).mean()\n",
    "        \n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            x,y = sample_batch(val_source_seqs, source_letter_to_ix, val_target_seqs, target_letter_to_ix, batch_size)\n",
    "            val_avg_cost += compute_cost(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} train average loss = {}\".format(epoch, train_avg_cost / train_batches_per_epoch))\n",
    "        print(\"Epoch {} val average loss = {}\".format(epoch, val_avg_cost / val_batches_per_epoch))\n",
    "        \n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(val_source_seqs))\n",
    "            print (val_source_seqs[ind],':', ''.join(generate_output(val_source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(val_target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
