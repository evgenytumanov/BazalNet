{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition experiment\n",
    "\n",
    "An implementation of sequence to sequence learning for performing addition\n",
    "\n",
    "* Input: \"535+61\"\n",
    "\n",
    "* Output: \"596\"\n",
    "\n",
    "Padding is handled by using a repeated sentinel character (space)\n",
    "Input may optionally be inverted, shown to increase performance in many tasks in:\n",
    "\"Learning to Execute\"\n",
    "http://arxiv.org/abs/1410.4615\n",
    "and\n",
    "\"Sequence to Sequence Learning with Neural Networks\"\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "Theoretically it introduces shorter term dependencies between source and target.\n",
    "Two digits inverted:\n",
    "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
    "Three digits inverted:\n",
    "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
    "Four digits inverted:\n",
    "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
    "Five digits inverted:\n",
    "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:\n",
      "b'C:\\\\Users\\\\tumanov\\\\AppData\\\\Local\\\\Temp\\\\try_flags_lz0bdfr7.c:4:19: fatal error: cudnn.h: No such file or directory\\r\\ncompilation terminated.\\r\\n'\n",
      "Mapped name None to device cuda0: GeForce GTX 1070 (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_trainable(net):\n",
    "    for tags in net.params.values():\n",
    "        tags -= {'trainable', 'regularizable'}\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "INVERT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try replacing GRU, or SimpleRNN.\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(size, digits, problem = lambda a, b: a+b, problem_operator='{}+{}'):\n",
    "    source_seqs = []\n",
    "    target_seqs = []\n",
    "    seen = set()\n",
    "    print('Generating data...')\n",
    "    while len(source_seqs) < TRAINING_SIZE:\n",
    "        f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                        for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "        a, b = f(), f()\n",
    "        # Skip any addition questions we've already seen\n",
    "        # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "        key = tuple(sorted((a, b)))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        # Pad the data with spaces such that it is always MAXLEN.\n",
    "        q = problem_operator.format(a, b)\n",
    "        ans = str(problem(a, b))\n",
    "\n",
    "        source_seqs.append(q)\n",
    "        target_seqs.append([\"START\"] + list(ans) + [\"END\"])\n",
    "\n",
    "    print('Total addition questions:', len(source_seqs))\n",
    "    \n",
    "    target_letters = list(set([token for ts in target_seqs for token in ts]))\n",
    "    target_letter_to_ix = {ph:i for i,ph in enumerate(target_letters)}\n",
    "    \n",
    "    source_letters = list(set([token for word in source_seqs for token in word]))\n",
    "    source_letter_to_ix = {l:i for i,l in enumerate(source_letters)}\n",
    "    \n",
    "    return np.array(source_seqs), source_letters, source_letter_to_ix, \\\n",
    "           np.array(target_seqs), target_letters, target_letter_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n"
     ]
    }
   ],
   "source": [
    "source_seqs, source_letters, source_letter_to_ix, target_seqs, target_letters, target_letter_to_ix =\\\n",
    "                    generate_data(TRAINING_SIZE, DIGITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8+0 : 8\n",
      "4+9 : 13\n",
      "569+5 : 574\n",
      "82+350 : 432\n",
      "9+34 : 43\n"
     ]
    }
   ],
   "source": [
    "for source, target in zip(source_seqs[:5],target_seqs[:5]):\n",
    "    print( source,':',\"\".join(target[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nx9qTQ6k8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19ab8c8bd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(list(map(len,target_seqs)),bins=25);\n",
    "\n",
    "# Truncate names longer than MAX_LEN characters. This can be changed\n",
    "MAX_LEN = min([150,max(list(map(len, target_seqs)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast everything from symbols into matrix of int32. Pad with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_matrix(sequences, token_to_i, max_len=None, PAX_ix=-1):\n",
    "    \"\"\"\n",
    "    Converts several sequences of tokens to a matrix, edible a neural network.\n",
    "    Crops at max_len(if given), pads shorter sequences with -1 or PAD_ix.\n",
    "    \"\"\"\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences),max_len),dtype='int8') -1\n",
    "    for i,seq in enumerate(sequences):\n",
    "        \n",
    "        row_ix = [token_to_i.get(_, 0) for _ in seq[:max_len]]\n",
    "        matrix[i,:len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = T.matrix('token sequence', 'int32')\n",
    "output_sequence = T.matrix('target target_letters', 'int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##ENCODER\n",
    "l_in = InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "l_mask = InputLayer(shape=(None, None),input_var=T.neq(input_sequence,-1)) \n",
    "\n",
    "\n",
    "l_emb = non_trainable(EmbeddingLayer(l_in, len(source_letters),  len(source_letters), W=np.diag(np.ones(len(source_letters)))))\n",
    "l_rnn = LSTMLayer(l_emb, HIDDEN_SIZE, only_return_final=True, mask_input=l_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##DECODER\n",
    "dec_in = InputLayer(shape=(None, None),input_var=output_sequence)\n",
    "dec_mask = InputLayer(shape=(None, None),input_var=T.neq(output_sequence,-1))\n",
    "\n",
    "dec_emb = non_trainable(EmbeddingLayer(dec_in, len(target_letters), len(target_letters), W=np.diag(np.ones(len(target_letters)))))\n",
    "dec_rnn = LSTMLayer(dec_emb, num_units=HIDDEN_SIZE, cell_init=l_rnn, mask_input=dec_mask)\n",
    "# WARNING! if it's lstm use cell_init, not hid_init\n",
    "\n",
    "\n",
    "#flatten batch and time to be compatible with feedforward layers (will un-flatten later)\n",
    "dec_rnn_flat = reshape(dec_rnn, (-1,dec_rnn.output_shape[-1]))\n",
    "\n",
    "l_out = DenseLayer(dec_rnn_flat, len(target_letters), nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_model(nn, learning_rate=0.001):\n",
    "    # Model weights\n",
    "    weights = get_all_params(nn)\n",
    "    network_output = get_output(nn)\n",
    "    network_output = network_output.reshape([output_sequence.shape[0],\\\n",
    "                                         output_sequence.shape[1], -1])\n",
    "    predictions_flat = network_output[:,:-1,:].reshape([-1,len(target_letters)])\n",
    "    targets = output_sequence[:,1:].ravel()\n",
    "\n",
    "    #do not count loss for '-1' tokens\n",
    "    mask = T.nonzero(T.neq(targets,-1))\n",
    "\n",
    "    loss = lasagne.objectives.categorical_crossentropy(predictions_flat[mask], targets[mask]).mean()\n",
    "    updates = lasagne.updates.adam(loss, weights, learning_rate=learning_rate)\n",
    "    #training\n",
    "    train = theano.function([input_sequence, output_sequence], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    #computing loss without training\n",
    "    compute_cost = theano.function([input_sequence, output_sequence], loss, allow_input_downcast=True)\n",
    "    #compile the function that computes probabilities for next token given previous text.\n",
    "\n",
    "    last_probas =network_output[:, -1]\n",
    "\n",
    "    probs = theano.function([input_sequence, output_sequence], last_probas)\n",
    "    return train, compute_cost, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "We now need to implement a function that generates output sequence given input.\n",
    "\n",
    "Such function must work thusly:\n",
    "```\n",
    "Init:\n",
    "x = input\n",
    "y = [\"START\"]\n",
    "\n",
    "While not_too_long:\n",
    "  p(y_next|x,y) = probabilities of next letter for y\n",
    "  \n",
    "  y_next ~ p(y_next|x,y)\n",
    "  \n",
    "  y.append(y_next)\n",
    "  \n",
    "  if y_next == \"END\":\n",
    "      break\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_output(input, probs, target_letters, target_letter_to_ix, source_letter_to_ix,\n",
    "                    output_prefix = (\"START\",),\n",
    "                    END_token=\"END\",\n",
    "                    temperature=1,\n",
    "                    sample=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement a function that generates output sequence given input.\n",
    "    \n",
    "    We recommend (but not require) you to use the pseudo-code above and inline instructions.\n",
    "    \"\"\"\n",
    "    x = as_matrix([input], source_letter_to_ix) \n",
    "    output = list(output_prefix)\n",
    "    while True:\n",
    "        y = as_matrix([output], target_letter_to_ix)\n",
    "        next_y_probs = probs(x, y)\n",
    "        next_y_probs = (next_y_probs ** temperature) / (next_y_probs ** temperature).sum()\n",
    "        if sample:\n",
    "            next_y = np.random.choice(target_letters, p=next_y_probs[0])\n",
    "        else:\n",
    "            next_y = target_letters[next_y_probs[0].argmax()]\n",
    "        next_y = str(next_y)             \n",
    "        assert type(next_y) is str, \"please return token(string/character), not it's index\"\n",
    "        \n",
    "        output.append(next_y)\n",
    "\n",
    "        if next_y==END_token:\n",
    "            break\n",
    "            \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#source_seqs = np.array(source_seqs)\n",
    "#target_seqs = np.array(target_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(source_seqs, source_letter_to_ix, target_seqs, target_letter_to_ix, batch_size):\n",
    "    \"\"\"samples a random batch of source and target sequences, batch_size elements\"\"\"\n",
    "    batch_ix = np.random.randint(0,len(source_seqs),size=batch_size)\n",
    "    source_seqs_batch=as_matrix(source_seqs[batch_ix], source_letter_to_ix) \n",
    "    target_seqs_batch=as_matrix(target_seqs[batch_ix], target_letter_to_ix)\n",
    "    \n",
    "    return source_seqs_batch,target_seqs_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 average loss = 1.7253314948323974\n",
      "23+321 : 360\n",
      "460+1 : 299\n",
      "408+846 : 1210\n",
      "662+55 : 1024\n",
      "151+600 : 910\n",
      "Epoch 1 average loss = 1.4339594839586183\n",
      "176+596 : 799\n",
      "696+24 : 653\n",
      "924+1 : 998\n",
      "5+138 : 169\n",
      "197+618 : 786\n",
      "Epoch 2 average loss = 1.24666821097135\n",
      "19+31 : 34\n",
      "849+18 : 868\n",
      "82+948 : 1009\n",
      "594+904 : 1444\n",
      "90+819 : 918\n",
      "Epoch 3 average loss = 1.1148210965253742\n",
      "99+278 : 650\n",
      "824+573 : 1528\n",
      "471+818 : 1317\n",
      "849+757 : 1532\n",
      "40+386 : 342\n",
      "Epoch 4 average loss = 1.0053605147956945\n",
      "6+498 : 508\n",
      "968+622 : 1569\n",
      "413+927 : 1305\n",
      "77+25 : 101\n",
      "999+928 : 1908\n",
      "Epoch 5 average loss = 0.9228021664615181\n",
      "58+829 : 875\n",
      "503+94 : 606\n",
      "61+193 : 362\n",
      "190+600 : 737\n",
      "983+7 : 999\n",
      "Epoch 6 average loss = 0.8624573520094373\n",
      "49+126 : 159\n",
      "703+348 : 111\n",
      "31+37 : 73\n",
      "7+518 : 526\n",
      "180+731 : 918\n",
      "Epoch 7 average loss = 0.8106911486171051\n",
      "6+870 : 876\n",
      "631+306 : 953\n",
      "683+46 : 718\n",
      "4+282 : 289\n",
      "593+93 : 694\n",
      "Epoch 8 average loss = 0.7688101012460603\n",
      "882+980 : 1853\n",
      "414+685 : 1092\n",
      "23+39 : 73\n",
      "449+133 : 572\n",
      "438+18 : 371\n",
      "Epoch 9 average loss = 0.7322625755532872\n",
      "109+56 : 164\n",
      "158+6 : 234\n",
      "824+65 : 884\n",
      "873+63 : 936\n",
      "56+78 : 135\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=10\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "batches_per_epoch = 500\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        avg_cost = 0;\n",
    "\n",
    "        for _ in range(batches_per_epoch):\n",
    "\n",
    "            x,y = sample_batch(source_seqs, source_letter_to_ix, target_seqs, target_letter_to_ix, batch_size)\n",
    "            avg_cost += train(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} average loss = {}\".format(epoch, avg_cost / batches_per_epoch))\n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(source_seqs))\n",
    "            print (source_seqs[ind],':', ''.join(generate_output(source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bazal module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CerMemory(lasagne.layers.Layer):\n",
    "    def __init__(self, incoming, memory_size, M=lasagne.init.Orthogonal(), **kwargs):\n",
    "        super(CerMemory, self).__init__(incoming, **kwargs)\n",
    "        self.query_shape = self.input_shape[1]\n",
    "        self.memory_size = memory_size\n",
    "        self.M = self.add_param(M, (self.query_shape, memory_size), name='M')\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        m = self.M / T.sqrt(T.sqr(self.M).sum(axis=0)).reshape(self.M.shape[1], 1)\n",
    "        weights =  T.dot(input, m)\n",
    "        return T.dot(weights, m.T)\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], self.query_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EvcNormalizer(lasagne.layers.Layer):\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return (input.T / T.sqrt(T.sqr(input).sum(axis=1)).reshape(input.shape[0], 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_SIZE = 16\n",
    "MEMORY_SIZE = 64\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bazal_model(query_size, memory_size, hidden_size, memory_benchmark=False):\n",
    "\n",
    "    ##ENCODER\n",
    "    l_in = InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "    l_mask = InputLayer(shape=(None, None),input_var=T.neq(input_sequence,-1)) \n",
    "\n",
    "\n",
    "    l_emb = non_trainable(EmbeddingLayer(l_in, len(source_letters),  len(source_letters), W=np.diag(np.ones(len(source_letters)))))\n",
    "    features = LSTMLayer(l_emb, HIDDEN_SIZE, only_return_final=True, mask_input=l_mask)\n",
    "    \n",
    "    if not memory_benchmark:\n",
    "        ## QUERY BUILDER\n",
    "        query = DenseLayer(features, QUERY_SIZE, nonlinearity=None)\n",
    "        query = EvcNormalizer(query)\n",
    "        ## Memory\n",
    "        memory = CerMemory(query, MEMORY_SIZE)\n",
    "    else:\n",
    "        memory = DenseLayer(DenseLayer(features, QUERY_SIZE), QUERY_SIZE)\n",
    "        \n",
    "    to_decode = ConcatLayer([features, memory])\n",
    "    \n",
    "    ##DECODER\n",
    "    dec_in = InputLayer(shape=(None, None),input_var=output_sequence)\n",
    "    dec_mask = InputLayer(shape=(None, None),input_var=T.neq(output_sequence,-1))\n",
    "\n",
    "    dec_emb = non_trainable(EmbeddingLayer(dec_in, len(target_letters), len(target_letters), W=np.diag(np.ones(len(target_letters)))))\n",
    "    dec_rnn = LSTMLayer(dec_emb, num_units=to_decode.output_shape[-1], cell_init=to_decode, mask_input=dec_mask)\n",
    "    # WARNING! if it's lstm use cell_init, not hid_init\n",
    "\n",
    "\n",
    "    #flatten batch and time to be compatible with feedforward layers (will un-flatten later)\n",
    "    dec_rnn_flat = reshape(dec_rnn, (-1,dec_rnn.output_shape[-1]))\n",
    "\n",
    "    l_out = DenseLayer(dec_rnn_flat, len(target_letters), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out, memory = bazal_model(QUERY_SIZE, MEMORY_SIZE, HIDDEN_SIZE, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 average loss = 0.08192493701372242\n",
      "900+142 : 1032\n",
      "425+357 : 782\n",
      "88+150 : 238\n",
      "64+709 : 773\n",
      "773+831 : 1594\n",
      "Epoch 1 average loss = 0.07422627220413225\n",
      "639+621 : 1260\n",
      "280+73 : 353\n",
      "366+71 : 437\n",
      "103+802 : 905\n",
      "75+14 : 89\n",
      "Epoch 2 average loss = 0.07308663476110505\n",
      "229+5 : 234\n",
      "24+335 : 359\n",
      "548+53 : 691\n",
      "949+72 : 1021\n",
      "967+138 : 1105\n",
      "Epoch 3 average loss = 0.062076398790943656\n",
      "843+5 : 848\n",
      "608+798 : 1406\n",
      "31+588 : 619\n",
      "98+763 : 861\n",
      "27+828 : 854\n",
      "Epoch 4 average loss = 0.05984968836892858\n",
      "81+963 : 1044\n",
      "109+75 : 184\n",
      "93+1 : 94\n",
      "42+295 : 337\n",
      "0+446 : 446\n",
      "Epoch 5 average loss = 0.056843585614065974\n",
      "95+965 : 1060\n",
      "257+775 : 1032\n",
      "50+752 : 802\n",
      "609+65 : 674\n",
      "74+435 : 509\n",
      "Epoch 6 average loss = 0.04981979398739722\n",
      "3+126 : 129\n",
      "85+478 : 563\n",
      "679+150 : 829\n",
      "256+807 : 1063\n",
      "4+16 : 20\n",
      "Epoch 7 average loss = 0.05039138476792447\n",
      "990+583 : 1563\n",
      "83+204 : 287\n",
      "235+3 : 238\n",
      "616+866 : 1482\n",
      "221+0 : 221\n",
      "Epoch 8 average loss = 0.04552381015230711\n",
      "16+293 : 309\n",
      "95+13 : 108\n",
      "484+83 : 567\n",
      "617+590 : 1207\n",
      "751+144 : 895\n",
      "Epoch 9 average loss = 0.04702616370891344\n",
      "8+158 : 166\n",
      "93+248 : 341\n",
      "75+228 : 303\n",
      "613+795 : 1408\n",
      "162+930 : 1092\n",
      "Epoch 10 average loss = 0.0385921900248357\n",
      "32+300 : 332\n",
      "591+833 : 1424\n",
      "602+695 : 1297\n",
      "501+200 : 700\n",
      "341+76 : 417\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=30\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "batches_per_epoch = 500\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        avg_cost = 0;\n",
    "\n",
    "        for _ in range(batches_per_epoch):\n",
    "\n",
    "            x,y = sample_batch(source_seqs, source_letter_to_ix, target_seqs, target_letter_to_ix, batch_size)\n",
    "            avg_cost += train(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} average loss = {}\".format(epoch, avg_cost / batches_per_epoch))\n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(source_seqs))\n",
    "\n",
    "            print (source_seqs[ind],':', ''.join(generate_output(source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(target_seqs[ind][1:-1]))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M_add = np.array(memory.M.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./zoo/addprob_memory_after30epochs.npy', M_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same for prod problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 100000\n",
    "DIGITS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 100000\n"
     ]
    }
   ],
   "source": [
    "source_seqs, source_letters, source_letter_to_ix, target_seqs, target_letters, target_letter_to_ix =\\\n",
    "                    generate_data(TRAINING_SIZE, DIGITS, lambda a, b: a * b, '{}*{}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4*5 : 20\n",
      "57*4 : 228\n",
      "16*728 : 11648\n",
      "4*72 : 288\n",
      "134*49 : 6566\n"
     ]
    }
   ],
   "source": [
    "for source, target in zip(source_seqs[:5],target_seqs[:5]):\n",
    "    print( source,':',\"\".join(target[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_SIZE = 16\n",
    "MEMORY_SIZE = 64\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_out, memory = bazal_model(QUERY_SIZE, MEMORY_SIZE, HIDDEN_SIZE,  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 average loss = 1.963512947573288\n",
      "279*464 : 122014  |  129456\n",
      "49*948 : 55445  |  46452\n",
      "46*908 : 40524  |  41768\n",
      "73*862 : 67708  |  62926\n",
      "779*61 : 54852  |  47519\n",
      "Epoch 1 average loss = 1.5034067121394223\n",
      "648*114 : 94024  |  73872\n",
      "90*40 : 3700  |  3600\n",
      "16*72 : 1344  |  1152\n",
      "830*45 : 42200  |  37350\n",
      "24*315 : 6630  |  7560\n",
      "Epoch 2 average loss = 1.2854283706530525\n",
      "197*50 : 9750  |  9850\n",
      "10*42 : 560  |  420\n",
      "39*189 : 6363  |  7371\n",
      "996*696 : 728016  |  693216\n",
      "62*427 : 25382  |  26474\n",
      "Epoch 3 average loss = 1.1760758876032449\n",
      "13*688 : 9104  |  8944\n",
      "37*947 : 35833  |  35039\n",
      "899*712 : 595256  |  640088\n",
      "59*874 : 53162  |  51566\n",
      "556*297 : 154596  |  165132\n",
      "Epoch 4 average loss = 1.112071435838922\n",
      "596*39 : 23848  |  23244\n",
      "604*226 : 135632  |  136504\n",
      "57*218 : 12322  |  12426\n",
      "704*42 : 31312  |  29568\n",
      "826*129 : 104738  |  106554\n",
      "Epoch 5 average loss = 1.072393224169519\n",
      "43*959 : 41699  |  41237\n",
      "21*174 : 3814  |  3654\n",
      "386*601 : 234438  |  231986\n",
      "64*211 : 13624  |  13504\n",
      "43*854 : 37358  |  36722\n",
      "Epoch 6 average loss = 1.0339987956532741\n",
      "390*113 : 41910  |  44070\n",
      "545*12 : 6740  |  6540\n",
      "97*273 : 27381  |  26481\n",
      "475*64 : 30660  |  30400\n",
      "858*12 : 10048  |  10296\n",
      "Epoch 7 average loss = 1.006190946640481\n",
      "95*74 : 7030  |  7030\n",
      "831*777 : 662711  |  645687\n",
      "262*66 : 15912  |  17292\n",
      "471*31 : 13563  |  14601\n",
      "951*905 : 868465  |  860655\n",
      "Epoch 8 average loss = 0.9839063625186705\n",
      "331*847 : 273597  |  280357\n",
      "957*676 : 684964  |  646932\n",
      "870*302 : 276980  |  262740\n",
      "680*17 : 12120  |  11560\n",
      "53*31 : 1651  |  1643\n",
      "Epoch 9 average loss = 0.9617733196788506\n",
      "736*58 : 43712  |  42688\n",
      "728*23 : 16872  |  16744\n",
      "37*200 : 7800  |  7400\n",
      "8*245 : 2040  |  1960\n",
      "816*878 : 721424  |  716448\n",
      "Epoch 10 average loss = 0.9454739217440298\n",
      "674*307 : 207654  |  206918\n",
      "172*455 : 80360  |  78260\n",
      "947*89 : 82577  |  84283\n",
      "86*277 : 23994  |  23822\n",
      "125*254 : 31330  |  31750\n",
      "Epoch 11 average loss = 0.9290221953614045\n",
      "534*98 : 55164  |  52332\n",
      "69*26 : 1778  |  1794\n",
      "388*73 : 28732  |  28324\n",
      "493*33 : 16621  |  16269\n",
      "24*604 : 14504  |  14496\n",
      "Epoch 12 average loss = 0.9098488765230753\n",
      "14*78 : 1064  |  1092\n",
      "128*606 : 76368  |  77568\n",
      "654*960 : 617360  |  627840\n",
      "762*413 : 315046  |  314706\n",
      "496*56 : 26832  |  27776\n",
      "Epoch 13 average loss = 0.8994983273973828\n",
      "955*96 : 86260  |  91680\n",
      "858*43 : 35398  |  36894\n",
      "62*887 : 55874  |  54994\n",
      "442*46 : 19856  |  20332\n",
      "329*699 : 235539  |  229971\n",
      "Epoch 14 average loss = 0.8873500666467513\n",
      "105*125 : 15075  |  13125\n",
      "857*854 : 752054  |  731878\n",
      "9*465 : 4045  |  4185\n",
      "149*11 : 1663  |  1639\n",
      "65*894 : 57790  |  58110\n",
      "Epoch 15 average loss = 0.8796535365524594\n",
      "782*386 : 300148  |  301852\n",
      "1*67 : 69  |  67\n",
      "17*598 : 10778  |  10166\n",
      "4*559 : 2276  |  2236\n",
      "842*586 : 476068  |  493412\n",
      "Epoch 16 average loss = 0.8725858738479422\n",
      "523*212 : 113236  |  110876\n",
      "363*23 : 8297  |  8349\n",
      "10*12 : 120  |  120\n",
      "122*38 : 4764  |  4636\n",
      "995*4 : 3980  |  3980\n",
      "Epoch 17 average loss = 0.8636831274268587\n",
      "475*562 : 273950  |  266950\n",
      "657*923 : 600369  |  606411\n",
      "756*9 : 6856  |  6804\n",
      "739*729 : 537301  |  538731\n",
      "88*948 : 84024  |  83424\n",
      "Epoch 18 average loss = 0.85822429587198\n",
      "610*67 : 41530  |  40870\n",
      "63*657 : 41771  |  41391\n",
      "87*444 : 38116  |  38628\n",
      "68*91 : 6148  |  6188\n",
      "723*72 : 51408  |  52056\n",
      "Epoch 19 average loss = 0.8483644320914925\n",
      "888*8 : 6944  |  7104\n",
      "858*98 : 84388  |  84084\n",
      "707*952 : 668736  |  673064\n",
      "39*281 : 11639  |  10959\n",
      "827*75 : 62625  |  62025\n",
      "Epoch 20 average loss = 0.8436533536564224\n",
      "835*21 : 17215  |  17535\n",
      "651*452 : 293668  |  294252\n",
      "162*72 : 11864  |  11664\n",
      "187*954 : 185706  |  178398\n",
      "79*364 : 28684  |  28756\n",
      "Epoch 21 average loss = 0.8337214656841986\n",
      "74*136 : 9952  |  10064\n",
      "88*42 : 3736  |  3696\n",
      "51*232 : 11928  |  11832\n",
      "657*454 : 303042  |  298278\n",
      "611*14 : 8626  |  8554\n",
      "Epoch 22 average loss = 0.8250104814851055\n",
      "904*139 : 128336  |  125656\n",
      "347*21 : 7213  |  7287\n",
      "13*136 : 1776  |  1768\n",
      "43*232 : 9808  |  9976\n",
      "943*587 : 549781  |  553541\n",
      "Epoch 23 average loss = 0.8150195913395262\n",
      "285*541 : 153925  |  154185\n",
      "595*91 : 53885  |  54145\n",
      "8*321 : 2624  |  2568\n",
      "251*2 : 482  |  502\n",
      "62*658 : 40596  |  40796\n",
      "Epoch 24 average loss = 0.8064130764519312\n",
      "474*184 : 89632  |  87216\n",
      "924*235 : 214420  |  217140\n",
      "445*15 : 6775  |  6675\n",
      "111*64 : 7208  |  7104\n",
      "10*740 : 7300  |  7400\n",
      "Epoch 25 average loss = 0.7915789687804943\n",
      "22*428 : 9384  |  9416\n",
      "414*898 : 365804  |  371772\n",
      "92*987 : 89828  |  90804\n",
      "365*20 : 7300  |  7300\n",
      "14*627 : 8854  |  8778\n",
      "Epoch 26 average loss = 0.776609499241594\n",
      "13*815 : 10995  |  10595\n",
      "313*77 : 24731  |  24101\n",
      "3*648 : 2024  |  1944\n",
      "490*86 : 43740  |  42140\n",
      "682*723 : 497046  |  493086\n",
      "Epoch 27 average loss = 0.7615894587364344\n",
      "201*980 : 196940  |  196980\n",
      "828*30 : 25160  |  24840\n",
      "7*630 : 4490  |  4410\n",
      "107*69 : 7279  |  7383\n",
      "34*431 : 14794  |  14654\n",
      "Epoch 28 average loss = 0.7507993620462713\n",
      "405*73 : 29455  |  29565\n",
      "52*770 : 38680  |  40040\n",
      "567*4 : 2244  |  2268\n",
      "778*78 : 60924  |  60684\n",
      "615*661 : 397695  |  406515\n",
      "Epoch 29 average loss = 0.7404301953133332\n",
      "513*796 : 407924  |  408348\n",
      "881*67 : 59027  |  59027\n",
      "15*951 : 14005  |  14265\n",
      "220*367 : 80660  |  80740\n",
      "84*576 : 48128  |  48384\n",
      "Epoch 30 average loss = 0.7280135437375291\n",
      "92*849 : 78788  |  78108\n",
      "840*96 : 81120  |  80640\n",
      "85*944 : 79360  |  80240\n",
      "683*546 : 373158  |  372918\n",
      "775*14 : 10650  |  10850\n",
      "Epoch 31 average loss = 0.7155160350554155\n",
      "877*624 : 546144  |  547248\n",
      "437*648 : 284976  |  283176\n",
      "581*632 : 370376  |  367192\n",
      "17*365 : 6365  |  6205\n",
      "522*83 : 43598  |  43326\n",
      "Epoch 32 average loss = 0.7046075954259994\n",
      "427*32 : 13288  |  13664\n",
      "33*63 : 2029  |  2079\n",
      "35*937 : 33135  |  32795\n",
      "812*444 : 360816  |  360528\n",
      "26*90 : 2340  |  2340\n",
      "Epoch 33 average loss = 0.6932680837258159\n",
      "767*870 : 674690  |  667290\n",
      "224*398 : 86736  |  89152\n",
      "763*511 : 391603  |  389893\n",
      "38*565 : 21130  |  21470\n",
      "57*262 : 15158  |  14934\n",
      "Epoch 34 average loss = 0.6776702319221736\n",
      "965*414 : 399370  |  399510\n",
      "70*921 : 64690  |  64470\n",
      "177*9 : 1653  |  1593\n",
      "357*53 : 18981  |  18921\n",
      "69*738 : 51062  |  50922\n",
      "Epoch 35 average loss = 0.6692887514678185\n",
      "43*642 : 27826  |  27606\n",
      "938*881 : 829278  |  826378\n",
      "728*193 : 139304  |  140504\n",
      "352*330 : 113760  |  116160\n",
      "453*611 : 272083  |  276783\n",
      "Epoch 36 average loss = 0.6605237584941754\n",
      "53*330 : 17490  |  17490\n",
      "34*106 : 3652  |  3604\n",
      "864*421 : 363944  |  363744\n",
      "59*461 : 26939  |  27199\n",
      "973*707 : 687281  |  687911\n",
      "Epoch 37 average loss = 0.6530150246199211\n",
      "51*463 : 23971  |  23613\n",
      "75*270 : 20250  |  20250\n",
      "41*227 : 9357  |  9307\n",
      "23*85 : 1945  |  1955\n",
      "483*321 : 154433  |  155043\n",
      "Epoch 38 average loss = 0.6438706846126564\n",
      "820*1 : 820  |  820\n",
      "726*71 : 49906  |  51546\n",
      "88*98 : 8624  |  8624\n",
      "389*987 : 378833  |  383943\n",
      "823*915 : 760595  |  753045\n",
      "Epoch 39 average loss = 0.6363512721168756\n",
      "86*416 : 34896  |  35776\n",
      "682*381 : 264362  |  259842\n",
      "14*632 : 8928  |  8848\n",
      "333*418 : 138774  |  139194\n",
      "691*122 : 81642  |  84302\n",
      "Epoch 40 average loss = 0.628160612028413\n",
      "666*42 : 28196  |  27972\n",
      "602*818 : 492876  |  492436\n",
      "739*727 : 538553  |  537253\n",
      "475*93 : 45125  |  44175\n",
      "75*557 : 41925  |  41775\n",
      "Epoch 41 average loss = 0.6249500732157905\n",
      "674*875 : 587550  |  589750\n",
      "888*7 : 6136  |  6216\n",
      "972*81 : 78612  |  78732\n",
      "533*303 : 162279  |  161499\n",
      "912*176 : 160912  |  160512\n",
      "Epoch 42 average loss = 0.6173822884317108\n",
      "142*7 : 1034  |  994\n",
      "914*140 : 127560  |  127960\n",
      "117*25 : 2725  |  2925\n",
      "162*130 : 22060  |  21060\n",
      "989*964 : 941316  |  953396\n",
      "Epoch 43 average loss = 0.616785231992473\n",
      "749*152 : 114248  |  113848\n",
      "869*877 : 762103  |  762113\n",
      "628*32 : 20256  |  20096\n",
      "243*23 : 5779  |  5589\n",
      "311*78 : 24338  |  24258\n",
      "Epoch 44 average loss = 0.6108704439205519\n",
      "63*893 : 55959  |  56259\n",
      "34*116 : 3944  |  3944\n",
      "581*682 : 395202  |  396242\n",
      "438*745 : 325910  |  326310\n",
      "22*651 : 14242  |  14322\n",
      "Epoch 45 average loss = 0.6078667369710881\n",
      "785*488 : 384840  |  383080\n",
      "364*54 : 20056  |  19656\n",
      "773*227 : 179481  |  175471\n",
      "5*53 : 265  |  265\n",
      "643*354 : 226462  |  227622\n",
      "Epoch 46 average loss = 0.6009975951364738\n",
      "417*214 : 91858  |  89238\n",
      "71*72 : 5032  |  5112\n",
      "26*41 : 1086  |  1066\n",
      "62*382 : 23284  |  23684\n",
      "239*277 : 63273  |  66203\n",
      "Epoch 47 average loss = 0.5998357267878305\n",
      "586*57 : 32962  |  33402\n",
      "315*67 : 21345  |  21105\n",
      "602*66 : 39772  |  39732\n",
      "967*564 : 542308  |  545388\n",
      "16*314 : 5184  |  5024\n",
      "Epoch 48 average loss = 0.5928024644142964\n",
      "95*806 : 77130  |  76570\n",
      "475*562 : 267650  |  266950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369*165 : 61685  |  60885\n",
      "432*57 : 24944  |  24624\n",
      "161*678 : 110338  |  109158\n",
      "Epoch 49 average loss = 0.5917274701723195\n",
      "3*105 : 305  |  315\n",
      "659*40 : 26360  |  26360\n",
      "587*398 : 238706  |  233626\n",
      "35*69 : 2515  |  2415\n",
      "258*646 : 164788  |  166668\n",
      "Epoch 50 average loss = 0.5902895835652807\n",
      "817*603 : 495111  |  492651\n",
      "88*898 : 78624  |  79024\n",
      "19*155 : 2755  |  2945\n",
      "87*467 : 40589  |  40629\n",
      "924*504 : 468816  |  465696\n",
      "Epoch 51 average loss = 0.5900596120400515\n",
      "231*590 : 135390  |  136290\n",
      "889*8 : 6952  |  7112\n",
      "71*542 : 37782  |  38482\n",
      "263*88 : 22744  |  23144\n",
      "405*997 : 396845  |  403785\n",
      "Epoch 52 average loss = 0.584726741100671\n",
      "81*434 : 34534  |  35154\n",
      "69*337 : 23483  |  23253\n",
      "893*12 : 10696  |  10716\n",
      "74*947 : 69738  |  70078\n",
      "324*174 : 56216  |  56376\n",
      "Epoch 53 average loss = 0.5859896405568876\n",
      "140*164 : 21360  |  22960\n",
      "42*94 : 4068  |  3948\n",
      "31*713 : 22463  |  22103\n",
      "75*143 : 10775  |  10725\n",
      "652*22 : 14624  |  14344\n",
      "Epoch 54 average loss = 0.5779847186002541\n",
      "817*153 : 125831  |  125001\n",
      "71*989 : 69939  |  70219\n",
      "31*627 : 19487  |  19437\n",
      "22*984 : 21648  |  21648\n",
      "48*22 : 1056  |  1056\n",
      "Epoch 55 average loss = 0.5783334868505403\n",
      "148*26 : 3848  |  3848\n",
      "22*963 : 21346  |  21186\n",
      "516*550 : 280600  |  283800\n",
      "98*149 : 14962  |  14602\n",
      "859*33 : 28257  |  28347\n",
      "Epoch 56 average loss = 0.5742930902612802\n",
      "60*284 : 16880  |  17040\n",
      "271*6 : 1626  |  1626\n",
      "391*922 : 361382  |  360502\n",
      "42*680 : 28960  |  28560\n",
      "0*780 : 0  |  0\n",
      "Epoch 57 average loss = 0.5722579484866557\n",
      "505*656 : 325080  |  331280\n",
      "5*825 : 4075  |  4125\n",
      "71*766 : 54206  |  54386\n",
      "466*9 : 4154  |  4194\n",
      "19*243 : 4797  |  4617\n",
      "Epoch 58 average loss = 0.5696170049453597\n",
      "50*996 : 49800  |  49800\n",
      "199*554 : 107606  |  110246\n",
      "83*961 : 79773  |  79763\n",
      "744*302 : 226768  |  224688\n",
      "35*196 : 6780  |  6860\n",
      "Epoch 59 average loss = 0.5719102982264996\n",
      "41*730 : 29930  |  29930\n",
      "862*76 : 65832  |  65512\n",
      "115*680 : 78600  |  78200\n",
      "810*41 : 32810  |  33210\n",
      "522*10 : 5140  |  5220\n",
      "Epoch 60 average loss = 0.5710613415262984\n",
      "435*2 : 870  |  870\n",
      "429*752 : 321168  |  322608\n",
      "0*334 : 0  |  0\n",
      "252*274 : 67368  |  69048\n",
      "730*739 : 544570  |  539470\n",
      "Epoch 61 average loss = 0.5651342674266421\n",
      "168*30 : 5120  |  5040\n",
      "157*987 : 159419  |  154959\n",
      "74*777 : 57558  |  57498\n",
      "585*83 : 48735  |  48555\n",
      "80*207 : 16560  |  16560\n",
      "Epoch 62 average loss = 0.5639084988490853\n",
      "169*119 : 19531  |  20111\n",
      "955*8 : 7560  |  7640\n",
      "266*363 : 96098  |  96558\n",
      "663*55 : 36785  |  36465\n",
      "124*611 : 73444  |  75764\n",
      "Epoch 63 average loss = 0.5642486998171858\n",
      "35*441 : 15365  |  15435\n",
      "643*214 : 140782  |  137602\n",
      "239*378 : 89982  |  90342\n",
      "29*716 : 20904  |  20764\n",
      "43*642 : 27566  |  27606\n",
      "Epoch 64 average loss = 0.5602062796631535\n",
      "202*176 : 36752  |  35552\n",
      "388*73 : 28164  |  28324\n",
      "506*3 : 1418  |  1518\n",
      "10*333 : 3330  |  3330\n",
      "368*62 : 22896  |  22816\n",
      "Epoch 65 average loss = 0.5608122329024\n",
      "873*520 : 462760  |  453960\n",
      "331*229 : 73209  |  75799\n",
      "2*986 : 1972  |  1972\n",
      "72*665 : 47760  |  47880\n",
      "61*497 : 29797  |  30317\n",
      "Epoch 66 average loss = 0.5576963766631284\n",
      "75*237 : 17675  |  17775\n",
      "88*291 : 25448  |  25608\n",
      "278*57 : 15766  |  15846\n",
      "756*619 : 461932  |  467964\n",
      "726*98 : 71108  |  71148\n",
      "Epoch 67 average loss = 0.5581126184753135\n",
      "591*410 : 242110  |  242310\n",
      "753*15 : 11305  |  11295\n",
      "639*37 : 23453  |  23643\n",
      "983*974 : 958502  |  957442\n",
      "79*704 : 55736  |  55616\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=100\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "batches_per_epoch = 500\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        avg_cost = 0;\n",
    "\n",
    "        for _ in range(batches_per_epoch):\n",
    "\n",
    "            x,y = sample_batch(source_seqs, source_letter_to_ix, target_seqs, target_letter_to_ix, batch_size)\n",
    "            avg_cost += train(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} average loss = {}\".format(epoch, avg_cost / batches_per_epoch))\n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(source_seqs))\n",
    "            print (source_seqs[ind],':', ''.join(generate_output(source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grand problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob1, prob_op1 = lambda a, b, c: (a + b) * c, '({}+{})*{}'\n",
    "prob2, prob_op2 = lambda a, b, c:  a * (b + c), '{}*({}+{})'\n",
    "prob3, prob_op3 = lambda a, b, c:  a + b * c, '{}+{}*{}'\n",
    "prob4, prob_op4 = lambda a, b, c:  a * b + c, '{}*{}+{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grand_data(size, digits, problems = [prob1, prob2, prob3, prob4],\\\n",
    "                        problem_operators=[prob_op1, prob_op2, prob_op3, prob_op4]):\n",
    "    source_seqs = []\n",
    "    target_seqs = []\n",
    "    seen = set()\n",
    "    assert len(problem_operators) == len(problems)\n",
    "    print('Generating data...')\n",
    "    while len(source_seqs) < TRAINING_SIZE:\n",
    "        f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                        for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "        a, b, c = f(), f(), f()\n",
    "        # Skip any addition questions we've already seen\n",
    "        # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "        key = tuple(sorted((a, b, c)))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        \n",
    "        coin = np.random.randint(0, len(problems))\n",
    "        \n",
    "        # Pad the data with spaces such that it is always MAXLEN.\n",
    "        q = problem_operators[coin].format(a, b, c)\n",
    "        ans = str(problems[coin](a, b, c))\n",
    "\n",
    "        source_seqs.append(q)\n",
    "        target_seqs.append([\"START\"] + list(ans) + [\"END\"])\n",
    "\n",
    "    print('Total addition questions:', len(source_seqs))\n",
    "    \n",
    "    target_letters = list(set([token for ts in target_seqs for token in ts]))\n",
    "    target_letter_to_ix = {ph:i for i,ph in enumerate(target_letters)}\n",
    "    \n",
    "    source_letters = list(set([token for word in source_seqs for token in word]))\n",
    "    source_letter_to_ix = {l:i for i,l in enumerate(source_letters)}\n",
    "    \n",
    "    return np.array(source_seqs), source_letters, source_letter_to_ix, \\\n",
    "           np.array(target_seqs), target_letters, target_letter_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n"
     ]
    }
   ],
   "source": [
    "source_seqs, source_letters, source_letter_to_ix, target_seqs, target_letters, target_letter_to_ix =\\\n",
    "                    generate_grand_data(TRAINING_SIZE, DIGITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97*2+875 : 1069\n",
      "5*1+85 : 90\n",
      "56+33*30 : 1046\n",
      "7*29+943 : 1146\n",
      "67+24*9 : 283\n"
     ]
    }
   ],
   "source": [
    "for source, target in zip(source_seqs[:5],target_seqs[:5]):\n",
    "    print( source,':',\"\".join(target[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_SIZE = 16\n",
    "MEMORY_SIZE = 64\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_out, memory = bazal_model(QUERY_SIZE, MEMORY_SIZE, HIDDEN_SIZE,  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, compute_cost, probs = handle_model(l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 average loss = 1.9762239150104726\n",
      "86+97*253 : 67941\n",
      "834*(98+636) : 277498\n",
      "(311+9)*939 : 48005\n",
      "864+6*58 : 770\n",
      "0*219+23 : 985\n",
      "Epoch 1 average loss = 1.8523058288088392\n",
      "241+808*2 : 2014\n",
      "351+0*81 : 297\n",
      "(8+99)*29 : 2478\n",
      "9+4*48 : 611\n",
      "19*(56+8) : 1600\n",
      "Epoch 2 average loss = 1.8081808853328298\n",
      "160+5*9 : 325\n",
      "6+513*723 : 727103\n",
      "7*(2+910) : 11626\n",
      "9*622+9 : 12284\n",
      "70*6+38 : 727\n",
      "Epoch 3 average loss = 1.7739406315271635\n",
      "(609+4)*87 : 58900\n",
      "80*48+79 : 3726\n",
      "92*(2+34) : 1300\n",
      "9+0*134 : 4\n",
      "396+554*347 : 202498\n",
      "Epoch 4 average loss = 1.7330751586989965\n",
      "7+87*0 : 0\n",
      "83*343+588 : 51978\n",
      "9+67*15 : 407\n",
      "786+462*566 : 233954\n",
      "(21+701)*591 : 208556\n",
      "Epoch 5 average loss = 1.6808404178752085\n",
      "0*(185+319) : 0\n",
      "516*555+482 : 226651\n",
      "688+2*9 : 616\n",
      "(30+59)*232 : 10124\n",
      "97*(506+942) : 55088\n",
      "Epoch 6 average loss = 1.6085857724059756\n",
      "(524+535)*52 : 59792\n",
      "1*(706+8) : 528\n",
      "(97+1)*71 : 5033\n",
      "314*(1+2) : 592\n",
      "8*(9+924) : 2488\n",
      "Epoch 7 average loss = 1.5282006095057041\n",
      "0*(60+7) : 0\n",
      "9+191*6 : 1127\n",
      "(750+43)*4 : 3124\n",
      "8*(391+5) : 3256\n",
      "938+1*561 : 1073\n",
      "Epoch 8 average loss = 1.4507762444730983\n",
      "175+494*97 : 44749\n",
      "49*(78+855) : 69897\n",
      "8+30*640 : 20064\n",
      "970*1+838 : 1768\n",
      "941*(161+5) : 188262\n",
      "Epoch 9 average loss = 1.3900516641550673\n",
      "400+8*0 : 300\n",
      "(46+45)*77 : 11515\n",
      "4*254+0 : 1000\n",
      "533+8*9 : 1301\n",
      "(52+65)*2 : 182\n",
      "Epoch 10 average loss = 1.348635421659069\n",
      "(73+909)*3 : 2762\n",
      "34*44+96 : 1632\n",
      "38*37+42 : 1338\n",
      "79+445*43 : 23088\n",
      "5*87+679 : 956\n",
      "Epoch 11 average loss = 1.3168797388295663\n",
      "4+89*622 : 51232\n",
      "852+3*878 : 3534\n",
      "521*(507+965) : 638606\n",
      "44*(515+4) : 24302\n",
      "(411+1)*10 : 5200\n",
      "Epoch 12 average loss = 1.286315307994327\n",
      "66*7+978 : 1226\n",
      "414+69*6 : 780\n",
      "81*(37+99) : 10656\n",
      "8*(127+4) : 1016\n",
      "(768+7)*431 : 345415\n",
      "Epoch 13 average loss = 1.2655295355869374\n",
      "52*815+70 : 42620\n",
      "319*(598+338) : 334410\n",
      "361*41+342 : 16265\n",
      "(3+39)*86 : 3518\n",
      "87*265+18 : 23177\n",
      "Epoch 14 average loss = 1.2402932361623327\n",
      "3+78*222 : 17825\n",
      "24*(2+924) : 25238\n",
      "660+418*854 : 336478\n",
      "51*171+9 : 8014\n",
      "112*(55+590) : 92256\n",
      "Epoch 15 average loss = 1.2245390681766406\n",
      "295*(9+324) : 104735\n",
      "(5+348)*536 : 188532\n",
      "(600+3)*8 : 4644\n",
      "3*904+52 : 2892\n",
      "410+11*1 : 501\n",
      "Epoch 16 average loss = 1.2024129553842478\n",
      "299*(637+1) : 197246\n",
      "203*(1+8) : 1963\n",
      "720*(6+381) : 271080\n",
      "707+18*1 : 615\n",
      "3*(0+895) : 2375\n",
      "Epoch 17 average loss = 1.1893162448340235\n",
      "(20+26)*3 : 132\n",
      "132*(378+93) : 58194\n",
      "(253+6)*6 : 1538\n",
      "40*917+2 : 36506\n",
      "35*(95+52) : 4395\n",
      "Epoch 18 average loss = 1.1826292327755485\n",
      "(33+157)*89 : 16624\n",
      "(475+73)*7 : 3784\n",
      "530*(282+49) : 187080\n",
      "54+5*2 : 66\n",
      "92*976+86 : 90994\n",
      "Epoch 19 average loss = 1.1629174795525152\n",
      "1+489*95 : 44806\n",
      "988+806*447 : 344076\n",
      "16+26*98 : 2386\n",
      "47+1*293 : 300\n",
      "215*(533+37) : 126020\n",
      "Epoch 20 average loss = 1.1469790505769892\n",
      "751*54+8 : 40222\n",
      "926+895*487 : 457885\n",
      "970*(801+748) : 1380650\n",
      "13*(2+686) : 8334\n",
      "928+90*316 : 28848\n",
      "Epoch 21 average loss = 1.1391108295157515\n",
      "336*75+8 : 26616\n",
      "62*(156+13) : 11496\n",
      "120+94*11 : 1016\n",
      "4*(0+378) : 1546\n",
      "444+740*94 : 68214\n",
      "Epoch 22 average loss = 1.1373637422170364\n",
      "8*(92+88) : 1272\n",
      "8*372+16 : 2956\n",
      "78*186+260 : 15738\n",
      "43+2*44 : 135\n",
      "63*93+3 : 5316\n",
      "Epoch 23 average loss = 1.1165935982389406\n",
      "(85+28)*0 : 0\n",
      "6+1*86 : 97\n",
      "6*(82+9) : 600\n",
      "(97+945)*138 : 150702\n",
      "589*19+783 : 12546\n",
      "Epoch 24 average loss = 1.1090152640548085\n",
      "912+24*199 : 7124\n",
      "12*(9+6) : 180\n",
      "39*3+308 : 439\n",
      "786*4+315 : 3647\n",
      "4+30*1 : 52\n",
      "Epoch 25 average loss = 1.102060202394343\n",
      "(119+16)*88 : 13354\n",
      "(62+53)*441 : 47381\n",
      "(815+2)*2 : 2030\n",
      "(454+93)*1 : 463\n",
      "939*9+3 : 7934\n",
      "Epoch 26 average loss = 1.0947695981137102\n",
      "93*67+136 : 6483\n",
      "3*(95+3) : 281\n",
      "90+7*405 : 2965\n",
      "642+26*6 : 832\n",
      "481+9*763 : 7114\n",
      "Epoch 27 average loss = 1.0840353680460069\n",
      "(348+0)*7 : 2476\n",
      "147*(18+252) : 51216\n",
      "(7+9)*91 : 1466\n",
      "115*(39+6) : 5935\n",
      "(79+135)*6 : 1192\n",
      "Epoch 28 average loss = 1.0837323220371065\n",
      "624+705*44 : 33086\n",
      "3+224*74 : 16989\n",
      "(766+68)*0 : 0\n",
      "3*(29+34) : 181\n",
      "63*(261+1) : 16302\n",
      "Epoch 29 average loss = 1.0674505938476793\n",
      "(6+132)*107 : 20164\n",
      "8*6+869 : 1001\n",
      "76*802+24 : 60120\n",
      "4*806+976 : 4280\n",
      "1*0+16 : 18\n"
     ]
    }
   ],
   "source": [
    "#total N iterations\n",
    "n_epochs=30\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "batches_per_epoch = 500\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    try:\n",
    "\n",
    "        avg_cost = 0;\n",
    "\n",
    "        for _ in range(batches_per_epoch):\n",
    "\n",
    "            x,y = sample_batch(source_seqs, source_letter_to_ix, target_seqs, target_letter_to_ix, batch_size)\n",
    "            avg_cost += train(x, y).mean()\n",
    "\n",
    "        print(\"Epoch {} average loss = {}\".format(epoch, avg_cost / batches_per_epoch))\n",
    "        for i in range(5):\n",
    "            ind = np.random.randint(len(source_seqs))\n",
    "\n",
    "            print (source_seqs[ind],':', ''.join(generate_output(source_seqs[ind], probs, target_letters, target_letter_to_ix, \\\n",
    "                                                             source_letter_to_ix, sample=True)[1:-1]),' | ', ''.join(target_seqs[ind][1:-1]))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
